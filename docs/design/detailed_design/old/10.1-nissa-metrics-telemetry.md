# Nissa Metrics and Telemetry Design

## Document Metadata

| Field | Value |
|-------|-------|
| Parent Document | [10 - Nissa Unified Design](10-nissa-unified-design.md) |
| Component Type | Observability - Metrics and Telemetry |
| Version | 3.0 |
| Status | Production Ready with C-016 Enhancements |
| Date | 2025-09-10 |
| Author | System Architecture Team |

---

## Overview

This document details the metrics collection and telemetry processing subsystems of Nissa. These components form the data ingestion layer, handling high-throughput metric collection, event processing, and multi-resolution storage with comprehensive resilience patterns from the C-016 External Review.

### Key Capabilities

- **High-throughput ingestion**: 1M+ datapoints/second, 250K events/second
- **Multi-resolution downsampling**: Automatic aggregation for long-term storage
- **Protocol Buffer v2 messaging**: Standardized telemetry format with validation
- **Memory management**: TTL-based cleanup preventing memory leaks
- **Circuit breaker protection**: Graceful degradation under failures

## Technical Design

### Metrics Engine Architecture

```python
class MetricsEngine:
    """High-throughput metrics collection with C-016 enhancements"""

    def __init__(self):
        self.batch_collector = MetricsBatch()
        self.downsampler = DownsamplingAggregator()
        self.cardinality_limiter = CardinalityLimiter(max_series=1_000_000)

        # [R6-FIX] Circuit breaker protection
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(), "metrics_engine"
        )

        # [R6-FIX] Memory management
        self.memory_manager = MemoryManager()

        # [R6-FIX] Conservative mode
        self.conservative_mode = ConservativeModeManager()

        # Resolution tiers with _ms suffix
        self.resolutions = {
            "raw": {"interval_ms": 1_000, "retention_hours": 6},        # 1-second for 6 hours
            "1m": {"interval_ms": 60_000, "retention_hours": 24},       # 1-minute for 24 hours
            "5m": {"interval_ms": 300_000, "retention_days": 7},        # 5-minute for 7 days
            "1h": {"interval_ms": 3_600_000, "retention_days": 30},     # 1-hour for 30 days
            "1d": {"interval_ms": 86_400_000, "retention_days": 365}    # 1-day for 1 year
        }

    async def ingest_metrics(self, metrics: List[MetricPoint]) -> Dict[str, Any]:
        """Ingest metrics with circuit breaker protection"""

        return self.circuit_breaker.execute_with_protection(
            self._ingest_metrics_internal,
            "ingest_metrics",
            metrics
        )

    async def _ingest_metrics_internal(self, metrics: List[MetricPoint]) -> Dict[str, Any]:
        """Internal metrics ingestion with conservative mode handling"""

        # Get processing limits based on current mode
        limits = self.conservative_mode.get_processing_limits()
        batch_size = limits["batch_size"]

        processed_count = 0

        # Process in batches based on conservative mode
        for i in range(0, len(metrics), batch_size):
            batch = metrics[i:i + batch_size]

            # Store in memory manager with TTL
            for metric in batch:
                await self.memory_manager.store_with_ttl(
                    "metric",
                    metric.epoch,
                    metric.request_id,
                    metric.data
                )

            processed_count += len(batch)

            # Add delay if in conservative mode
            if limits["processing_delay_ms"] > 0:
                await asyncio.sleep(limits["processing_delay_ms"] / 1000.0)

        return {
            "status": "success",
            "processed_count": processed_count,
            "mode": self.conservative_mode.current_mode.value
        }
```

### Telemetry Processor Implementation

The telemetry processor handles event ingestion with comprehensive resilience:

```python
class TelemetryProcessor:
    """Event processing with Protocol Buffer v2 and circuit breakers"""

    def __init__(self):
        self.event_router = EventRouter()
        self.event_enricher = EventEnricher()
        self.event_validator = EventValidator()

        # Protocol Buffer v2 handler
        self.protobuf_handler = ProtocolBufferV2Handler()

        # Resilience components
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(), "telemetry_processor"
        )
        self.memory_manager = MemoryManager()

        # Processing pipeline
        self.pipeline = EventProcessingPipeline([
            self.validate_event,
            self.enrich_event,
            self.route_event,
            self.store_event
        ])
```

### Protocol Buffer v2 Integration

All telemetry messages use Protocol Buffer v2 with validation:

```python
class ProtocolBufferV2Handler:
    """[R6-FIX] Protocol Buffers v2 message handling with validation"""

    def __init__(self):
        # Message type registry
        self.message_types = {
            "telemetry_event": TelemetryEventProto,
            "metric_point": MetricPointProto,
            "alert_notification": AlertNotificationProto,
            "control_command": ControlCommandProto
        }

        # Validation metrics
        self.encode_operations = 0
        self.decode_operations = 0
        self.validation_failures = 0

    async def encode_message(
        self,
        message_type: str,
        data: Dict[str, Any]
    ) -> bytes:
        """Encode data to Protocol Buffer v2 format with validation"""

        try:
            proto_class = self.message_types.get(message_type)
            if not proto_class:
                raise ValueError(f"Unknown message type: {message_type}")

            # Create proto message
            proto_message = proto_class()

            # Populate fields
            for field, value in data.items():
                if hasattr(proto_message, field):
                    setattr(proto_message, field, value)
                else:
                    logging.warning(f"Unknown field {field} for message type {message_type}")

            # Serialize to bytes
            encoded = proto_message.SerializeToString()

            # Validate by decode-reencode
            await self._validate_message_integrity(encoded, proto_class)

            self.encode_operations += 1
            return encoded

        except Exception as e:
            self.validation_failures += 1
            logging.error(f"Failed to encode {message_type}: {e}")
            raise

    async def decode_message(
        self,
        message_type: str,
        encoded_data: bytes
    ) -> Dict[str, Any]:
        """Decode Protocol Buffer v2 message with validation"""

        try:
            proto_class = self.message_types.get(message_type)
            if not proto_class:
                raise ValueError(f"Unknown message type: {message_type}")

            # Parse from bytes
            proto_message = proto_class()
            proto_message.ParseFromString(encoded_data)

            # Convert to dictionary
            result = {}
            for field, value in proto_message.ListFields():
                result[field.name] = value

            # Validate decode-reencode integrity
            reencoded = proto_message.SerializeToString()
            if reencoded != encoded_data:
                logging.warning(f"Message {message_type} failed round-trip validation")

            self.decode_operations += 1
            return result

        except Exception as e:
            self.validation_failures += 1
            logging.error(f"Failed to decode {message_type}: {e}")
            raise

    async def _validate_message_integrity(
        self,
        encoded_data: bytes,
        proto_class: type
    ) -> None:
        """Validate message integrity with decode-reencode"""

        # Decode the encoded message
        temp_message = proto_class()
        temp_message.ParseFromString(encoded_data)

        # Re-encode and compare
        reencoded = temp_message.SerializeToString()

        if reencoded != encoded_data:
            raise ValueError("Message failed decode-reencode validation")
```

### Memory Management System

TTL-based memory management prevents leaks:

```python
class MemoryManager:
    """[R6-FIX] Prevent memory leaks with TTL-based cleanup"""

    def __init__(self):
        # TTL-based storage with (epoch, request_id) composite keys
        self.event_cache: Dict[Tuple[int, str], Dict] = {}
        self.metric_cache: Dict[Tuple[int, str], Dict] = {}
        self.alert_cache: Dict[Tuple[int, str], Dict] = {}

        # TTL settings (all in milliseconds)
        self.event_ttl_ms = 3_600_000      # 1 hour
        self.metric_ttl_ms = 7_200_000     # 2 hours
        self.alert_ttl_ms = 86_400_000     # 24 hours

        # Cleanup configuration
        self.cleanup_interval_ms = 300_000   # 5 minutes
        self.cleanup_batch_size = 10_000     # Process in batches

        # Metrics tracking
        self.total_entries_cleaned = 0
        self.cleanup_operations = 0

    async def store_with_ttl(
        self,
        cache_type: str,
        epoch: int,
        request_id: str,
        data: Dict,
        custom_ttl_ms: Optional[int] = None
    ) -> None:
        """Store data with TTL for automatic cleanup"""

        composite_key = (epoch, request_id)
        current_time_ms = int(time.time() * 1000)

        # Select appropriate cache and TTL
        if cache_type == "event":
            cache = self.event_cache
            ttl_ms = custom_ttl_ms or self.event_ttl_ms
        elif cache_type == "metric":
            cache = self.metric_cache
            ttl_ms = custom_ttl_ms or self.metric_ttl_ms
        elif cache_type == "alert":
            cache = self.alert_cache
            ttl_ms = custom_ttl_ms or self.alert_ttl_ms
        else:
            raise ValueError(f"Unknown cache type: {cache_type}")

        # Store with expiration timestamp
        cache[composite_key] = {
            "data": data,
            "stored_at_ms": current_time_ms,
            "expires_at_ms": current_time_ms + ttl_ms,
            "epoch": epoch,
            "request_id": request_id
        }

    async def cleanup_expired_entries(self) -> Dict[str, int]:
        """Clean up expired entries from all caches"""
        current_time_ms = int(time.time() * 1000)
        cleanup_stats = {
            "events_cleaned": 0,
            "metrics_cleaned": 0,
            "alerts_cleaned": 0,
            "total_cleaned": 0
        }

        # Clean each cache type
        for cache_name, cache in [
            ("events", self.event_cache),
            ("metrics", self.metric_cache),
            ("alerts", self.alert_cache)
        ]:
            expired_keys = []

            # Identify expired entries
            for key, entry in cache.items():
                if entry["expires_at_ms"] <= current_time_ms:
                    expired_keys.append(key)

            # Remove expired entries in batches
            for i in range(0, len(expired_keys), self.cleanup_batch_size):
                batch = expired_keys[i:i + self.cleanup_batch_size]
                for key in batch:
                    cache.pop(key, None)

                cleanup_stats[f"{cache_name}_cleaned"] += len(batch)

                # Yield control to prevent blocking
                await asyncio.sleep(0)

        cleanup_stats["total_cleaned"] = sum([
            cleanup_stats["events_cleaned"],
            cleanup_stats["metrics_cleaned"],
            cleanup_stats["alerts_cleaned"]
        ])

        self.total_entries_cleaned += cleanup_stats["total_cleaned"]
        self.cleanup_operations += 1

        logging.info(f"Memory cleanup completed: {cleanup_stats}")
        return cleanup_stats
```

### Multi-Resolution Downsampling

Automatic aggregation for storage efficiency:

```python
class DownsamplingAggregator:
    """Multi-resolution metric aggregation"""

    def __init__(self):
        self.aggregation_functions = {
            "counter": self.aggregate_counter,
            "gauge": self.aggregate_gauge,
            "histogram": self.aggregate_histogram,
            "summary": self.aggregate_summary
        }

        self.resolution_configs = {
            "1m": {
                "source": "raw",
                "interval_ms": 60_000,
                "aggregations": ["mean", "min", "max", "count", "sum"]
            },
            "5m": {
                "source": "1m",
                "interval_ms": 300_000,
                "aggregations": ["mean", "min", "max", "p50", "p95", "p99"]
            },
            "1h": {
                "source": "5m",
                "interval_ms": 3_600_000,
                "aggregations": ["mean", "min", "max", "p50", "p95", "p99", "stddev"]
            },
            "1d": {
                "source": "1h",
                "interval_ms": 86_400_000,
                "aggregations": ["mean", "min", "max", "p50", "p95", "p99", "stddev"]
            }
        }

    async def downsample_metrics(
        self,
        source_resolution: str,
        target_resolution: str,
        time_range: TimeRange
    ) -> List[AggregatedMetric]:
        """Downsample metrics from source to target resolution"""

        config = self.resolution_configs[target_resolution]

        # Fetch source metrics
        source_metrics = await self.fetch_metrics(
            source_resolution,
            time_range
        )

        # Group by interval
        grouped = self.group_by_interval(
            source_metrics,
            config["interval_ms"]
        )

        # Apply aggregations
        aggregated = []
        for interval, metrics in grouped.items():
            for aggregation in config["aggregations"]:
                result = await self.apply_aggregation(
                    metrics,
                    aggregation
                )
                aggregated.append(AggregatedMetric(
                    timestamp=interval,
                    resolution=target_resolution,
                    aggregation=aggregation,
                    value=result
                ))

        return aggregated
```

### Prometheus Backend Integration

Time-series storage with Prometheus:

```python
class PrometheusBackend:
    """Prometheus integration for metrics storage"""

    def __init__(self):
        self.remote_write_url = "http://prometheus:9090/api/v1/write"
        self.query_url = "http://prometheus:9090/api/v1/query"
        self.batch_size = 10_000
        self.compression = "snappy"

        # Circuit breaker for Prometheus operations
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(), "prometheus_backend"
        )

    async def write_metrics(self, metrics: List[MetricPoint]) -> None:
        """Write metrics to Prometheus with circuit breaker protection"""

        return self.circuit_breaker.execute_with_protection(
            self._write_metrics_internal,
            "write_metrics",
            metrics
        )

    async def _write_metrics_internal(self, metrics: List[MetricPoint]) -> None:
        """Internal metric writing with batching and compression"""

        # Convert to Prometheus format
        timeseries = []
        for metric in metrics:
            timeseries.append({
                "__name__": metric.name,
                "job": "nissa",
                **metric.labels,
                "value": metric.value,
                "timestamp": metric.timestamp_ms
            })

        # Process in batches
        for i in range(0, len(timeseries), self.batch_size):
            batch = timeseries[i:i + self.batch_size]

            # Compress batch
            compressed = self.compress_batch(batch)

            # Send to Prometheus
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.remote_write_url,
                    data=compressed,
                    headers={"Content-Encoding": self.compression}
                ) as response:
                    if response.status != 200:
                        raise Exception(f"Prometheus write failed: {response.status}")
```

## Performance Optimization

### Batch Processing

Efficient batch processing for high throughput:

```python
class MetricsBatch:
    """Batch collector for efficient processing"""

    def __init__(self):
        self.batch_size = 1000
        self.flush_interval_ms = 1000
        self.current_batch = []
        self.last_flush_ms = int(time.time() * 1000)

    async def add_metric(self, metric: MetricPoint) -> None:
        """Add metric to batch"""
        self.current_batch.append(metric)

        if len(self.current_batch) >= self.batch_size:
            await self.flush()
        elif self._should_flush_by_time():
            await self.flush()

    async def flush(self) -> None:
        """Flush current batch"""
        if not self.current_batch:
            return

        # Process batch
        await self.process_batch(self.current_batch)

        # Clear batch
        self.current_batch = []
        self.last_flush_ms = int(time.time() * 1000)

    def _should_flush_by_time(self) -> bool:
        """Check if batch should be flushed based on time"""
        current_time_ms = int(time.time() * 1000)
        return (current_time_ms - self.last_flush_ms) >= self.flush_interval_ms
```

### Cardinality Management

Control metric cardinality to prevent explosion:

```python
class CardinalityLimiter:
    """Limit metric cardinality to prevent resource exhaustion"""

    def __init__(self, max_series: int):
        self.max_series = max_series
        self.current_series = {}
        self.rejected_count = 0

    def should_accept(self, metric: MetricPoint) -> bool:
        """Check if metric should be accepted based on cardinality"""

        series_id = self._get_series_id(metric)

        if series_id in self.current_series:
            return True

        if len(self.current_series) >= self.max_series:
            self.rejected_count += 1
            logging.warning(f"Cardinality limit reached, rejecting metric: {series_id}")
            return False

        self.current_series[series_id] = True
        return True

    def _get_series_id(self, metric: MetricPoint) -> str:
        """Generate unique series identifier"""
        labels = sorted(metric.labels.items())
        return f"{metric.name}:{labels}"
```

## Integration Patterns

### Oona Message Bus Integration

High-performance integration with the message bus:

```python
class EnhancedOonaIntegration:
    """High-performance integration with Oona message bus - C-016 enhanced"""

    def __init__(self, message_bus: OonaMessageBus):
        self.message_bus = message_bus
        self.consumer_group = "nissa-observability"

        # Circuit breaker protection
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(), "oona_integration"
        )

        # Protocol Buffer v2 handling
        self.protobuf_handler = ProtocolBufferV2Handler()

        # Memory management
        self.memory_manager = MemoryManager()

        # Enhanced subscriptions
        self.subscriptions = [
            "telemetry.seed.health",
            "control.kasmina.commands",
            "innovation.field_reports",
            "compilation.kernel.ready",
            "validation.kernel.characterized",
            "system.events.epoch",
            "morphogenetic.adaptation.triggered",
            "circuit_breaker.state_changed"
        ]

        # Batch processing
        self.batch_processor = BatchProcessor(
            batch_size=1000,
            flush_interval_ms=1_000
        )

    async def start_consuming(self) -> None:
        """Start consuming events with circuit breaker protection"""

        for topic in self.subscriptions:
            consumption_task = self.circuit_breaker.execute_with_protection(
                self._start_topic_consumer,
                "start_consumer",
                topic
            )

            asyncio.create_task(consumption_task)
```

## Performance Metrics

### Target Performance

- **Event ingestion rate**: 250,000 events/second
- **Metric ingestion rate**: 1,000,000 datapoints/second
- **Batch processing size**: 1,000-10,000 metrics per batch
- **Flush interval**: 1 second
- **Compression ratio**: 85% for metrics (0.15 ratio)

### Memory Management Targets

- **Cleanup interval**: 5 minutes (300,000ms)
- **Event cache TTL**: 1 hour (3,600,000ms)
- **Metric cache TTL**: 2 hours (7,200,000ms)
- **Alert cache TTL**: 24 hours (86,400,000ms)
- **Cleanup batch size**: 10,000 records per batch

### Storage Efficiency

Multi-resolution retention:
- **Raw**: 6 hours at 1-second resolution
- **1-minute**: 24 hours aggregated
- **5-minute**: 7 days aggregated
- **1-hour**: 30 days aggregated
- **1-day**: 365 days aggregated

## Related Documentation

- [10 - Nissa Unified Design](10-nissa-unified-design.md)
- [10.2 - Nissa Mission Control Design](10.2-nissa-mission-control.md)
- [10.3 - Nissa Alerting and SLO Design](10.3-nissa-alerting-slo.md)
- [09 - Oona Message Bus Design](09-oona-unified-design.md)