# Kasmina - Parameter Registration and Enforcement

**Parent Document**: [02-kasmina-unified-design.md](./02-kasmina-unified-design.md)
**Component Type**: System|Safety
**Version**: 3.3
**Status**: PRODUCTION - Invariant L2 Compliance + Leyline Integration + C-024 Teacher Exclusion
**C-024 Updates**: Teacher parameter registration without optimizer, gradient verification enforcement

---

## Overview

This component provides parameter registration protocol and enforcement mechanisms for Kasmina. It ensures complete parameter tracking and validation to prevent Invariant L2 breaches that cause production failures. All seed parameters must be registered before any optimizer operations can proceed.

**CRITICAL IMPORTANCE**: This system prevents parameter registration violations causing Invariant L2 breaches - a major source of production instability identified in C-016.

**C-024 TEACHER EXCLUSION**: Teacher model parameters are registered for tracking but NEVER receive gradients or optimizer updates. They remain in eval mode permanently.

**Leyline Integration**: All parameter registration events and violations are reported through Leyline contracts for optimal monitoring and debugging.

## 1. Parameter Registration Protocol

### 1.1 Unified LR Controller Integration with C-024 Teacher Handling

```python
from typing import List, Dict, Set, Optional
import torch
import logging

class UnifiedLRControllerIntegration:
    """Enforce parameter registration for ALL seed parameters - C-024: exclude teacher from optimization"""

    def __init__(self):
        self.registered_parameter_groups: Dict[str, Set[int]] = {}
        self.seed_parameter_registry: Dict[str, List[torch.nn.Parameter]] = {}
        self.registration_lock = threading.Lock()
        self.logger = logging.getLogger(__name__)

        # C-024: Separate tracking for teacher parameters (no optimizer)
        self.teacher_parameter_ids: Set[int] = set()
        self.teacher_registered = False

        # Leyline integration
        from esper.leyline.contracts import TelemetryPacket, TelemetryLevel
        self.telemetry_reporter = LeylineTelemetryReporter()

    def register_teacher_parameters(self, parameters: List[torch.nn.Parameter]) -> bool:
        """C-024: Register teacher parameters WITHOUT creating optimizer"""

        with self.registration_lock:
            try:
                if self.teacher_registered:
                    self.logger.warning("Teacher parameters already registered")
                    return True

                # Validate all parameters have requires_grad=False
                for param in parameters:
                    if param.requires_grad:
                        # Force disable gradients on teacher
                        param.requires_grad = False
                        self.logger.warning("Forcing requires_grad=False on teacher parameter")

                # Track teacher parameter IDs (for exclusion from optimizer)
                self.teacher_parameter_ids = {id(p) for p in parameters}
                self.teacher_registered = True

                self.logger.info(f"Registered {len(parameters)} teacher parameters (eval only)")

                # Report teacher registration to Leyline
                self._report_teacher_registration(len(parameters))

                return True

            except Exception as e:
                self.logger.error(f"Failed to register teacher parameters: {e}")
                self._report_teacher_registration_failure(str(e))
                return False

    def register_seed_parameters(self,
                               seed_id: str,
                               parameters: List[torch.nn.Parameter],
                               lr_group: str = 'seed_default') -> bool:
        """Register seed parameters BEFORE any updates - C-024: verify not teacher params"""

        with self.registration_lock:
            try:
                # Validate parameters
                if not parameters:
                    raise ValueError(f"No parameters provided for seed {seed_id}")

                # C-024: Check for teacher parameter contamination
                param_ids = {id(p) for p in parameters}
                teacher_overlap = param_ids & self.teacher_parameter_ids
                if teacher_overlap:
                    raise ValueError(
                        f"Seed {seed_id} attempting to register {len(teacher_overlap)} teacher parameters - FORBIDDEN"
                    )

                # Check for duplicate registration
                if seed_id in self.seed_parameter_registry:
                    self.logger.warning(f"Seed {seed_id} already registered, updating")

                # Register with Unified LR Controller
                if lr_group not in self.registered_parameter_groups:
                    self.registered_parameter_groups[lr_group] = set()

                # Add to registered groups
                self.registered_parameter_groups[lr_group].update(param_ids)
                self.seed_parameter_registry[seed_id] = parameters

                self.logger.info(f"Registered {len(parameters)} parameters for seed {seed_id} in group {lr_group}")

                # Report successful registration to Leyline
                self._report_parameter_registration(seed_id, len(parameters), True)

                return True

            except Exception as e:
                self.logger.error(f"Failed to register parameters for seed {seed_id}: {e}")

                # Report failed registration to Leyline
                self._report_parameter_registration(seed_id, 0, False, str(e))

                return False

    def validate_parameter_update(self,
                                seed_id: str,
                                parameters: List[torch.nn.Parameter]) -> bool:
        """CRITICAL: Validate parameters are registered before ANY update - C-024: block teacher updates"""

        with self.registration_lock:
            try:
                # C-024: Check for teacher parameter update attempt
                update_param_ids = {id(p) for p in parameters}
                teacher_in_update = update_param_ids & self.teacher_parameter_ids
                if teacher_in_update:
                    raise ValueError(
                        f"Seed {seed_id} attempting to update {len(teacher_in_update)} teacher parameters - TEACHER PARAMS ARE IMMUTABLE"
                    )

                # Check seed is registered
                if seed_id not in self.seed_parameter_registry:
                    raise ValueError(f"Seed {seed_id} not registered - INVARIANT L2 VIOLATION")

                # Get registered parameters for this seed
                registered_params = self.seed_parameter_registry[seed_id]
                registered_param_ids = {id(p) for p in registered_params}

                # Check for unregistered parameters
                unregistered = update_param_ids - registered_param_ids
                if unregistered:
                    unregistered_count = len(unregistered)
                    raise ValueError(
                        f"Seed {seed_id} attempting to update {unregistered_count} unregistered parameters - INVARIANT L2 VIOLATION"
                    )

                # Check for parameters belonging to other seeds
                for param_id in update_param_ids:
                    # Find which seed owns this parameter
                    owning_seed = None
                    for other_seed_id, other_params in self.seed_parameter_registry.items():
                        if param_id in {id(p) for p in other_params}:
                            owning_seed = other_seed_id
                            break

                    if owning_seed and owning_seed != seed_id:
                        raise ValueError(
                            f"Seed {seed_id} attempting to update parameter owned by {owning_seed} - INVARIANT L2 VIOLATION"
                        )

                # All validations passed
                return True

            except ValueError as e:
                self.logger.error(f"Parameter validation failed for seed {seed_id}: {e}")

                # Report validation failure to Leyline
                self._report_validation_failure(seed_id, str(e))

                return False

    def verify_teacher_eval_mode(self, teacher_model: torch.nn.Module) -> bool:
        """C-024: Verify teacher model remains in eval mode with no gradients"""

        if teacher_model.training:
            self.logger.error("Teacher model found in training mode - forcing eval")
            teacher_model.eval()

        # Verify all teacher parameters have requires_grad=False
        violations = 0
        for param in teacher_model.parameters():
            if param.requires_grad:
                param.requires_grad = False
                violations += 1

        if violations > 0:
            self.logger.warning(f"Disabled gradients on {violations} teacher parameters")
            self._report_teacher_gradient_violation(violations)

        return True

    def unregister_seed_parameters(self, seed_id: str) -> bool:
        """Unregister seed parameters when seed is removed"""

        with self.registration_lock:
            try:
                if seed_id not in self.seed_parameter_registry:
                    self.logger.warning(f"Attempting to unregister non-existent seed {seed_id}")
                    return False

                # Remove from all parameter groups
                registered_params = self.seed_parameter_registry[seed_id]
                param_ids = {id(p) for p in registered_params}

                for lr_group in self.registered_parameter_groups:
                    self.registered_parameter_groups[lr_group] -= param_ids

                # Remove from seed registry
                del self.seed_parameter_registry[seed_id]

                self.logger.info(f"Unregistered {len(registered_params)} parameters for seed {seed_id}")

                # Report successful unregistration to Leyline
                self._report_parameter_unregistration(seed_id, len(registered_params), True)

                return True

            except Exception as e:
                self.logger.error(f"Failed to unregister parameters for seed {seed_id}: {e}")

                # Report failed unregistration to Leyline
                self._report_parameter_unregistration(seed_id, 0, False, str(e))

                return False

    def get_registration_stats(self) -> Dict[str, Any]:
        """Get parameter registration statistics including C-024 teacher stats"""

        with self.registration_lock:
            total_registered_seeds = len(self.seed_parameter_registry)
            total_registered_params = sum(len(params) for params in self.seed_parameter_registry.values())

            lr_group_stats = {}
            for lr_group, param_ids in self.registered_parameter_groups.items():
                lr_group_stats[lr_group] = len(param_ids)

            return {
                'total_registered_seeds': total_registered_seeds,
                'total_registered_parameters': total_registered_params,
                'lr_group_distribution': lr_group_stats,
                'registered_seed_ids': list(self.seed_parameter_registry.keys()),
                'avg_parameters_per_seed': (
                    total_registered_params / total_registered_seeds
                    if total_registered_seeds > 0 else 0.0
                ),
                # C-024: Teacher stats
                'teacher_registered': self.teacher_registered,
                'teacher_parameter_count': len(self.teacher_parameter_ids)
            }

    def _report_teacher_registration(self, param_count: int) -> None:
        """C-024: Report teacher parameter registration to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_registration_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_parameter_registration"
        event.severity = TelemetryLevel.TELEMETRY_INFO
        event.message = f"Registered {param_count} teacher parameters (eval only)"
        event.timestamp.GetCurrentTime()

        event.attributes["parameter_count"] = str(param_count)
        event.attributes["requires_grad"] = "False"
        event.attributes["optimizer_created"] = "False"

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_teacher_gradient_violation(self, violation_count: int) -> None:
        """C-024: Report teacher gradient violations to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_gradient_violation_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_gradient_violation"
        event.severity = TelemetryLevel.TELEMETRY_WARN
        event.message = f"Found {violation_count} teacher parameters with gradients enabled"
        event.timestamp.GetCurrentTime()

        event.attributes["violation_count"] = str(violation_count)
        event.attributes["action_taken"] = "forced_requires_grad_false"

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_parameter_registration(self, seed_id: str, param_count: int, success: bool, error: str = None) -> None:
        """Report parameter registration event to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"param_registration_{seed_id}_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add registration event
        event = telemetry.events.add()
        event.event_name = "parameter_registration"
        event.severity = TelemetryLevel.TELEMETRY_INFO if success else TelemetryLevel.TELEMETRY_ERROR
        event.message = f"Parameter registration {'succeeded' if success else 'failed'} for seed {seed_id}"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes (C-018 Option B)
        event.attributes["seed_id"] = seed_id
        event.attributes["parameter_count"] = str(param_count)
        event.attributes["success"] = str(success)
        if error:
            event.attributes["error"] = error

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_validation_failure(self, seed_id: str, error_message: str) -> None:
        """Report parameter validation failure to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"param_validation_failure_{seed_id}_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add validation failure event
        event = telemetry.events.add()
        event.event_name = "parameter_validation_failure"
        event.severity = TelemetryLevel.TELEMETRY_ERROR
        event.message = f"INVARIANT L2 VIOLATION: {error_message}"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        event.attributes["seed_id"] = seed_id
        event.attributes["violation_type"] = "INVARIANT_L2"
        event.attributes["error_message"] = error_message

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_parameter_unregistration(self, seed_id: str, param_count: int, success: bool, error: str = None) -> None:
        """Report parameter unregistration event to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"param_unregistration_{seed_id}_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add unregistration event
        event = telemetry.events.add()
        event.event_name = "parameter_unregistration"
        event.severity = TelemetryLevel.TELEMETRY_INFO if success else TelemetryLevel.TELEMETRY_ERROR
        event.message = f"Parameter unregistration {'succeeded' if success else 'failed'} for seed {seed_id}"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        event.attributes["seed_id"] = seed_id
        event.attributes["parameter_count"] = str(param_count)
        event.attributes["success"] = str(success)
        if error:
            event.attributes["error"] = error

        self.telemetry_reporter.send_telemetry_async(telemetry)
```

## 2. Seed Optimizer with Registration Enforcement

### 2.1 Registration-Validated Optimizer with C-024 Teacher Exclusion

```python
class RegistrationValidatedSeedOptimizer:
    """Seed optimizer that REQUIRES parameter registration before ANY operations - C-024: excludes teacher"""

    def __init__(self,
                 seed_id: str,
                 lr_controller: UnifiedLRControllerIntegration,
                 **optimizer_kwargs):

        self.seed_id = seed_id
        self.lr_controller = lr_controller
        self.base_optimizer = None
        self.optimizer_kwargs = optimizer_kwargs
        self.is_initialized = False

        # Performance tracking
        self.step_count = 0
        self.validation_failures = 0
        self.teacher_update_attempts = 0  # C-024: Track forbidden updates

    def initialize_with_parameters(self, parameters: List[torch.nn.Parameter]) -> None:
        """Initialize optimizer ONLY after parameter registration - C-024: verify no teacher params"""

        # CRITICAL: Validate registration BEFORE creating optimizer
        if not self.lr_controller.validate_parameter_update(self.seed_id, parameters):
            raise ValueError(f"Cannot initialize optimizer for unregistered seed {self.seed_id}")

        # C-024: Double-check no teacher parameters in optimizer
        param_ids = {id(p) for p in parameters}
        if param_ids & self.lr_controller.teacher_parameter_ids:
            raise ValueError(f"Cannot include teacher parameters in optimizer for seed {self.seed_id}")

        # Create base optimizer
        self.base_optimizer = torch.optim.AdamW(parameters, **self.optimizer_kwargs)
        self.is_initialized = True

    def step(self) -> None:
        """Step optimizer with registration validation - C-024: verify no teacher updates"""
        if not self.is_initialized:
            raise RuntimeError(f"Optimizer for seed {self.seed_id} not initialized")

        # Validate parameters before update
        param_groups = self.base_optimizer.param_groups
        all_params = []
        for group in param_groups:
            all_params.extend(group['params'])

        try:
            # CRITICAL: Check registration before each step
            self.lr_controller.validate_parameter_update(self.seed_id, all_params)

            # C-024: Final check - no teacher parameters should ever reach here
            param_ids = {id(p) for p in all_params}
            if param_ids & self.lr_controller.teacher_parameter_ids:
                self.teacher_update_attempts += 1
                raise ValueError("CRITICAL: Teacher parameters in optimizer step - BLOCKED")

            # Perform update
            self.base_optimizer.step()
            self.step_count += 1

        except ValueError as e:
            self.validation_failures += 1
            logger.error(f"Parameter validation failed for seed {self.seed_id}: {e}")
            raise

    def zero_grad(self) -> None:
        """Zero gradients with safety checks"""
        if not self.is_initialized:
            raise RuntimeError(f"Optimizer for seed {self.seed_id} not initialized")

        self.base_optimizer.zero_grad()

    def get_optimizer_stats(self) -> Dict[str, Any]:
        """Get optimizer statistics for telemetry"""
        return {
            'seed_id': self.seed_id,
            'is_initialized': self.is_initialized,
            'step_count': self.step_count,
            'validation_failures': self.validation_failures,
            'teacher_update_attempts': self.teacher_update_attempts,  # C-024
            'parameter_group_count': len(self.base_optimizer.param_groups) if self.base_optimizer else 0
        }
```

## 3. Parameter Ownership Tracking

### 3.1 Parameter Ownership Manager with C-024 Teacher Awareness

```python
class ParameterOwnershipManager:
    """Track and enforce parameter ownership across seeds - C-024: special handling for teacher"""

    def __init__(self):
        self.parameter_ownership: Dict[int, str] = {}  # param_id -> seed_id
        self.seed_parameters: Dict[str, Set[int]] = {}  # seed_id -> param_ids
        self.ownership_lock = threading.RLock()

        # C-024: Teacher ownership tracking
        self.teacher_parameter_ids: Set[int] = set()
        self.teacher_ownership_claimed = False

        # Violation tracking
        self.ownership_violations = 0
        self.double_registration_attempts = 0
        self.teacher_claim_attempts = 0  # C-024: Track teacher claim attempts

        # Leyline integration
        from esper.leyline.contracts import TelemetryPacket
        self.telemetry_reporter = LeylineTelemetryReporter()

    def claim_teacher_ownership(self, parameters: List[torch.nn.Parameter]) -> bool:
        """C-024: Claim ownership of teacher parameters (special handling)"""

        with self.ownership_lock:
            if self.teacher_ownership_claimed:
                return True  # Already claimed

            # Mark all teacher parameters as owned by special "TEACHER" entity
            for param in parameters:
                param_id = id(param)
                self.parameter_ownership[param_id] = "TEACHER"
                self.teacher_parameter_ids.add(param_id)

            self.teacher_ownership_claimed = True
            self._report_teacher_ownership_claimed(len(parameters))

            return True

    def claim_parameter_ownership(self, seed_id: str, parameter: torch.nn.Parameter) -> bool:
        """Claim ownership of a parameter for a seed - C-024: block teacher param claims"""

        param_id = id(parameter)

        with self.ownership_lock:
            # C-024: Check if this is a teacher parameter
            if param_id in self.teacher_parameter_ids:
                self.teacher_claim_attempts += 1
                self._report_teacher_claim_attempt(seed_id, param_id)
                return False  # Cannot claim teacher parameters

            # Check if parameter is already owned
            if param_id in self.parameter_ownership:
                existing_owner = self.parameter_ownership[param_id]
                if existing_owner != seed_id:
                    self.ownership_violations += 1
                    self._report_ownership_violation(seed_id, existing_owner, param_id)
                    return False
                else:
                    # Already owned by this seed - this is a double registration attempt
                    self.double_registration_attempts += 1
                    return True  # Allow, but track

            # Claim ownership
            self.parameter_ownership[param_id] = seed_id

            if seed_id not in self.seed_parameters:
                self.seed_parameters[seed_id] = set()
            self.seed_parameters[seed_id].add(param_id)

            return True

    def release_parameter_ownership(self, seed_id: str, parameter: torch.nn.Parameter) -> bool:
        """Release ownership of a parameter - C-024: block teacher param release"""

        param_id = id(parameter)

        with self.ownership_lock:
            # C-024: Teacher parameters cannot be released
            if param_id in self.teacher_parameter_ids:
                self.logger.warning(f"Cannot release teacher parameter ownership")
                return False

            # Check ownership
            if param_id not in self.parameter_ownership:
                return False  # Parameter not owned by anyone

            current_owner = self.parameter_ownership[param_id]
            if current_owner != seed_id:
                self.ownership_violations += 1
                self._report_ownership_violation(seed_id, current_owner, param_id)
                return False

            # Release ownership
            del self.parameter_ownership[param_id]
            if seed_id in self.seed_parameters:
                self.seed_parameters[seed_id].discard(param_id)

            return True

    def validate_parameter_access(self, seed_id: str, parameter: torch.nn.Parameter) -> bool:
        """Validate that a seed can access a parameter - C-024: block teacher access"""

        param_id = id(parameter)

        with self.ownership_lock:
            # C-024: Seeds cannot access teacher parameters
            if param_id in self.teacher_parameter_ids:
                return False

            if param_id not in self.parameter_ownership:
                return False  # Unowned parameter

            return self.parameter_ownership[param_id] == seed_id

    def get_seed_parameters(self, seed_id: str) -> List[int]:
        """Get all parameter IDs owned by a seed"""

        with self.ownership_lock:
            return list(self.seed_parameters.get(seed_id, set()))

    def get_parameter_owner(self, parameter: torch.nn.Parameter) -> Optional[str]:
        """Get the owner of a parameter - C-024: returns 'TEACHER' for teacher params"""

        param_id = id(parameter)

        with self.ownership_lock:
            return self.parameter_ownership.get(param_id)

    def cleanup_seed_parameters(self, seed_id: str) -> int:
        """Clean up all parameters owned by a seed"""

        with self.ownership_lock:
            if seed_id not in self.seed_parameters:
                return 0

            param_ids = list(self.seed_parameters[seed_id])

            # Remove all parameters owned by this seed
            for param_id in param_ids:
                if param_id in self.parameter_ownership:
                    del self.parameter_ownership[param_id]

            # Remove seed from tracking
            del self.seed_parameters[seed_id]

            return len(param_ids)

    def get_ownership_stats(self) -> Dict[str, Any]:
        """Get parameter ownership statistics including C-024 teacher stats"""

        with self.ownership_lock:
            total_owned_params = len(self.parameter_ownership)
            total_seeds = len(self.seed_parameters)

            seed_param_counts = {
                seed_id: len(param_ids)
                for seed_id, param_ids in self.seed_parameters.items()
            }

            return {
                'total_owned_parameters': total_owned_params,
                'total_seeds_with_parameters': total_seeds,
                'ownership_violations': self.ownership_violations,
                'double_registration_attempts': self.double_registration_attempts,
                'seed_parameter_counts': seed_param_counts,
                'avg_parameters_per_seed': (
                    total_owned_params / total_seeds if total_seeds > 0 else 0.0
                ),
                # C-024: Teacher stats
                'teacher_parameters_owned': len(self.teacher_parameter_ids),
                'teacher_claim_attempts': self.teacher_claim_attempts
            }

    def _report_teacher_ownership_claimed(self, param_count: int) -> None:
        """C-024: Report teacher ownership claim to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_ownership_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_ownership_claimed"
        event.severity = TelemetryLevel.TELEMETRY_INFO
        event.message = f"Claimed ownership of {param_count} teacher parameters"
        event.timestamp.GetCurrentTime()

        event.attributes["parameter_count"] = str(param_count)
        event.attributes["owner"] = "TEACHER"

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_teacher_claim_attempt(self, seed_id: str, param_id: int) -> None:
        """C-024: Report attempted claim of teacher parameter"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_claim_attempt_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_parameter_claim_attempt"
        event.severity = TelemetryLevel.TELEMETRY_WARN
        event.message = f"Seed {seed_id} attempted to claim teacher parameter"
        event.timestamp.GetCurrentTime()

        event.attributes["seed_id"] = seed_id
        event.attributes["parameter_id"] = str(param_id)
        event.attributes["action"] = "BLOCKED"

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_ownership_violation(self, attempting_seed: str, current_owner: str, param_id: int) -> None:
        """Report parameter ownership violation to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"ownership_violation_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add violation event
        event = telemetry.events.add()
        event.event_name = "parameter_ownership_violation"
        event.severity = TelemetryLevel.TELEMETRY_ERROR
        event.message = f"INVARIANT L2 VIOLATION: Seed {attempting_seed} attempted to claim parameter owned by {current_owner}"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        event.attributes["attempting_seed"] = attempting_seed
        event.attributes["current_owner"] = current_owner
        event.attributes["parameter_id"] = str(param_id)
        event.attributes["violation_type"] = "PARAMETER_OWNERSHIP"

        self.telemetry_reporter.send_telemetry_async(telemetry)
```

## 4. Integration Contract

This component integrates with the main Kasmina architecture by providing:

### 4.1 Parameter Registration Interface with C-024 Extensions

```python
@dataclass
class ParameterRegistrationContract:
    """Interface contract for parameter registration component with C-024 teacher support"""

    # Parameter registration methods
    def register_seed_parameters(self, seed_id: str, parameters: List[torch.nn.Parameter], lr_group: str = 'seed_default') -> bool
    def validate_parameter_update(self, seed_id: str, parameters: List[torch.nn.Parameter]) -> bool
    def unregister_seed_parameters(self, seed_id: str) -> bool
    def get_registration_stats(self) -> Dict[str, Any]

    # C-024: Teacher parameter methods
    def register_teacher_parameters(self, parameters: List[torch.nn.Parameter]) -> bool
    def verify_teacher_eval_mode(self, teacher_model: torch.nn.Module) -> bool
    def claim_teacher_ownership(self, parameters: List[torch.nn.Parameter]) -> bool

    # Parameter ownership methods
    def claim_parameter_ownership(self, seed_id: str, parameter: torch.nn.Parameter) -> bool
    def release_parameter_ownership(self, seed_id: str, parameter: torch.nn.Parameter) -> bool
    def validate_parameter_access(self, seed_id: str, parameter: torch.nn.Parameter) -> bool
    def get_parameter_owner(self, parameter: torch.nn.Parameter) -> Optional[str]
    def cleanup_seed_parameters(self, seed_id: str) -> int
    def get_ownership_stats(self) -> Dict[str, Any]

    # Optimizer integration methods
    def create_validated_optimizer(self, seed_id: str, **optimizer_kwargs) -> RegistrationValidatedSeedOptimizer
    def initialize_optimizer_with_parameters(self, optimizer: RegistrationValidatedSeedOptimizer, parameters: List[torch.nn.Parameter]) -> None

    # Leyline integration methods
    def report_to_leyline_telemetry(self, event_data: Dict[str, Any]) -> None
```

### 4.2 Component Coordination

This component coordinates with other Kasmina components:

- **[02.1-kasmina-kernel-execution.md](./02.1-kasmina-kernel-execution.md)**: Parameter isolation enforcement during kernel execution + C-024 teacher exclusion
- **[02.2-kasmina-memory-pools.md](./02.2-kasmina-memory-pools.md)**: Memory allocation tracking for registered parameters + C-024 teacher memory
- **[02.4-kasmina-safety-mechanisms.md](./02.4-kasmina-safety-mechanisms.md)**: Circuit breaker integration for registration violations + C-024 teacher update blocks
- **[02.5-kasmina-performance-validation.md](./02.5-kasmina-performance-validation.md)**: Parameter registration overhead benchmarking + C-024 teacher metrics

## 5. References

- **Parent Document**: [02-kasmina-unified-design.md](./02-kasmina-unified-design.md)
- **Kernel Execution**: [02.1-kasmina-kernel-execution.md](./02.1-kasmina-kernel-execution.md)
- **Memory Pools**: [02.2-kasmina-memory-pools.md](./02.2-kasmina-memory-pools.md)
- **Safety Mechanisms**: [02.4-kasmina-safety-mechanisms.md](./02.4-kasmina-safety-mechanisms.md)
- **Performance Validation**: [02.5-kasmina-performance-validation.md](./02.5-kasmina-performance-validation.md)
- **Leyline Contracts**: [00-leyline-shared-contracts.md](./00-leyline-shared-contracts.md)
- **C-018 Final Consensus**: Option B (Performance-First) implementation
- **C-024 KD Amendment**: Teacher parameter registration without optimization
- **Conclave C-016**: Emergency Production Safety Response - Invariant L2 compliance

---

**COMPONENT STATUS**: COMPLETE - Invariant L2 Compliance + Leyline Integrated + C-024 Teacher Exclusion
**Production Safety**: All parameter registration violations prevented
**Parameter Tracking**: Comprehensive ownership management with validation
**C-024 Enhancements**: Teacher parameters tracked but never optimized, always eval mode
**Optimizer Integration**: Registration-validated optimizers ensure compliance
**Integration**: Fully coordinated with kernel execution and safety systems