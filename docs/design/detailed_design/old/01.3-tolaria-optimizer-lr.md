# Tolaria - Optimizer and Learning Rate Management

**Parent Document**: [01-tolaria-unified-design.md](01-tolaria-unified-design.md)  
**Component Type**: Optimizer State and LR Control  
**Version**: 3.0 (Split from v2.2)  
**Date**: 2025-01-10

## Overview

This document details Tolaria's dynamic optimizer management and the UnifiedLRController, which has exclusive rights to mutate learning rates across the entire system. The implementation preserves optimizer state during morphogenetic operations while enforcing strict learning rate invariants to prevent production crashes.

## Dynamic Optimizer State Preservation

### Core Algorithm

**[C-016 FIX] Enhanced with UnifiedLRController integration:**

```python
class DynamicOptimizerStatePreserver:
    """
    Complete algorithm for preserving optimizer state during morphogenetic operations.
    Handles Adam, SGD, and RMSprop optimizers with full momentum preservation.
    """
    
    def __init__(self, unified_lr_controller: 'UnifiedLRController'):
        self.unified_lr_controller = unified_lr_controller  # [C-016 FIX]
    
    def preserve_optimizer_state(
        self,
        old_optimizer: torch.optim.Optimizer,
        new_model: nn.Module,
        old_model: nn.Module,
        parameter_mappings: Optional[List[ParameterMapping]] = None
    ) -> torch.optim.Optimizer:
        """
        Main entry point for optimizer state preservation.
        Preserves momentum states when architecture changes occur.
        """
        # Extract old optimizer configuration
        optimizer_type = type(old_optimizer)
        old_state_dict = old_optimizer.state_dict()
        old_params = old_state_dict['param_groups'][0].copy()
        del old_params['params']  # Remove parameter references
        
        # Create parameter mappings if not provided
        if parameter_mappings is None:
            parameter_mappings = self._create_automatic_mappings(old_model, new_model)
        
        # Create new optimizer with same configuration
        new_optimizer = optimizer_type(new_model.parameters(), **old_params)
        
        # [C-016 FIX] Unregister old optimizer and register new one with UnifiedLRController
        self.unified_lr_controller.unregister_optimizer("primary")
        
        # Build parameter ID maps
        old_param_map = self._build_parameter_map(old_model)
        new_param_map = self._build_parameter_map(new_model)
        
        # Preserve state for each parameter
        new_state = {}
        for mapping in parameter_mappings:
            old_param = old_param_map.get(mapping.old_id)
            new_param = new_param_map.get(mapping.new_id)
            
            if old_param is None or new_param is None:
                continue
                
            # Get old state for this parameter
            old_param_state = old_state_dict['state'].get(id(old_param), {})
            
            if not old_param_state:
                continue
                
            # Transform state based on mapping type
            new_param_state = self._transform_parameter_state(
                old_param_state,
                mapping,
                optimizer_type
            )
            
            # Assign transformed state to new parameter
            new_state[new_param] = new_param_state
        
        # Handle new parameters without mappings
        self._initialize_new_parameters(
            new_model,
            new_optimizer,
            new_state,
            parameter_mappings,
            old_state_dict
        )
        
        # Apply preserved state to new optimizer
        self._apply_state_to_optimizer(new_optimizer, new_state)
        
        # [C-016 FIX] Register new optimizer with UnifiedLRController
        primary_config = GroupConfig(
            name="primary",
            policy=LRPolicy.HOST_COSINE,
            base_lr=old_params['lr'],
            group_id="host_model"
        )
        self.unified_lr_controller.register_optimizer("primary", new_optimizer, primary_config)
        
        # Validate state preservation
        self._validate_state_preservation(new_optimizer, old_optimizer)
        
        return new_optimizer
```

### Momentum Tensor Transformation

```python
def _transform_momentum_tensor(
    self,
    old_tensor: torch.Tensor,
    mapping: ParameterMapping,
    momentum_type: str
) -> torch.Tensor:
    """
    Transform a momentum tensor based on shape changes.
    
    Mathematical formulation for expansion [n, m] â†’ [n, m+k]:
    - M_new[:, :m] = M_old  # Preserve old momentum
    - M_new[:, m:] = interpolate(M_old)  # Initialize new dimensions
    """
    old_shape = mapping.old_shape
    new_shape = mapping.new_shape
    
    if mapping.mapping_type == 'identity':
        return old_tensor.clone()
        
    elif mapping.mapping_type == 'expand':
        # Expand tensor dimensions
        new_tensor = torch.zeros(new_shape, dtype=old_tensor.dtype, device=old_tensor.device)
        
        # Copy old values to corresponding positions
        slices = tuple(slice(0, min(o, n)) for o, n in zip(old_shape, new_shape))
        new_tensor[slices] = old_tensor[slices]
        
        # Initialize expanded regions based on momentum type
        for dim, (old_size, new_size) in enumerate(zip(old_shape, new_shape)):
            if new_size > old_size:
                if momentum_type == 'first_moment':
                    # Use mean of existing momentum
                    init_value = old_tensor.mean(dim=dim, keepdim=True)
                elif momentum_type == 'second_moment':
                    # Use mean of second moments (variance-like)
                    init_value = old_tensor.mean(dim=dim, keepdim=True)
                else:  # momentum
                    # Initialize with small values
                    init_value = old_tensor.mean(dim=dim, keepdim=True) * 0.1
                
                # Apply initialization to expanded region
                idx = [slice(None)] * len(new_shape)
                idx[dim] = slice(old_size, new_size)
                new_tensor[tuple(idx)] = init_value.expand_as(new_tensor[tuple(idx)])
        
        return new_tensor
        
    elif mapping.mapping_type == 'contract':
        # Contract tensor dimensions
        slices = tuple(slice(0, n) for n in new_shape)
        return old_tensor[slices].clone()
        
    else:  # reshape
        # Complex reshape - use interpolation
        return self._interpolate_momentum(old_tensor, new_shape, momentum_type)
```

## UnifiedLRController Design

### [C-016 FIX] Exclusive Learning Rate Authority

**Complete implementation with exclusive mutation rights:**

```python
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import torch
from time import perf_counter

class LRPolicy(Enum):
    HOST_COSINE = "host_cosine"          # Primary host model
    SEED_WARMUP = "seed_warmup"          # New seeds need warmup
    MORPHO_ADAPTIVE = "morpho_adaptive"  # Morphogenetic groups
    FROZEN = "frozen"                     # LR=0, no updates

@dataclass
class GroupConfig:
    name: str
    policy: LRPolicy
    base_lr: float
    warmup_epochs: int = 0
    min_lr: float = 1e-6
    max_lr: Optional[float] = None
    warmup_factor: float = 0.1
    warmup_type: str = "linear"
    # [C-016 FIX] Stable group identification
    group_id: str = None  # Stable identifier for fingerprinting

@dataclass 
class LRSnapshot:
    epoch: int
    step: int
    group_lrs: Dict[str, float]
    checksums: Dict[str, str]
    timestamp: float

class LRIntegrityError(Exception):
    """Raised when LR integrity violation is detected"""
    pass

class UnifiedLRController:
    """[C-016 FIX] Single source of truth for all learning rate management."""
    
    def __init__(self, 
                 tolaria_config: Dict[str, Any],
                 enable_runtime_guards: bool = True):
        self.config = tolaria_config
        self.enable_guards = enable_runtime_guards
        
        # Core state
        self.optimizers: Dict[str, torch.optim.Optimizer] = {}
        self.schedulers: Dict[str, torch.optim.lr_scheduler._LRScheduler] = {}
        self.group_configs: Dict[str, GroupConfig] = {}
        self.param_checksums: Dict[str, str] = {}
        
        # Tracking
        self.current_epoch = 0
        self.global_step = 0
        self.lr_history: List[LRSnapshot] = []
        
        # [C-016 FIX] Circuit breaker state instead of panic
        self.last_verified_lrs: Dict[str, Dict[int, float]] = {}
        self.circuit_breaker = CircuitBreaker(failure_threshold=3)
```

### Optimizer Registration

```python
def register_optimizer(self, 
                      name: str, 
                      optimizer: torch.optim.Optimizer,
                      config: GroupConfig) -> None:
    """[C-016 FIX] Enhanced registration with param group support"""
    if name in self.optimizers:
        raise ValueError(f"Optimizer {name} already registered")
        
    # [C-016 FIX] Support multiple param groups with stable IDs
    for idx, param_group in enumerate(optimizer.param_groups):
        group_id = f"{name}_group_{idx}"
        if config.group_id:
            group_id = f"{config.group_id}_{idx}"
            
        # Register each param group separately for fingerprinting
        self._register_param_group(name, idx, group_id, param_group)
        
    self.optimizers[name] = optimizer
    self.group_configs[name] = config
    
    # Create appropriate scheduler based on policy
    scheduler = self._create_scheduler(optimizer, config)
    self.schedulers[name] = scheduler
    
    # Initialize runtime guard state
    self._snapshot_lr_state(name, optimizer)
    self._compute_param_checksums(name, optimizer)
```

### The ONLY Learning Rate Mutation Point

```python
def step(self, 
        epoch: int,
        metrics: Dict[str, float]) -> Dict[str, float]:
    """[C-016 FIX] The ONLY learning rate mutation point in the system."""
    
    start_time = perf_counter()
    
    # [C-016 FIX] Circuit breaker verification instead of assert
    if self.enable_guards and self.circuit_breaker.is_closed():
        try:
            self._verify_lr_integrity()
        except LRIntegrityError as e:
            self.circuit_breaker.record_failure()
            self._trigger_conservative_mode(str(e))
            # Continue with fallback behavior instead of crashing
    
    self.current_epoch = epoch
    self.global_step += 1
    
    applied_lrs = {}
    
    for name, optimizer in self.optimizers.items():
        config = self.group_configs[name]
        scheduler = self.schedulers[name]
        
        # Apply scheduler step based on policy
        if config.policy == LRPolicy.MORPHO_ADAPTIVE:
            scheduler.step(metrics.get('loss', 0.0))
        elif config.policy != LRPolicy.FROZEN:
            scheduler.step()
        
        # Record applied LR
        current_lr = optimizer.param_groups[0]['lr']
        applied_lrs[name] = current_lr
        
        # Update guard state
        if self.enable_guards:
            self._snapshot_lr_state(name, optimizer)
    
    # Record history
    snapshot = LRSnapshot(
        epoch=epoch,
        step=self.global_step,
        group_lrs=applied_lrs.copy(),
        checksums=self._get_current_checksums(),
        timestamp=time.time()
    )
    self.lr_history.append(snapshot)
    
    # [C-016 FIX] Timing budget check with circuit breaker
    MonotonicTimer.within_budget_ms(
        start_time, 
        5.0,  # 5ms budget for LR operations
        lambda d: self._handle_timing_violation("lr_step", d)
    )
    
    return applied_lrs
```

### Runtime Integrity Verification

```python
def _verify_lr_integrity(self) -> None:
    """[C-016 FIX] Runtime guard with relative epsilon tolerance"""
    for name, optimizer in self.optimizers.items():
        if name not in self.last_verified_lrs:
            continue
            
        for idx, param_group in enumerate(optimizer.param_groups):
            last_lr = self.last_verified_lrs[name].get(idx)
            current_lr = param_group['lr']
            
            if last_lr is not None and self._lr_changed(last_lr, current_lr):
                error_msg = (
                    f"LR INTEGRITY VIOLATION: Learning rate for {name}[{idx}] "
                    f"changed from {last_lr} to {current_lr} outside UnifiedLRController! "
                    f"This violates invariant L1."
                )
                raise LRIntegrityError(error_msg)
                
def _lr_changed(self, a: float, b: float) -> bool:
    """[C-016 FIX] Relative epsilon tolerance to prevent false positives"""
    eps = max(1e-12, 1e-6 * max(abs(a), abs(b)))
    return abs(a - b) > eps
    
def _trigger_conservative_mode(self, reason: str) -> None:
    """[C-016 FIX] Conservative mode instead of system crash"""
    logging.warning(f"Entering conservative mode: {reason}")
    # Implement conservative policies:
    # - Disable experimental features
    # - Reduce telemetry sampling
    # - Extend timeouts
    self.metrics.conservative_mode_triggered.inc()
```

### Morphogenetic Parameter Addition

```python
def add_new_parameters(
    self,
    optimizer: torch.optim.Optimizer,
    new_params: List[torch.nn.Parameter],
    parent_group_name: Optional[str] = None
):
    """
    [C-016 FIX] Handle new parameters added during morphogenetic operations.
    New parameters get differential learning rates with warmup.
    """
    # Create new parameter group
    group_name = f"morpho_group_{len(self.group_configs)}"
    
    # Configure differential learning rate
    base_lr = self.group_configs.get('primary', GroupConfig(
        name='primary', 
        policy=LRPolicy.HOST_COSINE, 
        base_lr=1e-4
    )).base_lr
    
    new_config = GroupConfig(
        name=group_name,
        policy=LRPolicy.MORPHO_ADAPTIVE,
        base_lr=base_lr * 0.1,  # 10% of base LR for new params
        warmup_epochs=10,  # Longer warmup for new params
        warmup_factor=0.01,  # Start very small
        group_id=f"morpho_{group_name}"
    )
    
    # Add to optimizer with differential LR
    optimizer.add_param_group({
        'params': new_params,
        'lr': new_config.base_lr * new_config.warmup_factor,
        'group_name': group_name
    })
    
    # Create scheduler for new group
    scheduler = self._create_scheduler_for_group(optimizer, new_config, len(optimizer.param_groups) - 1)
    self.schedulers[group_name] = scheduler
    self.group_configs[group_name] = new_config
    
    # Update guard state
    if self.enable_guards:
        self._snapshot_lr_state(group_name, optimizer)
```

## Learning Rate Coordination (DEPRECATED)

**[C-016 FIX] This section has been deprecated:**

All learning rate coordination is now handled exclusively by the UnifiedLRController. The previous LearningRateCoordinator violated the exclusive LR mutation invariant and has been replaced.

### Key Changes:
- `scheduler.step()` calls removed from training loop
- All LR updates go through `UnifiedLRController.step()`
- Multiple schedulers managed internally by UnifiedLRController
- Parameter group registration enforced at controller level

## Critical Design Invariants

### Invariant L1: Exclusive LR Mutation
**Only UnifiedLRController can modify learning rates**
- No direct `optimizer.param_groups[i]['lr'] = value` allowed
- No external `scheduler.step()` calls permitted
- All LR changes must go through `UnifiedLRController.step()`

### Invariant L2: State Preservation
**Optimizer state must survive morphogenetic operations**
- Momentum preserved across architecture changes
- Parameter mappings maintained through rebuilds
- New parameters get differential learning rates

### Invariant L3: Runtime Verification
**Learning rate integrity checked at runtime**
- Circuit breakers prevent crashes
- Conservative mode on violations
- Relative epsilon tolerance for floating point

## Performance Requirements

### Timing Budgets
| Operation | Budget | Critical |
|-----------|--------|----------|
| LR Step | 5ms | Yes |
| State Preservation | 100ms | No |
| Integrity Check | 2ms | Yes |
| Parameter Addition | 20ms | No |

### Memory Constraints
- LR history limited to 1000 snapshots
- Parameter checksums cached for fast verification
- State preservation uses copy-on-write when possible

## Integration Contract

### Dependencies
- **PyTorch Optimizers**: Adam, SGD, RMSprop supported
- **Circuit Breakers**: For integrity violations
- **MonotonicTimer**: For timing validation

### Outputs
- **Applied LRs**: Dictionary of current learning rates
- **LR History**: Snapshots for debugging
- **Conservative Mode**: Triggered on violations

## C-016 Safety Enhancements

1. **Exclusive Mutation**: Only UnifiedLRController can change LRs
2. **Circuit Breakers**: Replace assert statements
3. **Conservative Mode**: Graceful degradation
4. **Runtime Guards**: Continuous integrity verification
5. **Relative Epsilon**: Prevent floating point false positives

## References

- Parent: [01-tolaria-unified-design.md](01-tolaria-unified-design.md)
- Related: [01.1-tolaria-epoch-lifecycle.md](01.1-tolaria-epoch-lifecycle.md) for training loop
- Related: [01.2-tolaria-rollback-systems.md](01.2-tolaria-rollback-systems.md) for checkpoint recovery
- Integration: [01.4-tolaria-integration-protocols.md](01.4-tolaria-integration-protocols.md) for protocols