# Jace - Testing Frameworks

**Parent Document**: [11-jace-unified-design.md](11-jace-unified-design.md)
**Component Type**: Testing Framework
**Version**: 3.0
**Leyline Integration**: COMPLETE - Integrated with Leyline (shared contracts) per Option B

---

## Overview

This document specifies the comprehensive testing framework for Jace curriculum coordination, focusing on chaos engineering and property-based testing to ensure reliability under all failure scenarios. The framework validates coordination consistency, tests resilience to curriculum disruptions, and provides mathematical guarantees for coordination behavior. All testing integrates with Leyline (shared contracts) for consistent message handling.

## 1. Chaos Engineering Framework

### 1.1 Curriculum Disruption Testing

```python
import pytest
import asyncio
import random
from typing import List, Dict, Any
from dataclasses import dataclass
from enum import Enum

# Import from Leyline (shared contracts)
from esper.leyline.contracts import (
    SystemStatePacket,
    AdaptationCommand,
    EventEnvelope,
    TelemetryPacket,
    CircuitBreakerState,
    HealthStatus,
    MessagePriority
)
from esper.leyline.version import SchemaVersion

class ChaosScenario(Enum):
    """Chaos engineering scenarios for curriculum coordination"""
    CURRICULUM_STALL = "curriculum_stall"           # One curriculum stops advancing
    COMPATIBILITY_CORRUPTION = "compat_corruption"  # Compatibility matrix corrupted
    COORDINATION_TIMEOUT = "coordination_timeout"   # Coordination takes too long
    STAGE_TRANSITION_FAILURE = "stage_failure"     # Stage transitions fail
    STRATEGY_UNAVAILABLE = "strategy_unavailable"  # Recommended strategies missing
    MEMORY_PRESSURE = "memory_pressure"            # High memory usage
    NETWORK_PARTITION = "network_partition"        # Cannot reach other subsystems

@dataclass
class ChaosExperiment:
    """Chaos experiment configuration"""
    scenario: ChaosScenario
    duration_seconds: int
    intensity: float  # 0.0 to 1.0
    recovery_time_seconds: int
    expected_behavior: str

class CurriculumChaosFramework:
    """[C-016] Chaos engineering for curriculum coordination with Leyline integration"""

    def __init__(self, coordinator: JaceCoordinationManager):
        self.coordinator = coordinator
        self.original_state = None
        self.chaos_active = False

    async def execute_chaos_experiment(self, experiment: ChaosExperiment) -> Dict[str, Any]:
        """Execute chaos experiment and validate recovery"""

        results = {
            "scenario": experiment.scenario.value,
            "success": False,
            "recovery_time_seconds": 0,
            "errors": [],
            "metrics": {}
        }

        # Store original state
        self.original_state = self._capture_system_state()

        try:
            # Inject chaos
            await self._inject_chaos(experiment)

            # Monitor system behavior during chaos
            chaos_metrics = await self._monitor_chaos_behavior(experiment)
            results["metrics"]["chaos_period"] = chaos_metrics

            # Stop chaos injection
            await self._stop_chaos_injection()

            # Measure recovery time
            recovery_start = time.time()
            recovered = await self._wait_for_recovery(experiment.recovery_time_seconds)
            recovery_time = time.time() - recovery_start

            results["recovery_time_seconds"] = recovery_time
            results["success"] = recovered

            # Validate post-recovery state
            if recovered:
                validation_results = self._validate_post_recovery_state()
                results["metrics"]["post_recovery"] = validation_results
                results["success"] = validation_results["system_healthy"]

        except Exception as e:
            results["errors"].append(str(e))
            logging.error(f"Chaos experiment failed: {e}")

        finally:
            # Ensure cleanup
            await self._cleanup_chaos_state()

        return results

    async def _inject_chaos(self, experiment: ChaosExperiment):
        """Inject specific chaos scenario"""

        self.chaos_active = True

        if experiment.scenario == ChaosScenario.CURRICULUM_STALL:
            await self._inject_curriculum_stall(experiment.intensity)

        elif experiment.scenario == ChaosScenario.COMPATIBILITY_CORRUPTION:
            await self._inject_compatibility_corruption(experiment.intensity)

        elif experiment.scenario == ChaosScenario.COORDINATION_TIMEOUT:
            await self._inject_coordination_delays(experiment.intensity)

        elif experiment.scenario == ChaosScenario.STAGE_TRANSITION_FAILURE:
            await self._inject_transition_failures(experiment.intensity)

        elif experiment.scenario == ChaosScenario.STRATEGY_UNAVAILABLE:
            await self._inject_strategy_unavailability(experiment.intensity)

        elif experiment.scenario == ChaosScenario.MEMORY_PRESSURE:
            await self._inject_memory_pressure(experiment.intensity)

        elif experiment.scenario == ChaosScenario.NETWORK_PARTITION:
            await self._inject_network_partition(experiment.intensity)
```

### 1.2 Chaos Injection Implementations

```python
async def _inject_curriculum_stall(self, intensity: float):
    """Simulate curriculum advancement stall"""

    # Mock curriculum that stops advancing
    class StalledCurriculum:
        def __init__(self, original_curriculum, stall_probability: float):
            self.original = original_curriculum
            self.stall_prob = stall_probability

        async def get_next_stage(self, current_stage: int):
            if random.random() < self.stall_prob:
                # Stall - return current stage
                return current_stage
            return await self.original.get_next_stage(current_stage)

    # Replace curriculum with stalled version
    self.coordinator.task_curriculum = StalledCurriculum(
        self.coordinator.task_curriculum, intensity
    )

async def _inject_compatibility_corruption(self, intensity: float):
    """Simulate compatibility matrix corruption"""

    original_get_compatibility = self.coordinator.compatibility_matrix.get_compatibility

    def corrupted_compatibility(task_stage: int, strategy_stage: int) -> float:
        if random.random() < intensity:
            # Return invalid compatibility (negative or > 1.0)
            return random.uniform(-0.5, 1.5)
        return original_get_compatibility(task_stage, strategy_stage)

    self.coordinator.compatibility_matrix.get_compatibility = corrupted_compatibility

async def _inject_coordination_delays(self, intensity: float):
    """Simulate coordination timeouts"""

    original_optimize = self.coordinator._optimize_coordination_internal

    async def delayed_coordination(*args, **kwargs):
        # Random delay based on intensity
        delay_ms = intensity * 100  # Up to 100ms delay
        await asyncio.sleep(delay_ms / 1000.0)
        return original_optimize(*args, **kwargs)

    self.coordinator._optimize_coordination_internal = delayed_coordination

async def _monitor_chaos_behavior(self, experiment: ChaosExperiment) -> Dict[str, Any]:
    """Monitor system behavior during chaos using Leyline contracts"""

    metrics = {
        "coordination_decisions": 0,
        "fallback_decisions": 0,
        "circuit_breaker_trips": 0,
        "conservative_mode_activations": 0,
        "average_decision_time_ms": 0,
        "errors": []
    }

    # Monitor for experiment duration
    start_time = time.time()
    decision_times = []

    while time.time() - start_time < experiment.duration_seconds:
        try:
            # Generate test coordination request using Leyline SystemStatePacket
            test_state = self._create_test_system_state_packet()

            decision_start = time.time()
            decision = self.coordinator.optimize_coordination(test_state)
            decision_time = (time.time() - decision_start) * 1000

            decision_times.append(decision_time)
            metrics["coordination_decisions"] += 1

            if decision.fallback:
                metrics["fallback_decisions"] += 1
            if decision.conservative_mode:
                metrics["conservative_mode_activations"] += 1

            # Check circuit breaker state using Leyline enum
            if decision.circuit_breaker_state != CircuitBreakerState.BREAKER_CLOSED:
                metrics["circuit_breaker_trips"] += 1

        except Exception as e:
            metrics["errors"].append(str(e))

        await asyncio.sleep(0.1)  # 100ms interval

    if decision_times:
        metrics["average_decision_time_ms"] = sum(decision_times) / len(decision_times)

    return metrics

def _create_test_system_state_packet(self) -> SystemStatePacket:
    """Create test SystemStatePacket using Leyline contract"""

    # Use Leyline's native map support for training metrics
    training_metrics = {
        "task_stage": float(random.randint(0, 6)),
        "strategy_stage": float(random.randint(0, 6)),
        "performance_score": random.uniform(0.1, 0.9)
    }

    return SystemStatePacket(
        version=SchemaVersion.CURRENT_VERSION,
        current_epoch=random.randint(1, 1000),
        validation_accuracy=random.uniform(0.1, 0.9),
        validation_loss=random.uniform(0.1, 0.9),
        timestamp_ns=int(time.time() * 1_000_000_000),
        training_metrics=training_metrics,
        packet_id=str(uuid.uuid4()),
        source_subsystem="jace_test",
        global_step=random.randint(1000, 100000)
    )
```

### 1.3 Chaos Test Suite

```python
@pytest.mark.chaos
class TestCurriculumCoordinationChaos:
    """[C-016] Chaos engineering test suite with Leyline integration"""

    @pytest.fixture
    def chaos_framework(self):
        """Create chaos testing framework"""
        coordinator = JaceCoordinationManager({})
        return CurriculumChaosFramework(coordinator)

    @pytest.mark.asyncio
    async def test_curriculum_stall_resilience(self, chaos_framework):
        """Test resilience to curriculum advancement stall"""

        experiment = ChaosExperiment(
            scenario=ChaosScenario.CURRICULUM_STALL,
            duration_seconds=30,
            intensity=0.8,  # 80% stall probability
            recovery_time_seconds=10,
            expected_behavior="Should fallback to independent mode and recover"
        )

        results = await chaos_framework.execute_chaos_experiment(experiment)

        # Validate results
        assert results["success"], f"Chaos experiment failed: {results['errors']}"
        assert results["recovery_time_seconds"] < 10, "Recovery took too long"
        assert results["metrics"]["chaos_period"]["fallback_decisions"] > 0, "No fallback occurred"

    @pytest.mark.asyncio
    async def test_compatibility_corruption_resilience(self, chaos_framework):
        """Test resilience to compatibility matrix corruption"""

        experiment = ChaosExperiment(
            scenario=ChaosScenario.COMPATIBILITY_CORRUPTION,
            duration_seconds=20,
            intensity=0.5,  # 50% corruption rate
            recovery_time_seconds=5,
            expected_behavior="Should detect corruption and use conservative compatibility"
        )

        results = await chaos_framework.execute_chaos_experiment(experiment)

        assert results["success"], "Failed to handle compatibility corruption"
        assert results["metrics"]["chaos_period"]["conservative_mode_activations"] > 0

    @pytest.mark.asyncio
    async def test_coordination_timeout_resilience(self, chaos_framework):
        """Test resilience to coordination timeouts"""

        experiment = ChaosExperiment(
            scenario=ChaosScenario.COORDINATION_TIMEOUT,
            duration_seconds=25,
            intensity=0.7,  # High delay intensity
            recovery_time_seconds=8,
            expected_behavior="Should trigger circuit breaker and use fallback coordination"
        )

        results = await chaos_framework.execute_chaos_experiment(experiment)

        assert results["success"], "Failed to handle coordination timeouts"
        assert results["metrics"]["chaos_period"]["circuit_breaker_trips"] > 0
        assert results["metrics"]["chaos_period"]["average_decision_time_ms"] < 50  # Should use fast fallback

    @pytest.mark.asyncio
    async def test_memory_pressure_resilience(self, chaos_framework):
        """Test resilience to memory pressure scenarios"""

        experiment = ChaosExperiment(
            scenario=ChaosScenario.MEMORY_PRESSURE,
            duration_seconds=15,
            intensity=0.9,  # High memory pressure
            recovery_time_seconds=5,
            expected_behavior="Should trigger garbage collection and conservative mode"
        )

        results = await chaos_framework.execute_chaos_experiment(experiment)

        assert results["success"], "Failed to handle memory pressure"
        assert results["metrics"]["chaos_period"]["conservative_mode_activations"] > 0

    @pytest.mark.asyncio
    async def test_network_partition_resilience(self, chaos_framework):
        """Test resilience to network partitions"""

        experiment = ChaosExperiment(
            scenario=ChaosScenario.NETWORK_PARTITION,
            duration_seconds=20,
            intensity=0.6,  # 60% network failure rate
            recovery_time_seconds=10,
            expected_behavior="Should continue with cached decisions and fallback coordination"
        )

        results = await chaos_framework.execute_chaos_experiment(experiment)

        assert results["success"], "Failed to handle network partition"
        assert results["metrics"]["chaos_period"]["fallback_decisions"] > 0

    @pytest.mark.asyncio
    async def test_leyline_message_corruption(self, chaos_framework):
        """Test resilience to Leyline message corruption"""

        experiment = ChaosExperiment(
            scenario=ChaosScenario.NETWORK_PARTITION,  # Reuse for message corruption
            duration_seconds=15,
            intensity=0.4,  # 40% message corruption
            recovery_time_seconds=5,
            expected_behavior="Should validate Leyline messages and use fallback for corrupted data"
        )

        # Inject message corruption specific to Leyline contracts
        original_deserialize = SystemStatePacket.FromString

        def corrupted_deserialize(data):
            if random.random() < experiment.intensity:
                # Corrupt the serialized data
                corrupted_data = bytearray(data)
                if len(corrupted_data) > 4:
                    corrupted_data[4] = 255  # Corrupt version field
                return original_deserialize(bytes(corrupted_data))
            return original_deserialize(data)

        SystemStatePacket.FromString = corrupted_deserialize

        try:
            results = await chaos_framework.execute_chaos_experiment(experiment)
            assert results["success"], "Failed to handle Leyline message corruption"
            assert results["metrics"]["chaos_period"]["fallback_decisions"] > 0
        finally:
            # Restore original deserializer
            SystemStatePacket.FromString = original_deserialize
```

## 2. Property-Based Testing Framework

### 2.1 Coordination Property Tests

```python
import hypothesis
from hypothesis import strategies as st
from hypothesis.stateful import RuleBasedStateMachine, rule, invariant
from typing import List, Tuple
import pytest

# Import from Leyline (shared contracts)
from esper.leyline.contracts import SystemStatePacket, CircuitBreakerState
from esper.leyline.version import SchemaVersion

class CoordinationPropertyTests:
    """[C-016] Property-based testing for coordination guarantees with Leyline integration"""

    @hypothesis.given(
        task_stages=st.lists(
            st.integers(min_value=0, max_value=6),
            min_size=1,
            max_size=20
        ),
        strategy_stages=st.lists(
            st.integers(min_value=0, max_value=6),
            min_size=1,
            max_size=20
        )
    )
    def test_compatibility_matrix_symmetry(self, task_stages: List[int], strategy_stages: List[int]):
        """Compatibility matrix should have consistent properties"""

        matrix = CurriculumCompatibilityMatrix()

        for task_stage in task_stages:
            for strategy_stage in strategy_stages:
                # Property 1: Compatibility scores are bounded [0, 1]
                score = matrix.get_compatibility(task_stage, strategy_stage)
                assert 0.0 <= score <= 1.0, f"Compatibility score {score} out of bounds"

                # Property 2: Diagonal should have high compatibility (same levels)
                if task_stage == strategy_stage:
                    assert score >= 0.5, f"Diagonal compatibility {score} too low"

                # Property 3: Extreme combinations should have low compatibility
                if abs(task_stage - strategy_stage) > 3:
                    assert score <= 0.3, f"Extreme combination compatibility {score} too high"

    @hypothesis.given(
        trajectories=st.lists(
            st.lists(
                st.tuples(
                    st.integers(min_value=0, max_value=6),  # task_stage
                    st.integers(min_value=0, max_value=6)   # strategy_stage
                ),
                min_size=2,
                max_size=10
            ),
            min_size=1,
            max_size=5
        )
    )
    def test_trajectory_cost_monotonicity(self, trajectories: List[List[Tuple[int, int]]]):
        """Trajectory costs should increase with complexity and jumps"""

        matrix = CurriculumCompatibilityMatrix()

        for trajectory in trajectories:
            if len(trajectory) < 2:
                continue

            cost = matrix.calculate_trajectory_cost(trajectory)

            # Property 1: Cost should be non-negative
            assert cost >= 0.0, f"Trajectory cost {cost} is negative"

            # Property 2: Longer trajectories should not have lower cost
            if len(trajectory) > 2:
                shorter_trajectory = trajectory[:-1]
                shorter_cost = matrix.calculate_trajectory_cost(shorter_trajectory)
                assert cost >= shorter_cost * 0.8, "Longer trajectory has much lower cost"

            # Property 3: Identity trajectory should have zero cost
            start_point = trajectory[0]
            identity_trajectory = [start_point, start_point]
            identity_cost = matrix.calculate_trajectory_cost(identity_trajectory)
            assert identity_cost == 0.0, f"Identity trajectory cost {identity_cost} != 0"

    @hypothesis.given(
        system_states=st.lists(
            st.builds(
                lambda epoch, accuracy, loss: SystemStatePacket(
                    version=SchemaVersion.CURRENT_VERSION,
                    current_epoch=epoch,
                    validation_accuracy=accuracy,
                    validation_loss=loss,
                    timestamp_ns=int(time.time() * 1_000_000_000),
                    training_metrics={
                        "task_stage": float(random.randint(0, 6)),
                        "strategy_stage": float(random.randint(0, 6)),
                        "performance_score": accuracy
                    },
                    packet_id=str(uuid.uuid4()),
                    source_subsystem="test"
                ),
                epoch=st.integers(min_value=1, max_value=1000),
                accuracy=st.floats(min_value=0.0, max_value=1.0),
                loss=st.floats(min_value=0.0, max_value=1.0)
            ),
            min_size=1,
            max_size=10
        )
    )
    def test_coordination_decision_consistency(self, system_states: List[SystemStatePacket]):
        """Coordination decisions should be consistent and safe with Leyline contracts"""

        coordinator = JaceCoordinationManager({})

        for system_state in system_states:
            # Property 1: Decision should always be returned (no exceptions)
            try:
                decision = coordinator.optimize_coordination(system_state)
                assert decision is not None, "Coordination returned None"
            except Exception as e:
                pytest.fail(f"Coordination threw exception: {e}")

            # Property 2: Decision confidence should be bounded
            assert 0.0 <= decision.confidence <= 1.0, f"Confidence {decision.confidence} out of bounds"

            # Property 3: Recommended strategies should not be empty
            assert len(decision.recommended_strategies) > 0, "No strategies recommended"

            # Property 4: Timing budget should be realistic
            assert 5.0 <= decision.timing_budget_ms <= 50.0, f"Unrealistic timing budget {decision.timing_budget_ms}ms"

            # Property 5: Fallback decisions should have conservative properties
            if decision.fallback:
                assert decision.mode == CoordinationMode.SYNCHRONIZED, "Fallback not using safest mode"
                assert decision.confidence >= 0.4, "Fallback confidence too low"

            # Property 6: Circuit breaker state should be valid Leyline enum
            assert isinstance(decision.circuit_breaker_state, CircuitBreakerState), "Invalid circuit breaker state type"

    @hypothesis.given(
        version_numbers=st.integers(min_value=1, max_value=100)
    )
    def test_leyline_version_compatibility(self, version_numbers: int):
        """Test version compatibility with Leyline schema versioning"""

        # Property 1: Valid versions should be accepted
        if version_numbers == SchemaVersion.CURRENT_VERSION:
            assert SchemaVersion.validate_version(version_numbers), "Current version should be valid"

        # Property 2: Invalid versions should be rejected gracefully
        if version_numbers != SchemaVersion.CURRENT_VERSION:
            # Should handle gracefully, not crash
            try:
                is_valid = SchemaVersion.validate_version(version_numbers)
                assert isinstance(is_valid, bool), "Version validation should return boolean"
            except Exception as e:
                pytest.fail(f"Version validation should not throw exception: {e}")
```

### 2.2 Stateful Property Testing

```python
class CoordinationStateMachine(RuleBasedStateMachine):
    """[C-016] Stateful property testing for coordination lifecycle with Leyline contracts"""

    def __init__(self):
        super().__init__()
        self.coordinator = JaceCoordinationManager({})
        self.current_system_state = self._create_initial_system_state()
        self.decision_history = []

    def _create_initial_system_state(self) -> SystemStatePacket:
        """Create initial SystemStatePacket using Leyline contract"""
        return SystemStatePacket(
            version=SchemaVersion.CURRENT_VERSION,
            current_epoch=1,
            validation_accuracy=0.5,
            validation_loss=0.5,
            timestamp_ns=int(time.time() * 1_000_000_000),
            training_metrics={
                "task_stage": 0.0,
                "strategy_stage": 0.0,
                "performance_score": 0.5
            },
            packet_id=str(uuid.uuid4()),
            source_subsystem="test_state_machine"
        )

    @rule(
        epoch_delta=st.integers(min_value=1, max_value=10),
        accuracy_delta=st.floats(min_value=-0.1, max_value=0.1),
        task_stage_delta=st.integers(min_value=-1, max_value=1),
        strategy_stage_delta=st.integers(min_value=-1, max_value=1)
    )
    def advance_curriculum(self, epoch_delta: int, accuracy_delta: float,
                          task_stage_delta: int, strategy_stage_delta: int):
        """Advance curriculum state and get coordination decision"""

        # Extract current metrics from Leyline native map
        current_metrics = self.current_system_state.training_metrics
        current_task_stage = int(current_metrics.get("task_stage", 0))
        current_strategy_stage = int(current_metrics.get("strategy_stage", 0))

        # Update state within bounds
        new_epoch = self.current_system_state.current_epoch + epoch_delta
        new_accuracy = max(0.0, min(1.0, self.current_system_state.validation_accuracy + accuracy_delta))
        new_task_stage = max(0, min(6, current_task_stage + task_stage_delta))
        new_strategy_stage = max(0, min(6, current_strategy_stage + strategy_stage_delta))

        # Create new SystemStatePacket using Leyline contract
        self.current_system_state = SystemStatePacket(
            version=SchemaVersion.CURRENT_VERSION,
            current_epoch=new_epoch,
            validation_accuracy=new_accuracy,
            validation_loss=1.0 - new_accuracy,
            timestamp_ns=int(time.time() * 1_000_000_000),
            training_metrics={
                "task_stage": float(new_task_stage),
                "strategy_stage": float(new_strategy_stage),
                "performance_score": new_accuracy
            },
            packet_id=str(uuid.uuid4()),
            source_subsystem="test_state_machine"
        )

        # Get coordination decision
        decision = self.coordinator.optimize_coordination(self.current_system_state)
        self.decision_history.append((self.current_system_state, decision))

    @rule()
    def trigger_conservative_mode(self):
        """Trigger conservative mode and verify behavior"""

        # Simulate high load by making many rapid requests
        for _ in range(10):
            decision = self.coordinator.optimize_coordination(self.current_system_state)
            if decision.conservative_mode:
                break

    @invariant()
    def coordination_always_responds(self):
        """Coordination should always provide a response"""

        if self.decision_history:
            latest_state, latest_decision = self.decision_history[-1]
            assert latest_decision is not None, "No coordination decision provided"
            assert latest_decision.is_safe_to_apply(), "Unsafe decision provided"

    @invariant()
    def conservative_mode_is_safe(self):
        """Conservative mode decisions should always be safe"""

        for state, decision in self.decision_history:
            if decision.conservative_mode:
                assert decision.mode == CoordinationMode.SYNCHRONIZED, "Conservative mode not using safest coordination"
                assert len(decision.recommended_strategies) > 0, "Conservative mode provided no strategies"
                assert decision.confidence >= 0.4, "Conservative mode confidence too low"

    @invariant()
    def leyline_contract_consistency(self):
        """SystemStatePacket should maintain Leyline contract consistency"""

        for state, decision in self.decision_history:
            # Verify SystemStatePacket has valid Leyline structure
            assert state.version == SchemaVersion.CURRENT_VERSION, "Invalid schema version"
            assert isinstance(state.training_metrics, dict), "Training metrics should be native map"
            assert state.timestamp_ns > 0, "Invalid timestamp"
            assert state.packet_id, "Missing packet ID"

            # Verify decision uses Leyline enum types
            assert isinstance(decision.circuit_breaker_state, CircuitBreakerState), "Invalid circuit breaker state type"
```

### 2.3 Property Test Suite

```python
@pytest.mark.property
class TestCoordinationProperties:
    """Property-based test suite for coordination guarantees with Leyline integration"""

    def test_compatibility_properties(self):
        """Test compatibility matrix properties"""
        test_instance = CoordinationPropertyTests()

        # Run hypothesis test with custom settings
        hypothesis.settings(max_examples=100, deadline=None)(
            test_instance.test_compatibility_matrix_symmetry
        )()

    def test_trajectory_properties(self):
        """Test trajectory cost properties"""
        test_instance = CoordinationPropertyTests()

        hypothesis.settings(max_examples=100, deadline=None)(
            test_instance.test_trajectory_cost_monotonicity
        )()

    def test_decision_properties(self):
        """Test coordination decision properties"""
        test_instance = CoordinationPropertyTests()

        hypothesis.settings(max_examples=200, deadline=None)(
            test_instance.test_coordination_decision_consistency
        )()

    def test_leyline_version_properties(self):
        """Test Leyline version compatibility properties"""
        test_instance = CoordinationPropertyTests()

        hypothesis.settings(max_examples=50, deadline=None)(
            test_instance.test_leyline_version_compatibility
        )()

    def test_stateful_coordination_properties(self):
        """Test stateful coordination properties"""
        # Run the state machine for multiple rounds
        state_machine_test = CoordinationStateMachine.TestCase()
        state_machine_test.runTest()

    @hypothesis.given(
        compatibility_threshold=st.floats(min_value=0.1, max_value=0.9),
        timing_budget=st.floats(min_value=10.0, max_value=50.0),
        message_size_limit=st.integers(min_value=200, max_value=300)
    )
    def test_configuration_robustness(self, compatibility_threshold: float, timing_budget: float, message_size_limit: int):
        """Test robustness across different configuration parameters"""

        config = JaceConfig()
        config.compatibility_threshold = compatibility_threshold
        config.hardware_timings[HardwareProfile.A100_4X].coordination_budget_ms = int(timing_budget)
        config.max_message_size_bytes = message_size_limit  # From Leyline limits

        coordinator = JaceCoordinationManager(config)
        test_state = SystemStatePacket(
            version=SchemaVersion.CURRENT_VERSION,
            current_epoch=100,
            validation_accuracy=0.6,
            validation_loss=0.4,
            timestamp_ns=int(time.time() * 1_000_000_000),
            training_metrics={
                "task_stage": 3.0,
                "strategy_stage": 3.0,
                "performance_score": 0.6
            },
            packet_id=str(uuid.uuid4()),
            source_subsystem="property_test"
        )

        # Should always produce valid decision regardless of configuration
        decision = coordinator.optimize_coordination(test_state)

        assert decision is not None
        assert decision.is_safe_to_apply()
        assert decision.timing_budget_ms <= timing_budget * 1.5  # Allow conservative mode extension

        # Verify Leyline message size constraints
        serialized_state = test_state.SerializeToString()
        assert len(serialized_state) <= message_size_limit, f"Message size {len(serialized_state)} exceeds limit {message_size_limit}"
```

## 3. Test Orchestration

### 3.1 Test Execution Framework

```python
class JaceTestOrchestrator:
    """Orchestrate comprehensive Jace testing with Leyline integration"""

    def __init__(self):
        self.chaos_framework = None
        self.property_tests = CoordinationPropertyTests()
        self.test_results = {}

    async def run_comprehensive_test_suite(self) -> Dict[str, Any]:
        """Run complete test suite with chaos and property tests"""

        results = {
            "test_suite_version": "3.0",
            "execution_time": time.time(),
            "leyline_integration": True,
            "chaos_tests": {},
            "property_tests": {},
            "integration_tests": {},
            "performance_tests": {},
            "leyline_contract_tests": {},
            "overall_success": False
        }

        try:
            # 1. Chaos Engineering Tests
            chaos_results = await self._run_chaos_test_suite()
            results["chaos_tests"] = chaos_results

            # 2. Property-Based Tests
            property_results = await self._run_property_test_suite()
            results["property_tests"] = property_results

            # 3. Integration Tests
            integration_results = await self._run_integration_test_suite()
            results["integration_tests"] = integration_results

            # 4. Performance Tests
            performance_results = await self._run_performance_test_suite()
            results["performance_tests"] = performance_results

            # 5. Leyline Contract Tests
            leyline_results = await self._run_leyline_contract_tests()
            results["leyline_contract_tests"] = leyline_results

            # Determine overall success
            results["overall_success"] = all([
                chaos_results["success"],
                property_results["success"],
                integration_results["success"],
                performance_results["success"],
                leyline_results["success"]
            ])

        except Exception as e:
            results["error"] = str(e)
            results["overall_success"] = False

        return results

    async def _run_chaos_test_suite(self) -> Dict[str, Any]:
        """Run all chaos engineering scenarios"""

        coordinator = JaceCoordinationManager({})
        chaos_framework = CurriculumChaosFramework(coordinator)

        scenarios = [
            ChaosExperiment(ChaosScenario.CURRICULUM_STALL, 30, 0.8, 10, "Stall resilience"),
            ChaosExperiment(ChaosScenario.COMPATIBILITY_CORRUPTION, 20, 0.5, 5, "Corruption resilience"),
            ChaosExperiment(ChaosScenario.COORDINATION_TIMEOUT, 25, 0.7, 8, "Timeout resilience"),
            ChaosExperiment(ChaosScenario.MEMORY_PRESSURE, 15, 0.9, 5, "Memory pressure resilience"),
            ChaosExperiment(ChaosScenario.NETWORK_PARTITION, 20, 0.6, 10, "Partition resilience")
        ]

        results = {"scenarios": {}, "success": True}

        for experiment in scenarios:
            scenario_result = await chaos_framework.execute_chaos_experiment(experiment)
            results["scenarios"][experiment.scenario.value] = scenario_result

            if not scenario_result["success"]:
                results["success"] = False

        return results

    async def _run_leyline_contract_tests(self) -> Dict[str, Any]:
        """Run Leyline-specific contract validation tests"""

        results = {"tests": {}, "success": True}

        # Test 1: SystemStatePacket serialization/deserialization
        try:
            test_packet = SystemStatePacket(
                version=SchemaVersion.CURRENT_VERSION,
                current_epoch=42,
                validation_accuracy=0.85,
                validation_loss=0.15,
                timestamp_ns=int(time.time() * 1_000_000_000),
                training_metrics={
                    "test_metric": 1.0,
                    "another_metric": 2.0
                },
                packet_id=str(uuid.uuid4()),
                source_subsystem="test"
            )

            # Serialize and deserialize
            serialized = test_packet.SerializeToString()
            deserialized = SystemStatePacket.FromString(serialized)

            # Validate size constraint from Leyline
            assert len(serialized) <= 280, f"Message size {len(serialized)} exceeds Leyline limit 280 bytes"

            # Validate field consistency
            assert deserialized.version == test_packet.version
            assert deserialized.current_epoch == test_packet.current_epoch
            assert deserialized.validation_accuracy == test_packet.validation_accuracy

            results["tests"]["systemstatepacket_serialization"] = {"success": True, "message_size": len(serialized)}

        except Exception as e:
            results["tests"]["systemstatepacket_serialization"] = {"success": False, "error": str(e)}
            results["success"] = False

        # Test 2: Version compatibility
        try:
            # Valid version
            assert SchemaVersion.validate_version(SchemaVersion.CURRENT_VERSION), "Current version should be valid"

            # Invalid version handling
            invalid_version_handled = True
            try:
                SchemaVersion.validate_version(999)
            except Exception:
                invalid_version_handled = False

            results["tests"]["version_compatibility"] = {
                "success": True,
                "current_version_valid": True,
                "invalid_version_handled_gracefully": invalid_version_handled
            }

        except Exception as e:
            results["tests"]["version_compatibility"] = {"success": False, "error": str(e)}
            results["success"] = False

        # Test 3: Native map performance
        try:
            import time

            # Create large training metrics map
            large_metrics = {f"metric_{i}": float(i) for i in range(100)}

            packet = SystemStatePacket(
                version=SchemaVersion.CURRENT_VERSION,
                current_epoch=1,
                validation_accuracy=0.5,
                validation_loss=0.5,
                timestamp_ns=int(time.time() * 1_000_000_000),
                training_metrics=large_metrics,
                packet_id=str(uuid.uuid4()),
                source_subsystem="performance_test"
            )

            # Measure serialization time
            start_time = time.perf_counter()
            serialized = packet.SerializeToString()
            serialization_time_us = (time.perf_counter() - start_time) * 1_000_000

            # Should meet Leyline performance target of <80μs
            performance_target_met = serialization_time_us < 80.0

            results["tests"]["native_map_performance"] = {
                "success": True,
                "serialization_time_us": serialization_time_us,
                "performance_target_met": performance_target_met,
                "message_size": len(serialized)
            }

            if not performance_target_met:
                results["success"] = False

        except Exception as e:
            results["tests"]["native_map_performance"] = {"success": False, "error": str(e)}
            results["success"] = False

        return results
```

## 4. Integration Contract

### 4.1 Testing Interface

The testing framework integrates with the main Jace system through:

- **ChaosExperiment Configuration**: Standardized experiment definitions with Leyline contract support
- **PropertyTest Specifications**: Mathematical property validation using Leyline messages
- **TestResult Schemas**: Consistent result reporting format with Leyline integration metrics
- **MetricsCollection Integration**: Test metrics feed into monitoring via Leyline TelemetryPacket

### 4.2 Continuous Testing

- **Automated Chaos Tests**: Run chaos scenarios in CI/CD pipeline with Leyline message validation
- **Property Test Integration**: Hypothesis tests as part of unit test suite with SystemStatePacket generation
- **Performance Regression Testing**: Coordination latency, throughput validation, and Leyline serialization performance
- **Integration Validation**: Cross-subsystem coordination testing via Leyline shared contracts

### 4.3 Leyline Integration Testing

- **Contract Compatibility**: Validate all Leyline shared contracts work correctly
- **Serialization Performance**: Ensure <80μs serialization target is met
- **Message Size Validation**: Verify all messages stay under 280-byte limit
- **Version Compatibility**: Test schema version handling and migration
- **Cross-Subsystem Messaging**: Integration tests with other subsystems via Leyline contracts

## 5. References

**Back to Main Document**: [11-jace-unified-design.md](11-jace-unified-design.md) - Core architecture and coordination logic
**Related Components**:
- [11.2-jace-circuit-breakers.md](11.2-jace-circuit-breakers.md) - Circuit breaker implementations tested by chaos framework
- [11.3-jace-slo-framework.md](11.3-jace-slo-framework.md) - SLO monitoring validated by performance tests

**Leyline Integration**:
- Leyline Shared Contracts: 00-leyline-shared-contracts.md - Complete shared contract specifications
- C-018 Round 7 Consensus: Option B (Performance-First) implementation requirements

**External References**:
- Hypothesis Property-Based Testing Documentation
- Chaos Engineering Principles and Practices
- Circuit Breaker Pattern Testing Strategies
- Curriculum Learning Robustness Testing