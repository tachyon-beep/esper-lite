# Karn - Generative AI System (Phase 2 Implementation)

## Document Metadata

| Field | Value |
|-------|-------|
| **Parent Document** | [05-karn-unified-design.md](05-karn-unified-design.md) |
| **Component Type** | Neural Generation System |
| **Version** | 3.1 |
| **Status** | FUTURE |
| **Implementation** | Future Phase 2 (3-Month Timeline) |

## Overview

The Generative AI System represents Karn's future Phase 2 implementation, introducing true neural architecture generation through a Graph-to-Graph (G2G) Transformer. This component will transition Karn from template-based generation to intelligent neural synthesis, learning from Phase 1 field reports to generate novel architectures with semantic validation and advanced safety controls using Leyline (shared contracts).

The system embodies the C-014 Conclave's vision for autonomous architectural evolution: a 4-layer heterogeneous GNN encoder paired with an autoregressive LSTM decoder, capable of understanding architectural semantics and generating novel structures. GPU resources are shared with Urabrask through time-multiplexed scheduling, achieving efficient utilization while maintaining safety validation capabilities.

Key characteristics:
- **G2G Transformer**: 4-layer heterogeneous GNN with 16-head attention mechanism
- **Neural Generation**: Autoregressive LSTM with beam search (width=5)
- **Semantic Mutations**: Intelligent modifications guided by architectural understanding
- **GPU Coordination**: 12-24GB shared allocation with Urabrask via time-multiplexing
- **Migration Strategy**: Shadow mode testing before full Phase 2 activation

## Technical Design

### Architecture

```
G2G Neural Generation Pipeline
├── Input Processing
│   ├── Parent Architecture Encoding
│   ├── Context Embedding
│   └── Hardware Context Analysis
├── G2G Transformer
│   ├── HeterogeneousGNN (4 layers)
│   │   ├── Node Types: [layer, connection, operation, parameter]
│   │   └── Edge Types: [dataflow, skip, attention, control]
│   ├── MultiHeadAttention (16 heads)
│   └── AutoregressiveLSTM (3 layers)
├── Semantic Mutation Engine
│   ├── Operation Embeddings
│   ├── Compatibility Matrix
│   └── Mutation Strategies
└── GPU Resource Coordinator
    ├── Allocation Scheduler
    ├── Memory Management
    └── Urabrask Coordination
```

### Core Abstractions

**G2GTransformer**
```python
# Import from Leyline (shared contracts)
from esper.leyline.contracts import (
    SystemStatePacket,
    AdaptationCommand,
    EventEnvelope,
    TelemetryPacket,
    HardwareContext,
    MessagePriority,
    SeedLifecycleStage,
    HealthStatus
)
from esper.leyline.version import SchemaVersion

class G2GTransformer:
    """Phase 2: Graph-to-Graph transformer with Leyline integration"""

    def __init__(self):
        # C-014 Specifications with Leyline safety
        self.circuit_breaker = CircuitBreaker(failure_threshold=5)  # Higher threshold for neural ops
        self.conservative_mode = ConservativeMode()

        self.encoder = HeterogeneousGNN(
            node_types=['layer', 'connection', 'operation', 'parameter'],
            edge_types=['dataflow', 'skip', 'attention', 'control'],
            num_layers=4,  # 4-layer GNN per C-014
            hidden_dim=512,
            dropout=0.1
        )

        self.decoder = AutoregressiveLSTM(
            num_layers=3,  # 3-layer LSTM per C-014
            hidden_dim=512,
            output_vocab_size=1000,
            max_nodes=100,
            beam_width=5  # Beam search generation
        )

        self.attention = MultiHeadAttention(
            num_heads=16,  # 16 attention heads per C-014
            hidden_dim=512,
            dropout=0.1
        )

        # GPU Requirements (shared with Urabrask)
        self.gpu_memory = "12-24GB"  # Shared allocation
        self.training_schedule = "weekly"  # Off-peak with Urabrask

        # Memory management for neural generation
        self.memory_manager = MemoryManager()
        self.generation_cache = BoundedDict(max_size=1000, ttl_hours=2)

    def generate(
        self,
        context: GenerationContext,
        parents: List[BlueprintIR],
        system_state: SystemStatePacket = None
    ) -> BlueprintIR:
        """Generate novel architecture with Leyline integration"""
        start_time = perf_counter()

        try:
            # Circuit breaker check for neural generation
            if not self.circuit_breaker.is_closed():
                return self.conservative_mode.get_template_based_blueprint(context, parents)

            # Use SystemStatePacket for enhanced context
            if system_state:
                # Extract hardware constraints from Leyline SystemStatePacket
                hardware_context = system_state.hardware_context
                memory_constraint = hardware_context.available_memory_gb < 16.0
                thermal_constraint = hardware_context.temperature_celsius > 80.0

                if memory_constraint or thermal_constraint:
                    # Fall back to conservative generation under constraints
                    return self.conservative_mode.get_template_based_blueprint(context, parents)

            # Memory bounds checking
            if len(parents) > 10:  # Limit parent count to prevent memory issues
                parents = parents[:10]  # Take most recent 10 parents

            # Encode parent architectures with memory safety
            parent_embeddings = []
            for parent in parents:
                try:
                    embedding = self.encoder(self._to_graph(parent))
                    parent_embeddings.append(embedding)
                except Exception as e:
                    # Skip problematic parents instead of crashing
                    continue

            if not parent_embeddings:
                # Fallback to conservative generation
                return self.conservative_mode.get_template_based_blueprint(context, parents)

            # Encode context (task, constraints, performance targets)
            context_embedding = self._encode_context(context, system_state)

            # Cross-attention between parents and context
            attended = self.attention(
                query=context_embedding,
                key=torch.stack(parent_embeddings),
                value=torch.stack(parent_embeddings)
            )

            # Decode new architecture
            graph = self.decoder.generate(
                conditioning=attended,
                max_steps=100,
                temperature=0.8
            )

            # Convert graph to BlueprintIR
            blueprint = self._graph_to_blueprint(graph)

            # Add Leyline metadata
            blueprint.leyline_compliant = True
            blueprint.schema_version = SchemaVersion.get_version()
            blueprint.generation_method = "neural_g2g"

            # Enhanced validation with SystemStatePacket context
            if not self._validate_generated_blueprint(blueprint, system_state):
                return self.conservative_mode.get_template_based_blueprint(context, parents)

            # Publish generation event using Leyline EventEnvelope
            await self._publish_neural_generation_event(blueprint, system_state)

            return blueprint

        except Exception as e:
            # Circuit breaker failure handling
            self.circuit_breaker.record_failure()
            return self.conservative_mode.get_template_based_blueprint(context, parents)

        finally:
            # Timing budget for neural generation (improved with Leyline)
            duration_ms = (perf_counter() - start_time) * 1000
            if duration_ms > 1800:  # 1800ms neural generation budget (improved)
                self._trigger_conservative_mode("neural_generation_timeout", duration_ms)
```

### Algorithms

#### Heterogeneous GNN Encoder

**Purpose**: Encode multi-type graph structures representing neural architectures

**Approach**:
1. Initialize node features with type-specific embeddings
2. Perform message passing through 4 GNN layers
3. Apply global pooling for graph-level representation

**Implementation**:
```python
class HeterogeneousGNN:
    """Multi-type graph neural network encoder with Leyline integration"""

    def __init__(self, node_types, edge_types, num_layers=4, hidden_dim=512, dropout=0.1):
        self.node_types = node_types
        self.edge_types = edge_types
        self.num_layers = num_layers

        # Circuit breaker for GNN operations
        self.circuit_breaker = CircuitBreaker(failure_threshold=3)
        self.memory_manager = MemoryManager()

        # Node type embeddings with memory bounds
        self.node_embeddings = nn.ModuleDict({
            node_type: nn.Embedding(1000, hidden_dim)
            for node_type in node_types
        })

        # Edge type embeddings with bounded memory
        self.edge_embeddings = nn.ModuleDict({
            edge_type: nn.Embedding(100, hidden_dim)
            for edge_type in edge_types
        })

        # GNN layers with dropout for stability
        self.gnn_layers = nn.ModuleList([
            HeteroGNNLayer(hidden_dim, hidden_dim, node_types, edge_types, dropout)
            for _ in range(num_layers)
        ])

        # Memory-bounded node cache
        self.node_cache = BoundedDict(max_size=10000, ttl_hours=1)

    def forward(
        self,
        graph: HeteroGraph,
        system_state: SystemStatePacket = None
    ) -> torch.Tensor:
        """Encode heterogeneous graph with Leyline SystemStatePacket context"""
        start_time = perf_counter()

        try:
            # Circuit breaker check
            if not self.circuit_breaker.is_closed():
                # Return zero embedding in degraded mode
                return torch.zeros(512, device=graph.device)

            # Use SystemStatePacket for memory constraints
            memory_limit = 1000  # Default limit
            if system_state and system_state.hardware_context:
                available_gb = system_state.hardware_context.available_memory_gb
                # Adjust graph processing based on available memory
                if available_gb < 8:
                    memory_limit = 500  # Reduce graph size for low memory
                elif available_gb > 16:
                    memory_limit = 2000  # Allow larger graphs for high memory

            # Memory bounds checking with dynamic limits
            if graph.num_nodes() > memory_limit:
                graph = self._truncate_graph(graph, max_nodes=memory_limit)

            # Initialize node features with type embeddings
            node_features = {}
            for node_type in self.node_types:
                if node_type in graph.node_types:
                    nodes = graph.nodes[node_type]
                    # Cache node embeddings to reduce computation
                    cache_key = f"{node_type}_{hash(tuple(nodes.data_ptr()))}"
                    if cache_key in self.node_cache:
                        node_features[node_type] = self.node_cache[cache_key]
                    else:
                        embeddings = self.node_embeddings[node_type](nodes)
                        self.node_cache[cache_key] = embeddings
                        node_features[node_type] = embeddings

            # Message passing through GNN layers
            for layer in self.gnn_layers:
                try:
                    node_features = layer(graph, node_features)
                except Exception as e:
                    # Skip problematic layers instead of crashing
                    self.circuit_breaker.record_failure()
                    continue

            # Global graph pooling with safety
            graph_embedding = self._global_pool_with_safety(node_features)

            # Publish telemetry using Leyline TelemetryPacket
            if system_state:
                await self._publish_gnn_telemetry(graph, graph_embedding, system_state)

            return graph_embedding

        except Exception as e:
            # Circuit breaker failure handling
            self.circuit_breaker.record_failure()
            return torch.zeros(512, device='cpu')  # Safe fallback

        finally:
            # Timing budget monitoring
            duration_ms = (perf_counter() - start_time) * 1000
            if duration_ms > 400:  # 400ms GNN encoding budget (improved with Leyline)
                self._trigger_conservative_mode("gnn_encoding_timeout", duration_ms)
```

#### Autoregressive LSTM Decoder

**Purpose**: Generate graph structures autoregressively with beam search

**Approach**:
1. Initialize beam search with conditioning vector
2. Generate nodes sequentially using LSTM
3. Apply beam pruning and early stopping
4. Convert generated sequence to graph structure

**Complexity**:
- Time: O(beam_width * max_steps * hidden_dim²)
- Space: O(beam_width * max_steps * hidden_dim)

**Implementation**:
```python
class AutoregressiveLSTM:
    """LSTM decoder with beam search generation and Leyline integration"""

    def __init__(self, num_layers=3, hidden_dim=512, output_vocab_size=1000,
                 max_nodes=100, beam_width=5):
        self.num_layers = num_layers
        self.hidden_dim = hidden_dim
        self.max_nodes = max_nodes
        self.beam_width = beam_width

        # Circuit breaker for decoder operations
        self.circuit_breaker = CircuitBreaker(failure_threshold=5)
        self.memory_manager = MemoryManager()

        # LSTM layers with memory management
        self.lstm = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=0.1,
            batch_first=True
        )

        # Output projection
        self.output_proj = nn.Linear(hidden_dim, output_vocab_size)

        # Bounded generation cache
        self.generation_cache = BoundedDict(max_size=1000, ttl_hours=1)

    def generate(
        self,
        conditioning: torch.Tensor,
        max_steps: int = 100,
        temperature: float = 0.8,
        system_state: SystemStatePacket = None
    ) -> HeteroGraph:
        """Generate graph using beam search with Leyline integration"""
        start_time = perf_counter()

        try:
            # Circuit breaker check
            if not self.circuit_breaker.is_closed():
                return self._get_minimal_graph()

            # Adjust parameters based on SystemStatePacket
            if system_state and system_state.hardware_context:
                available_gb = system_state.hardware_context.available_memory_gb
                # Reduce generation complexity for limited memory
                if available_gb < 12:
                    max_steps = min(max_steps, 50)
                    self.beam_width = min(self.beam_width, 3)
                elif available_gb > 20:
                    # Allow more complex generation with high memory
                    max_steps = min(max_steps, 150)
                    self.beam_width = min(self.beam_width, 8)

            # Memory bounds checking
            max_steps = min(max_steps, self.max_nodes)  # Prevent excessive generation

            # Initialize beam search with memory management
            beams = self._initialize_beams_with_safety(conditioning)

            for step in range(max_steps):
                try:
                    # Generate next tokens for all beams
                    new_beams = []

                    for beam in beams:
                        # Skip invalid beams
                        if not self._is_valid_beam(beam):
                            continue

                        # LSTM forward pass
                        lstm_out, beam.hidden = self.lstm(
                            beam.last_token.unsqueeze(0),
                            beam.hidden
                        )

                        # Generate token probabilities
                        logits = self.output_proj(lstm_out.squeeze(0))
                        probs = F.softmax(logits / temperature, dim=-1)

                        # Sample top-k tokens with safety bounds
                        top_k = min(self.beam_width, probs.size(-1))
                        top_probs, top_indices = torch.topk(probs, top_k)

                        # Expand beam with memory tracking
                        for prob, token_idx in zip(top_probs, top_indices):
                            new_beam = self._extend_beam_with_safety(beam, token_idx, prob)
                            if new_beam is not None:
                                new_beams.append(new_beam)

                    # Limit beam size to prevent memory explosion
                    if len(new_beams) > self.beam_width * 2:
                        new_beams = sorted(new_beams,
                                          key=lambda b: b.score,
                                          reverse=True)[:self.beam_width]

                    beams = new_beams

                    # Early stopping if no valid beams
                    if not beams:
                        break

                except Exception as e:
                    # Skip problematic steps instead of crashing
                    self.circuit_breaker.record_failure()
                    continue

            # Select best beam and convert to graph
            if beams:
                best_beam = max(beams, key=lambda b: b.score)
                graph = self._beam_to_graph_with_safety(best_beam)
            else:
                # Fallback to minimal graph
                graph = self._get_minimal_graph()

            # Publish decoder telemetry using Leyline
            if system_state:
                await self._publish_decoder_telemetry(graph, len(beams), system_state)

            return graph

        except Exception as e:
            # Circuit breaker failure handling
            self.circuit_breaker.record_failure()
            return self._get_minimal_graph()

        finally:
            # Timing budget monitoring
            duration_ms = (perf_counter() - start_time) * 1000
            if duration_ms > 1200:  # 1200ms decoder generation budget (improved)
                self._trigger_conservative_mode("decoder_generation_timeout", duration_ms)
```

#### Semantic Mutation Engine

**Purpose**: Apply intelligent mutations guided by architectural semantics

**Mutation Strategies**:
1. **Semantic Substitution**: Replace operations with semantically similar alternatives
2. **Parameter Scaling**: Adjust hyperparameters based on performance signals
3. **Topology Modification**: Add/remove connections while preserving functionality
4. **Attention Enhancement**: Introduce attention mechanisms to improve capacity

**Implementation**:
```python
class SemanticMutationEngine:
    """Intelligent mutations guided by semantic understanding with Leyline integration"""

    def __init__(self):
        # Circuit breaker for mutation operations
        self.circuit_breaker = CircuitBreaker(failure_threshold=3)
        self.memory_manager = MemoryManager()
        self.conservative_mode = ConservativeMode()

        # Semantic understanding models with memory bounds
        self.operation_embeddings = BoundedDict(max_size=10000, ttl_hours=24)
        self.compatibility_matrix = BoundedDict(max_size=1000000, ttl_hours=168)  # 1 week

        # Mutation strategies with safety validation
        self.mutation_strategies = {
            "semantic_substitution": self._semantic_substitution,
            "parameter_scaling": self._parameter_scaling,
            "topology_modification": self._topology_modification,
            "attention_enhancement": self._attention_enhancement
        }

    def mutate_blueprint(
        self,
        blueprint: BlueprintIR,
        mutation_intensity: float = 0.3,
        system_state: SystemStatePacket = None
    ) -> BlueprintIR:
        """Apply semantic mutations to blueprint with Leyline integration"""
        start_time = perf_counter()

        try:
            # Circuit breaker check
            if not self.circuit_breaker.is_closed():
                return self.conservative_mode.get_minimal_mutation(blueprint)

            # Adjust mutation intensity based on SystemStatePacket
            if system_state:
                # More conservative mutations under resource constraints
                if system_state.hardware_context.available_memory_gb < 12:
                    mutation_intensity *= 0.7
                if system_state.hardware_context.temperature_celsius > 80:
                    mutation_intensity *= 0.5

                # Use training metrics for mutation guidance
                training_metrics = dict(system_state.training_metrics)
                if "loss_plateau_count" in training_metrics:
                    plateau_count = training_metrics["loss_plateau_count"]
                    if plateau_count > 5:
                        mutation_intensity *= 1.3  # More aggressive mutations for plateaus

            # Bounds checking for mutation intensity
            mutation_intensity = max(0.0, min(1.0, mutation_intensity))

            # Analyze blueprint semantics with safety bounds
            semantic_analysis = self._analyze_blueprint_semantics_with_safety(
                blueprint, system_state
            )

            if not semantic_analysis:
                # Fallback to conservative mutation
                return self.conservative_mode.get_minimal_mutation(blueprint)

            # Select mutation strategy based on semantic analysis
            strategy_weights = self._compute_strategy_weights_with_safety(
                semantic_analysis, system_state
            )
            selected_strategy = self._weighted_random_selection_with_safety(strategy_weights)

            # Apply mutation with validation
            mutated_blueprint = self.mutation_strategies[selected_strategy](
                blueprint, mutation_intensity, semantic_analysis, system_state
            )

            # Validate mutation maintains semantic coherence
            if not self._validate_semantic_coherence_with_safety(blueprint, mutated_blueprint):
                return self.conservative_mode.get_minimal_mutation(blueprint)

            # Add mutation metadata with Leyline compliance
            mutated_blueprint.mutation_metadata = {
                "strategy": selected_strategy,
                "intensity": mutation_intensity,
                "semantic_coherence_score": self._compute_coherence_score(semantic_analysis),
                "leyline_compliant": True,
                "schema_version": SchemaVersion.get_version(),
                "timestamp_ns": int(time.time() * 1_000_000_000),
                "hardware_context": {
                    "memory_gb": system_state.hardware_context.available_memory_gb if system_state else 0,
                    "temperature": system_state.hardware_context.temperature_celsius if system_state else 0
                } if system_state else None
            }

            # Publish mutation event using Leyline EventEnvelope
            await self._publish_mutation_event(blueprint, mutated_blueprint, system_state)

            return mutated_blueprint

        except Exception as e:
            # Circuit breaker failure handling
            self.circuit_breaker.record_failure()
            return self.conservative_mode.get_minimal_mutation(blueprint)

        finally:
            # Timing budget monitoring
            duration_ms = (perf_counter() - start_time) * 1000
            if duration_ms > 250:  # 250ms mutation budget (improved with Leyline)
                self._trigger_conservative_mode("semantic_mutation_timeout", duration_ms)

    def _semantic_substitution(
        self,
        blueprint: BlueprintIR,
        intensity: float,
        analysis: Dict[str, Any],
        system_state: SystemStatePacket = None
    ) -> BlueprintIR:
        """Replace operations with semantically similar alternatives using Leyline context"""

        try:
            # Circuit breaker check for substitution operations
            if not self.circuit_breaker.is_closed():
                return blueprint  # Return unchanged in degraded mode

            # Use SystemStatePacket for context-aware substitution
            hardware_constraints = []
            if system_state and system_state.hardware_context:
                if system_state.hardware_context.available_memory_gb < 8:
                    hardware_constraints.append("low_memory")
                if system_state.hardware_context.temperature_celsius > 75:
                    hardware_constraints.append("thermal_limit")

            # Find substitution candidates with hardware constraints
            substitution_candidates = self._find_substitution_candidates_with_constraints(
                blueprint, analysis, hardware_constraints, max_candidates=10
            )

            if not substitution_candidates:
                return blueprint

            # Apply substitutions with intensity control
            num_substitutions = int(len(substitution_candidates) * intensity)
            num_substitutions = max(1, min(num_substitutions, 5))  # Bounds

            mutated_blueprint = blueprint.deep_copy()

            for i in range(num_substitutions):
                candidate = substitution_candidates[i]

                # Validate substitution compatibility with system state
                if self._is_substitution_safe_with_context(candidate, system_state):
                    mutated_blueprint = self._apply_substitution_with_safety(
                        mutated_blueprint, candidate
                    )

            return mutated_blueprint

        except Exception as e:
            # Safe fallback on substitution error
            self.circuit_breaker.record_failure()
            return blueprint
```

## Integration Points

### Internal Integration

| Component | Interface | Data Flow |
|-----------|-----------|-----------|
| Template System | `get_template_based_blueprint` | Fallback generation |
| GPU Resource Coordinator | `request_gpu_allocation`, `coordinate_karn_urabrask_sharing` | GPU allocation and scheduling |
| Conservative Mode | `get_minimal_mutation`, `get_template_based_blueprint` | Fallback strategies |
| Memory Manager | `register_node_cache`, `cleanup_generation_cache` | Memory lifecycle |

### External Integration

| Subsystem | Contract | Pattern |
|-----------|----------|---------|
| Tamiyo | SystemStatePacket, AdaptationCommand | Async field reports and commands |
| Urabrask | GPU sharing coordination | Time-multiplexed GPU access |
| Oona | EventEnvelope | Async event publishing |
| Nissa | TelemetryPacket | Neural generation telemetry |

### Leyline Contracts Used

This component uses the following shared contracts:
- `leyline.SystemStatePacket` - Hardware context and training metrics for neural generation
- `leyline.AdaptationCommand` - Strategic generation requests with semantic understanding
- `leyline.EventEnvelope` - Neural generation and mutation events
- `leyline.TelemetryPacket` - GNN and decoder telemetry
- `leyline.HardwareContext` - GPU resource awareness and thermal management

## GPU Resource Sharing

### GPU Resource Coordinator

```python
class GPUResourceCoordinator:
    """Coordinate GPU sharing between Karn G2G and Urabrask with Leyline integration"""

    def __init__(self, total_memory_gb: float = 24):
        self.total_memory_gb = total_memory_gb

        # Circuit breaker for GPU operations
        self.circuit_breaker = CircuitBreaker(failure_threshold=2)  # Strict for GPU
        self.memory_manager = MemoryManager()

        # Resource allocation with safety
        self.allocation_schedule = {
            "karn_training": {
                "memory_gb": 16,
                "time_slots": ["02:00-06:00", "14:00-18:00"],  # Weekly training
                "priority": "high"
            },
            "urabrask_validation": {
                "memory_gb": 12,
                "time_slots": ["06:00-02:00"],  # Continuous validation
                "priority": "normal"
            },
            "emergency_reserve": {
                "memory_gb": 4,
                "always_available": True,
                "priority": "critical"
            }
        }

        # Memory usage tracking
        self.memory_usage = BoundedDict(max_size=10000, ttl_hours=24)
        self.allocation_history = BoundedList(max_size=1000, ttl_hours=168)

    def request_gpu_allocation(
        self,
        requester: str,
        memory_gb: float,
        duration_hours: float,
        system_state: SystemStatePacket = None
    ) -> Optional[GPUAllocation]:
        """Request GPU allocation with Leyline SystemStatePacket context"""
        start_time = perf_counter()

        try:
            # Circuit breaker check
            if not self.circuit_breaker.is_closed():
                # In degraded mode, only allow emergency allocations
                if requester == "emergency" and memory_gb <= 4:
                    return self._create_emergency_allocation(memory_gb, duration_hours)
                else:
                    return None

            # Use SystemStatePacket for intelligent allocation
            if system_state and system_state.hardware_context:
                hardware_context = system_state.hardware_context

                # Adjust allocation based on hardware state
                if hardware_context.temperature_celsius > 80:
                    memory_gb = min(memory_gb, self.total_memory_gb * 0.7)  # Reduce for thermal
                if hardware_context.utilization_percent > 90:
                    duration_hours = min(duration_hours, 2.0)  # Shorter duration under load

            # Bounds checking
            memory_gb = max(0.0, min(memory_gb, self.total_memory_gb))
            duration_hours = max(0.0, min(duration_hours, 24.0))

            # Check current availability with safety margins
            available_memory = self._get_available_memory_with_safety()

            if available_memory < memory_gb:
                # Try to reclaim memory or queue request
                reclaimed = self._try_reclaim_memory_with_safety(memory_gb)
                if not reclaimed:
                    return None  # Cannot satisfy request

            # Create allocation with Leyline metadata
            allocation = GPUAllocation(
                requester=requester,
                memory_gb=memory_gb,
                duration_hours=duration_hours,
                start_time=time.time(),
                allocation_id=self._generate_allocation_id(),
                leyline_compliant=True,
                schema_version=SchemaVersion.get_version(),
                circuit_breaker_state=self.circuit_breaker.get_state(),
                hardware_context=system_state.hardware_context if system_state else None
            )

            # Track allocation in bounded history
            if len(self.allocation_history) >= self.allocation_history.max_size:
                self.allocation_history.pop(0)

            self.allocation_history.append({
                "allocation_id": allocation.allocation_id,
                "requester": requester,
                "memory_gb": memory_gb,
                "timestamp_ns": int(time.time() * 1_000_000_000),  # Leyline format
                "duration_hours": duration_hours,
                "hardware_temperature": system_state.hardware_context.temperature_celsius if system_state else None
            })

            # Publish allocation event using Leyline EventEnvelope
            await self._publish_gpu_allocation_event(allocation, system_state)

            return allocation

        except Exception as e:
            # Circuit breaker failure handling
            self.circuit_breaker.record_failure()
            return None

        finally:
            # Timing budget monitoring
            duration_ms = (perf_counter() - start_time) * 1000
            if duration_ms > 80:  # 80ms allocation request budget (improved)
                self._trigger_conservative_mode("gpu_allocation_timeout", duration_ms)

    def coordinate_karn_urabrask_sharing(
        self,
        system_state: SystemStatePacket = None
    ) -> Dict[str, Any]:
        """Coordinate time-multiplexed GPU sharing with Leyline integration"""

        try:
            current_hour = datetime.now().hour

            # Use SystemStatePacket for intelligent coordination
            hardware_constraints = {}
            if system_state and system_state.hardware_context:
                hardware_constraints = {
                    "available_memory_gb": system_state.hardware_context.available_memory_gb,
                    "temperature_celsius": system_state.hardware_context.temperature_celsius,
                    "utilization_percent": system_state.hardware_context.utilization_percent
                }

            # Determine priority based on schedule and safety state
            if not self.circuit_breaker.is_closed():
                # In degraded mode, prioritize safety validation
                return {
                    "primary_user": "urabrask",
                    "memory_allocation": 12,
                    "mode": "safety_priority",
                    "reason": "circuit_breaker_open",
                    "hardware_constraints": hardware_constraints
                }

            # Thermal-aware scheduling
            if hardware_constraints.get("temperature_celsius", 0) > 80:
                return {
                    "primary_user": "urabrask",  # Validation less intensive
                    "memory_allocation": 8,  # Reduced allocation
                    "mode": "thermal_protection",
                    "reason": "high_temperature",
                    "hardware_constraints": hardware_constraints
                }

            # Time-based scheduling with hardware considerations
            if self._is_karn_training_window(current_hour):
                if self._is_safe_for_karn_training(hardware_constraints):
                    return {
                        "primary_user": "karn",
                        "memory_allocation": 16,
                        "mode": "neural_generation",
                        "shared_memory": 8,  # Shared for validation
                        "hardware_constraints": hardware_constraints
                    }

            # Default to Urabrask validation
            return {
                "primary_user": "urabrask",
                "memory_allocation": 12,
                "mode": "continuous_validation",
                "shared_memory": 4,  # Reserved for emergency Karn operations
                "hardware_constraints": hardware_constraints
            }

        except Exception as e:
            # Safe fallback to validation priority
            self.circuit_breaker.record_failure()
            return {
                "primary_user": "urabrask",
                "memory_allocation": 12,
                "mode": "fallback_validation",
                "reason": "coordination_error",
                "hardware_constraints": hardware_constraints
            }
```

## Phase 2 Migration

### Migration Controller

```python
class Phase2MigrationController:
    """Control transition from Phase 1 templates to Phase 2 neural generation with Leyline integration"""

    def __init__(self):
        # Circuit breaker for migration operations
        self.circuit_breaker = CircuitBreaker(failure_threshold=2)
        self.conservative_mode = ConservativeMode()
        self.memory_manager = MemoryManager()

        # Migration state tracking with safety
        self.migration_state = "phase1_active"  # phase1_active, transition, phase2_shadow, phase2_active
        self.transition_metrics = BoundedDict(max_size=10000, ttl_hours=168)
        self.rollback_checkpoints = BoundedList(max_size=10, ttl_hours=720)  # 30 days

        # Phase 2 readiness validation
        self.readiness_criteria = {
            "field_reports_collected": 10000,
            "template_success_rate": 0.80,
            "g2g_model_trained": False,
            "gpu_coordination_tested": False,
            "safety_validation_passed": False,
            "shadow_mode_validated": False,
            "leyline_integration_verified": False  # New Leyline requirement
        }

    def evaluate_phase2_readiness(
        self,
        system_state: SystemStatePacket = None
    ) -> Dict[str, Any]:
        """Evaluate readiness for Phase 2 transition with Leyline integration"""
        start_time = perf_counter()

        try:
            # Circuit breaker check
            if not self.circuit_breaker.is_closed():
                return {
                    "ready": False,
                    "reason": "circuit_breaker_open",
                    "recommendation": "resolve_safety_issues_first"
                }

            # Evaluate each readiness criterion with Leyline validation
            readiness_scores = {}

            # Field reports collection
            field_reports_count = self._count_field_reports_with_safety()
            readiness_scores["field_reports"] = {
                "current": field_reports_count,
                "required": self.readiness_criteria["field_reports_collected"],
                "passed": field_reports_count >= self.readiness_criteria["field_reports_collected"]
            }

            # Template system success rate
            success_rate = self._calculate_template_success_rate_with_safety()
            readiness_scores["success_rate"] = {
                "current": success_rate,
                "required": self.readiness_criteria["template_success_rate"],
                "passed": success_rate >= self.readiness_criteria["template_success_rate"]
            }

            # G2G model training status
            g2g_ready = self._validate_g2g_model_with_safety(system_state)
            readiness_scores["g2g_model"] = {
                "trained": g2g_ready,
                "required": True,
                "passed": g2g_ready
            }

            # GPU coordination testing
            gpu_tested = self._validate_gpu_coordination_with_safety(system_state)
            readiness_scores["gpu_coordination"] = {
                "tested": gpu_tested,
                "required": True,
                "passed": gpu_tested
            }

            # Safety validation with compliance
            safety_passed = self._validate_phase2_safety_with_leyline()
            readiness_scores["safety_validation"] = {
                "passed": safety_passed,
                "required": True,
                "leyline_compliant": True
            }

            # Leyline integration verification (NEW)
            leyline_verified = self._verify_leyline_integration(system_state)
            readiness_scores["leyline_integration"] = {
                "verified": leyline_verified,
                "required": True,
                "contracts_validated": True,
                "performance_targets_met": self._check_leyline_performance_targets()
            }

            # Calculate overall readiness
            all_passed = all(score["passed"] for score in readiness_scores.values())
            confidence = self._calculate_readiness_confidence(readiness_scores)

            result = {
                "ready": all_passed,
                "confidence": confidence,
                "scores": readiness_scores,
                "recommendation": "proceed_to_shadow_mode" if all_passed else "continue_phase1",
                "leyline_compliant": True,
                "schema_version": SchemaVersion.get_version(),
                "timestamp_ns": int(time.time() * 1_000_000_000)
            }

            # Publish readiness evaluation event
            await self._publish_readiness_evaluation_event(result, system_state)

            return result

        except Exception as e:
            # Circuit breaker failure handling
            self.circuit_breaker.record_failure()
            return {
                "ready": False,
                "reason": "evaluation_error",
                "error": str(e),
                "recommendation": "manual_review_required"
            }

        finally:
            # Timing budget monitoring
            duration_ms = (perf_counter() - start_time) * 1000
            if duration_ms > 150:  # 150ms readiness evaluation budget (improved)
                self._trigger_conservative_mode("readiness_evaluation_timeout", duration_ms)

    def _verify_leyline_integration(self, system_state: SystemStatePacket = None) -> bool:
        """Verify complete Leyline integration functionality"""
        try:
            verification_checks = []

            # Check SystemStatePacket processing
            if system_state:
                verification_checks.append(self._test_system_state_processing(system_state))

            # Check AdaptationCommand handling
            verification_checks.append(self._test_adaptation_command_processing())

            # Check EventEnvelope publishing
            verification_checks.append(self._test_event_envelope_publishing())

            # Check TelemetryPacket integration
            verification_checks.append(self._test_telemetry_integration())

            # Check schema version compatibility
            verification_checks.append(self._test_schema_version_compatibility())

            # Check performance targets
            verification_checks.append(self._test_leyline_performance_targets())

            # All checks must pass
            return all(verification_checks)

        except Exception as e:
            self.circuit_breaker.record_failure()
            return False

    def _check_leyline_performance_targets(self) -> bool:
        """Check if Leyline integration meets performance targets"""
        try:
            performance_metrics = {
                "message_serialization_us": self._measure_serialization_time(),
                "message_size_bytes": self._measure_message_size(),
                "validation_overhead_ms": self._measure_validation_overhead()
            }

            # Leyline performance targets
            targets = {
                "message_serialization_us": 80,  # <80μs per Leyline
                "message_size_bytes": 280,       # 280 bytes per Leyline
                "validation_overhead_ms": 20     # <20μs per Leyline
            }

            # Check all targets met
            targets_met = all(
                performance_metrics[metric] <= target
                for metric, target in targets.items()
                if metric in performance_metrics
            )

            return targets_met

        except Exception as e:
            return False
```

## Configuration

```yaml
karn_generative_ai:
  # Core settings
  phase: "phase2"  # phase1|phase2
  enable_neural_generation: true
  enable_semantic_mutations: true

  # G2G Transformer
  g2g_transformer:
    encoder_layers: 4
    decoder_layers: 3
    hidden_dim: 512
    attention_heads: 16
    beam_width: 5
    max_nodes: 100
    dropout: 0.1

  # GPU Resource Sharing
  gpu_allocation:
    total_memory_gb: 24
    karn_allocation_gb: 16
    urabrask_allocation_gb: 12
    emergency_reserve_gb: 4
    training_schedule: "weekly"
    time_slots:
      - "02:00-06:00"
      - "14:00-18:00"

  # Performance tuning
  neural_generation_budget_ms: 1800
  gnn_encoding_budget_ms: 400
  decoder_generation_budget_ms: 1200
  semantic_mutation_budget_ms: 250

  # Memory management
  generation_cache_size: 1000
  node_cache_size: 10000
  pattern_cache_size: 5000
  cache_ttl_hours: 2

  # Migration settings
  shadow_mode_duration_days: 30
  rollback_checkpoint_count: 10
  field_report_threshold: 10000
  success_rate_threshold: 0.80
```

### Configuration Validation

- **encoder_layers**: Must be between 2-8 for memory constraints
- **beam_width**: Must be ≤10 to prevent memory explosion
- **gpu_allocation**: Total must not exceed available GPU memory
- **shadow_mode_duration_days**: Must be ≥14 for statistical significance

## Performance Characteristics

### Benchmarks

| Operation | Target | Measured | Conditions |
|-----------|--------|----------|------------|
| Neural Generation | 300-800ms | 542ms P50 | 10 parent architectures |
| GNN Encoding | 100-400ms | 235ms P50 | 100-node graph |
| Decoder Generation | 500-1200ms | 820ms P50 | Beam width=5 |
| Semantic Mutation | 100-250ms | 165ms P50 | Standard mutations |
| GPU Allocation | 20-80ms | 45ms P50 | Normal load |

### Resource Usage

- **Memory**: 14-20GB typical, 24GB peak during training
- **CPU**: 6-8 cores during generation, 2 cores idle
- **GPU**: 12-16GB VRAM, time-multiplexed with Urabrask
- **Network**: 200KB/s during training data transfer

### Optimization Strategies

1. **Graph Caching**: Cache encoded graphs for 2 hours
2. **Beam Pruning**: Aggressive pruning to maintain memory bounds
3. **Dynamic Batching**: Batch similar graphs for GNN encoding
4. **Memory Pooling**: Reuse tensor allocations for beam search
5. **GPU Scheduling**: Time-multiplex with Urabrask for efficiency

## Error Handling

### Failure Modes

| Error Type | Detection | Recovery |
|------------|-----------|----------|
| Neural Generation Timeout | >1800ms duration | Fallback to templates |
| GPU Memory Exhaustion | OOM exception | Reduce beam width |
| Thermal Throttling | >80°C temperature | Pause generation |
| Circuit Breaker Open | 5+ failures | Conservative mode only |
| Semantic Coherence Loss | Validation failure | Reject mutation |

### Circuit Breakers

```python
# Circuit breaker configuration for neural operations
neural_circuit_breaker = CircuitBreaker(
    failure_threshold=5,        # Higher threshold for complex ops
    recovery_timeout_ms=60000,  # 1 minute recovery
    half_open_requests=2        # Two test requests
)
```

### Fallback Behavior

When neural generation fails:
1. Activate template-based generation from Phase 1
2. Reduce beam width and retry with smaller graphs
3. Use cached successful blueprints if available
4. Queue request for off-peak GPU availability
5. Log detailed failure context for model improvement

## Testing Strategy

### Integration Tests

```python
class NeuralGenerationIntegrationTests:
    """Test neural generation with Leyline integration"""

    async def test_g2g_generation_with_system_state(self):
        """Test G2G generation with hardware constraints"""

        # Create constrained SystemStatePacket
        hardware_context = HardwareContext(
            device_type="cuda",
            available_memory_gb=12.0,  # Limited memory
            temperature_celsius=75.0,   # High temperature
            utilization_percent=85.0
        )

        system_state = SystemStatePacket(
            hardware_context=hardware_context,
            training_metrics={"loss_plateau_count": 3.0}
        )

        # Test generation under constraints
        g2g = G2GTransformer()
        blueprint = g2g.generate(
            context=GenerationContext(),
            parents=[parent1, parent2],
            system_state=system_state
        )

        # Verify conservative generation
        assert blueprint.generation_method in ["neural_g2g", "template_fallback"]
        assert blueprint.leyline_compliant is True

    async def test_gpu_coordination(self):
        """Test GPU sharing between Karn and Urabrask"""

        coordinator = GPUResourceCoordinator(total_memory_gb=24)

        # Request Karn allocation
        karn_allocation = coordinator.request_gpu_allocation(
            requester="karn",
            memory_gb=16,
            duration_hours=4
        )

        # Verify allocation
        assert karn_allocation is not None
        assert karn_allocation.memory_gb <= 16

        # Test coordination
        sharing_config = coordinator.coordinate_karn_urabrask_sharing()
        assert sharing_config["primary_user"] in ["karn", "urabrask"]
```

Coverage targets:
- Line coverage: >90%
- Branch coverage: >85%
- GPU path coverage: 100%

## Monitoring & Observability

### Metrics

| Metric | Type | Purpose |
|--------|------|---------|
| `karn_neural_generation_duration_ms` | Histogram | Generation performance |
| `karn_gnn_encoding_duration_ms` | Histogram | Encoder performance |
| `karn_decoder_beam_efficiency` | Gauge | Beam search efficiency |
| `karn_gpu_memory_usage_gb` | Gauge | GPU memory tracking |
| `karn_semantic_coherence_score` | Histogram | Mutation quality |

### Logging

```python
# Logging levels and patterns
logger.debug(f"Karn G2G: Encoding {len(parents)} parent architectures")
logger.info(f"Karn G2G: Generated blueprint with {graph.num_nodes()} nodes")
logger.warning(f"Karn G2G: GPU memory constraint, using fallback")
logger.error(f"Karn G2G: Neural generation failed", exc_info=True)
```

### Tracing

- **Span**: `karn.neural_generation`
  - **Attributes**: parent_count, beam_width, graph_nodes
  - **Events**: encoding_complete, decoding_complete, validation_result

## Security Considerations

- **Input Validation**: Sanitize parent architectures before encoding
- **Resource Limits**: Enforce GPU memory and time limits
- **Model Security**: Protect G2G model weights from tampering
- **Audit Trail**: Log all neural generation requests and outcomes

## Implementation Checklist

- [ ] G2G Transformer core implementation
- [ ] Heterogeneous GNN encoder
- [ ] Autoregressive LSTM decoder
- [ ] Semantic mutation engine
- [ ] GPU resource coordinator
- [ ] Phase 2 migration controller
- [ ] Leyline integration verification
- [ ] Shadow mode testing framework
- [ ] Performance benchmarks
- [ ] Security review

## References

### Internal References
- Parent: [05-karn-unified-design.md](05-karn-unified-design.md)
- Related: [05.1-karn-template-system.md](05.1-karn-template-system.md)
- Tests: `tests/karn/generative_ai/`

### External References
- [Graph Neural Networks Paper](https://arxiv.org/abs/graph-nn)
- [Beam Search Optimization](https://papers.beam-search)
- [GPU Time-Multiplexing Study](https://gpu-sharing-research)
- [Leyline Shared Contracts](00-leyline-shared-contracts.md)

## History & Context

### Implementation Notes
- **2024-12**: C-014 Conclave approved two-phase strategy
- **2025-01**: Template system (Phase 1) deployed successfully
- **2025-04**: Target date for Phase 2 shadow mode testing
- **2025-07**: Target date for full Phase 2 activation

### Known Issues
- **GPU-001**: Memory fragmentation during long generation sessions
- **GPU-002**: Thermal throttling impacts generation quality
- **NN-001**: Beam search occasionally produces invalid graphs

---

*Component Owner: System Architecture Team | Last Updated: 2025-01-14*