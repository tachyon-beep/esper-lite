# Nissa Alerting and SLO Design

## Document Metadata

| Field | Value |
|-------|-------|
| Parent Document | [10 - Nissa Unified Design](10-nissa-unified-design.md) |
| Component Type | Observability - Alerting and Service Level Objectives |
| Version | 3.0 |
| Status | Production Ready with C-016 Enhancements |
| Date | 2025-09-10 |
| Author | System Architecture Team |

---

## Overview

This document details the alerting and Service Level Objective (SLO) management subsystems of Nissa. These components ensure system reliability through intelligent alert generation, routing, and escalation, combined with comprehensive SLO tracking and error budget management.

### Key Capabilities

- **Intelligent Alert Management** with routing and escalation
- **SLO Framework** with error budget tracking
- **Conservative Mode Triggers** for automatic degradation
- **Circuit Breaker Integration** for fault isolation
- **Dead Letter Queue** for resilient event handling
- **Anomaly Detection** using ML algorithms

## Technical Design

### SLO Framework Architecture

```python
from dataclasses import dataclass
from typing import Dict, Optional
import time

@dataclass
class SLODefinition:
    """[R6-FIX] SLO definition with error budgets"""
    name: str
    target_percentage: float  # e.g., 99.9
    measurement_window_ms: int  # 30 days in milliseconds
    error_budget_burn_rate_threshold: float  # Alert when burn rate > threshold

class NissaSLOFramework:
    """[R6-FIX] Comprehensive SLO tracking for observability platform"""

    def __init__(self):
        # Primary SLOs
        self.slos = {
            "event_ingestion_latency": SLODefinition(
                name="event_ingestion_latency",
                target_percentage=99.9,
                measurement_window_ms=2_592_000_000,  # 30 days
                error_budget_burn_rate_threshold=10.0
            ),
            "metric_query_latency": SLODefinition(
                name="metric_query_latency",
                target_percentage=99.5,
                measurement_window_ms=2_592_000_000,  # 30 days
                error_budget_burn_rate_threshold=5.0
            ),
            "alert_processing_latency": SLODefinition(
                name="alert_processing_latency",
                target_percentage=99.9,
                measurement_window_ms=2_592_000_000,  # 30 days
                error_budget_burn_rate_threshold=20.0
            ),
            "websocket_availability": SLODefinition(
                name="websocket_availability",
                target_percentage=99.95,
                measurement_window_ms=2_592_000_000,  # 30 days
                error_budget_burn_rate_threshold=2.0
            )
        }

        # Performance Targets (all durations in milliseconds)
        self.performance_targets = {
            "event_ingestion_rate": 250_000,  # events/second
            "metric_ingestion_rate": 1_000_000,  # datapoints/second
            "query_latency_p99_ms": 150,  # milliseconds
            "alert_evaluation_frequency_ms": 1_000,  # 1 second
            "websocket_connections": 10_000,  # concurrent
            "memory_cleanup_interval_ms": 300_000,  # 5 minutes
            "ttl_cleanup_batch_size": 10_000  # records per cleanup batch
        }

    def calculate_error_budget_remaining(self, slo_name: str) -> float:
        """Calculate remaining error budget percentage"""
        slo = self.slos[slo_name]
        current_time_ms = int(time.time() * 1000)
        window_start_ms = current_time_ms - slo.measurement_window_ms

        # Query actual performance metrics
        total_requests = self._get_total_requests(slo_name, window_start_ms, current_time_ms)
        failed_requests = self._get_failed_requests(slo_name, window_start_ms, current_time_ms)

        if total_requests == 0:
            return 100.0

        actual_success_rate = ((total_requests - failed_requests) / total_requests) * 100
        target_success_rate = slo.target_percentage

        # Error budget remaining as percentage
        allowed_failure_rate = 100 - target_success_rate
        actual_failure_rate = 100 - actual_success_rate

        return max(0, (allowed_failure_rate - actual_failure_rate) / allowed_failure_rate * 100)
```

### Alert Manager Implementation

```python
class AlertManager:
    """Comprehensive alert management with C-016 enhancements"""

    def __init__(self):
        # Alert components
        self.alert_engine = AlertEngine()
        self.alert_router = AlertRouter()
        self.alert_deduplicator = AlertDeduplicator()
        self.alert_silencer = AlertSilencer()

        # Notification channels
        self.notification_channels = {
            "email": EmailNotifier(),
            "slack": SlackNotifier(),
            "pagerduty": PagerDutyNotifier(),
            "webhook": WebhookNotifier()
        }

        # Circuit breaker protection
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(), "alert_manager"
        )

        # Memory management
        self.memory_manager = MemoryManager()

        # Alert evaluation configuration
        self.evaluation_interval_ms = 1_000  # 1 second
        self.alert_retention_ms = 86_400_000  # 24 hours

    async def evaluate_alerts(self) -> Dict[str, Any]:
        """Evaluate all alert rules with circuit breaker protection"""

        return self.circuit_breaker.execute_with_protection(
            self._evaluate_alerts_internal,
            "evaluate_alerts"
        )

    async def _evaluate_alerts_internal(self) -> Dict[str, Any]:
        """Internal alert evaluation"""

        evaluation_results = {
            "evaluated_rules": 0,
            "triggered_alerts": 0,
            "suppressed_alerts": 0,
            "failed_evaluations": 0
        }

        # Get all active alert rules
        alert_rules = await self.alert_engine.get_active_rules()

        for rule in alert_rules:
            try:
                # Evaluate rule condition
                result = await self.alert_engine.evaluate_rule(rule)

                if result.should_trigger:
                    # Check for duplicates
                    if not await self.alert_deduplicator.is_duplicate(result):
                        # Check silencing rules
                        if not await self.alert_silencer.is_silenced(result):
                            # Route alert
                            await self.route_alert(result)
                            evaluation_results["triggered_alerts"] += 1
                        else:
                            evaluation_results["suppressed_alerts"] += 1

                evaluation_results["evaluated_rules"] += 1

            except Exception as e:
                logging.error(f"Failed to evaluate alert rule {rule.id}: {e}")
                evaluation_results["failed_evaluations"] += 1

        return evaluation_results

    async def route_alert(self, alert: Alert) -> None:
        """Route alert to appropriate channels"""

        # Determine routing based on severity and tags
        routes = await self.alert_router.determine_routes(alert)

        for route in routes:
            channel = self.notification_channels.get(route.channel)
            if channel:
                try:
                    await channel.send_notification(alert, route.config)

                    # Store alert with TTL
                    await self.memory_manager.store_with_ttl(
                        "alert",
                        alert.epoch,
                        alert.id,
                        alert.to_dict(),
                        custom_ttl_ms=self.alert_retention_ms
                    )

                except Exception as e:
                    logging.error(f"Failed to send alert via {route.channel}: {e}")
                    # Send to dead letter queue
                    await self.send_to_dlq(alert, str(e))
```

### Circuit Breaker Implementation

```python
from enum import Enum
from dataclasses import dataclass
import logging

class CircuitBreakerState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"         # Failing, reject calls
    HALF_OPEN = "half_open"  # Testing recovery

@dataclass
class CircuitBreakerConfig:
    """Circuit breaker configuration to replace assert statements"""
    failure_threshold: int = 5      # Number of failures before opening
    recovery_timeout_ms: int = 30_000  # 30s recovery timeout
    half_open_max_calls: int = 3    # Max calls in half-open state

class ObservabilityCircuitBreaker:
    """[R6-FIX] Circuit breaker to replace assert statements in observability"""

    def __init__(self, config: CircuitBreakerConfig, component_name: str):
        self.config = config
        self.component_name = component_name
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.last_failure_time_ms = 0
        self.half_open_calls = 0
        self.total_calls = 0
        self.successful_calls = 0

    def execute_with_protection(self, operation, operation_name: str, *args, **kwargs):
        """Execute operation with circuit breaker protection"""
        self.total_calls += 1

        if self.state == CircuitBreakerState.OPEN:
            if self._should_attempt_reset():
                self.state = CircuitBreakerState.HALF_OPEN
                self.half_open_calls = 0
                logging.info(f"{self.component_name} circuit breaker transitioning to HALF_OPEN")
            else:
                logging.warning(f"{self.component_name} circuit breaker rejecting {operation_name} - circuit OPEN")
                return self._get_conservative_fallback(operation_name)

        if self.state == CircuitBreakerState.HALF_OPEN:
            if self.half_open_calls >= self.config.half_open_max_calls:
                self.state = CircuitBreakerState.OPEN
                logging.error(f"{self.component_name} circuit breaker reopening - half-open limit exceeded")
                return self._get_conservative_fallback(operation_name)

        try:
            result = operation(*args, **kwargs)
            self._record_success()
            return result

        except Exception as e:
            self._record_failure()
            logging.error(f"{self.component_name} operation {operation_name} failed: {e}")

            if self.state == CircuitBreakerState.HALF_OPEN:
                self.state = CircuitBreakerState.OPEN
                logging.error(f"{self.component_name} circuit breaker reopening after half-open failure")

            return self._get_conservative_fallback(operation_name)

    def _get_conservative_fallback(self, operation_name: str):
        """Conservative fallback responses for different operations"""
        fallbacks = {
            "query_metrics": {"status": "circuit_open", "data": [], "cache_hit": True},
            "process_events": {"status": "circuit_open", "processed": 0, "dlq_queued": True},
            "send_alert": {"status": "circuit_open", "queued": True, "delivery": "delayed"},
            "websocket_broadcast": {"status": "circuit_open", "queued": True, "clients_notified": 0}
        }

        return fallbacks.get(operation_name, {"status": "circuit_open", "fallback": True})

    def _should_attempt_reset(self) -> bool:
        """Check if circuit breaker should attempt reset"""
        current_time_ms = int(time.time() * 1000)
        return (current_time_ms - self.last_failure_time_ms) >= self.config.recovery_timeout_ms

    def _record_success(self):
        """Record successful operation"""
        self.successful_calls += 1

        if self.state == CircuitBreakerState.HALF_OPEN:
            self.half_open_calls += 1
            if self.half_open_calls >= self.config.half_open_max_calls:
                # Successfully completed all half-open calls
                self.state = CircuitBreakerState.CLOSED
                self.failure_count = 0
                logging.info(f"{self.component_name} circuit breaker closing - recovery successful")

    def _record_failure(self):
        """Record failed operation"""
        self.failure_count += 1
        self.last_failure_time_ms = int(time.time() * 1000)

        if self.state == CircuitBreakerState.CLOSED:
            if self.failure_count >= self.config.failure_threshold:
                self.state = CircuitBreakerState.OPEN
                logging.error(f"{self.component_name} circuit breaker opening - failure threshold reached")
```

### Conservative Mode Management

```python
import asyncio
from typing import Optional, Dict, Callable
from enum import Enum
import logging

class ConservativeMode(Enum):
    NORMAL = "normal"           # Normal operation
    CONSERVATIVE = "conservative"  # Reduced functionality
    EMERGENCY = "emergency"     # Minimal functionality only

class ConservativeModeManager:
    """[R6-FIX] Conservative mode with backpressure handling"""

    def __init__(self):
        self.current_mode = ConservativeMode.NORMAL
        self.mode_transition_time_ms = 0

        # Trigger thresholds
        self.cpu_threshold_percent = 80.0
        self.memory_threshold_percent = 85.0
        self.event_queue_threshold = 100_000
        self.error_rate_threshold = 0.05  # 5% error rate

        # Conservative mode settings
        self.conservative_batch_size = 100  # Reduced from normal 1000
        self.conservative_processing_delay_ms = 10  # Add processing delays
        self.conservative_max_concurrent = 50  # Reduced concurrency

    async def evaluate_mode_transition(self, system_metrics: Dict[str, float]) -> bool:
        """Evaluate whether to transition between modes"""

        current_time_ms = int(time.time() * 1000)
        should_be_conservative = False

        # Check triggering conditions
        if (system_metrics.get("cpu_usage_percent", 0) > self.cpu_threshold_percent or
            system_metrics.get("memory_usage_percent", 0) > self.memory_threshold_percent or
            system_metrics.get("event_queue_size", 0) > self.event_queue_threshold or
            system_metrics.get("error_rate", 0) > self.error_rate_threshold):
            should_be_conservative = True

        # State transitions
        if should_be_conservative and self.current_mode == ConservativeMode.NORMAL:
            await self._transition_to_conservative()
            return True

        elif not should_be_conservative and self.current_mode == ConservativeMode.CONSERVATIVE:
            # Only transition back after stability period
            if current_time_ms - self.mode_transition_time_ms > 300_000:  # 5 minutes
                await self._transition_to_normal()
                return True

        return False

    async def _transition_to_conservative(self) -> None:
        """Transition to conservative mode"""
        self.current_mode = ConservativeMode.CONSERVATIVE
        self.mode_transition_time_ms = int(time.time() * 1000)

        logging.warning("Transitioning to CONSERVATIVE mode - reducing system load")

        # Implement backpressure mechanisms
        await self._enable_backpressure()

    async def _transition_to_normal(self) -> None:
        """Transition back to normal mode"""
        self.current_mode = ConservativeMode.NORMAL
        self.mode_transition_time_ms = int(time.time() * 1000)

        logging.info("Transitioning to NORMAL mode - full functionality restored")

        # Disable backpressure mechanisms
        await self._disable_backpressure()

    async def _enable_backpressure(self) -> None:
        """Enable backpressure mechanisms"""
        # Reduce batch sizes
        # Add processing delays
        # Limit concurrent operations
        # Prioritize critical operations only
        pass

    async def _disable_backpressure(self) -> None:
        """Disable backpressure mechanisms"""
        # Restore normal batch sizes
        # Remove processing delays
        # Restore normal concurrency limits
        pass

    def get_processing_limits(self) -> Dict[str, int]:
        """Get current processing limits based on mode"""
        if self.current_mode == ConservativeMode.NORMAL:
            return {
                "batch_size": 1000,
                "max_concurrent": 200,
                "processing_delay_ms": 0
            }
        elif self.current_mode == ConservativeMode.CONSERVATIVE:
            return {
                "batch_size": self.conservative_batch_size,
                "max_concurrent": self.conservative_max_concurrent,
                "processing_delay_ms": self.conservative_processing_delay_ms
            }
        else:  # EMERGENCY
            return {
                "batch_size": 10,
                "max_concurrent": 5,
                "processing_delay_ms": 100
            }
```

### Enhanced Event Processor with Resilience

```python
class EnhancedEventProcessor:
    """Production-grade event processing with C-016 comprehensive resilience"""

    def __init__(self):
        # Circuit breaker protection
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(
                failure_threshold=5,
                recovery_timeout_ms=30_000,
                half_open_max_calls=3
            ),
            "event_processor"
        )

        # Protocol Buffer v2 handling
        self.protobuf_handler = ProtocolBufferV2Handler()

        # Memory management
        self.memory_manager = MemoryManager()

        # Conservative mode
        self.conservative_mode = ConservativeModeManager()

        # Enhanced retry policy with _ms suffix
        self.retry_policy = RetryPolicy(
            max_attempts=3,
            backoff_strategy="exponential",
            base_delay_ms=1_000,     # 1 second
            max_delay_ms=30_000      # 30 seconds
        )

        self.bulkhead = Bulkhead(
            max_concurrent=100,
            queue_size=1000
        )

        # Timeout settings with _ms suffix
        self.timeout_settings = {
            "default_timeout_ms": 30_000,      # 30 seconds
            "heavy_computation_ms": 60_000,    # 1 minute
            "network_call_ms": 10_000          # 10 seconds
        }

    async def process_with_resilience(
        self,
        event: EventEnvelope,
        processor: Callable
    ) -> Dict[str, Any]:
        """Process event with full C-016 resilience patterns"""

        return self.circuit_breaker.execute_with_protection(
            self._process_with_resilience_internal,
            "process_event",
            event,
            processor
        )

    async def _process_with_resilience_internal(
        self,
        event: EventEnvelope,
        processor: Callable
    ) -> Dict[str, Any]:
        """Internal resilient processing with all C-016 patterns"""

        # Decode Protocol Buffer v2 event
        try:
            event_data = await self.protobuf_handler.decode_message(
                event.message_type,
                event.serialized_data
            )
        except Exception as e:
            logging.error(f"Failed to decode Protocol Buffer v2 event: {e}")
            return {"status": "decode_error", "error": str(e)}

        # Store in memory manager
        await self.memory_manager.store_with_ttl(
            "event",
            event.epoch,
            event.request_id,
            event_data
        )

        # Get processing limits
        limits = self.conservative_mode.get_processing_limits()

        # Acquire bulkhead permit with conservative limits
        async with self.bulkhead.acquire(max_concurrent=limits["max_concurrent"]) as permit:
            if not permit:
                return {
                    "status": "bulkhead_full",
                    "queued": True,
                    "mode": self.conservative_mode.current_mode.value
                }

            # Apply timeout based on event type
            timeout_ms = self.timeout_settings.get(
                f"{event.event_type}_ms",
                self.timeout_settings["default_timeout_ms"]
            )

            # Retry with exponential backoff
            last_error = None
            for attempt in range(self.retry_policy.max_attempts):
                try:
                    async with asyncio.timeout(timeout_ms / 1000.0):
                        result = await processor(event_data)

                        return {
                            "status": "success",
                            "result": result,
                            "attempts": attempt + 1,
                            "mode": self.conservative_mode.current_mode.value
                        }

                except asyncio.TimeoutError:
                    last_error = "timeout"
                    if attempt < self.retry_policy.max_attempts - 1:
                        delay_ms = self.retry_policy.get_delay_ms(attempt)
                        await asyncio.sleep(delay_ms / 1000.0)

                except Exception as e:
                    last_error = str(e)
                    if not self.retry_policy.should_retry(e):
                        break
                    if attempt < self.retry_policy.max_attempts - 1:
                        delay_ms = self.retry_policy.get_delay_ms(attempt)
                        await asyncio.sleep(delay_ms / 1000.0)

        return {
            "status": "max_retries_exceeded",
            "error": last_error,
            "attempts": self.retry_policy.max_attempts
        }
```

### Dead Letter Queue Implementation

```python
class DeadLetterQueue:
    """Dead letter queue for failed event handling"""

    def __init__(self):
        self.queue: asyncio.Queue = asyncio.Queue(maxsize=100_000)
        self.retry_attempts = {}
        self.max_retry_attempts = 5
        self.retry_delay_ms = 60_000  # 1 minute

        # Memory management
        self.memory_manager = MemoryManager()

        # Metrics
        self.total_items_queued = 0
        self.total_items_recovered = 0
        self.total_items_dropped = 0

    async def enqueue(self, item: Dict, error: str) -> None:
        """Add failed item to dead letter queue"""

        item_id = item.get("id", generate_id())

        # Track retry attempts
        self.retry_attempts[item_id] = self.retry_attempts.get(item_id, 0) + 1

        if self.retry_attempts[item_id] > self.max_retry_attempts:
            # Maximum retries exceeded, drop item
            self.total_items_dropped += 1
            logging.error(f"Dropping item {item_id} after {self.max_retry_attempts} attempts")
            return

        # Store in memory with TTL
        await self.memory_manager.store_with_ttl(
            "dlq",
            int(time.time()),
            item_id,
            {
                "item": item,
                "error": error,
                "attempts": self.retry_attempts[item_id],
                "queued_at": time.time()
            },
            custom_ttl_ms=86_400_000  # 24 hour TTL
        )

        # Add to queue
        await self.queue.put({
            "id": item_id,
            "item": item,
            "error": error,
            "retry_at": time.time() + (self.retry_delay_ms / 1000)
        })

        self.total_items_queued += 1

    async def process_dlq(self) -> None:
        """Process items in dead letter queue"""

        while True:
            try:
                # Get item from queue
                dlq_item = await self.queue.get()

                # Check if ready for retry
                if time.time() < dlq_item["retry_at"]:
                    # Not ready, re-queue
                    await self.queue.put(dlq_item)
                    await asyncio.sleep(1)
                    continue

                # Attempt to reprocess
                success = await self.reprocess_item(dlq_item["item"])

                if success:
                    self.total_items_recovered += 1
                    self.retry_attempts.pop(dlq_item["id"], None)
                else:
                    # Re-queue for retry
                    await self.enqueue(dlq_item["item"], "Reprocessing failed")

            except Exception as e:
                logging.error(f"Error processing DLQ: {e}")
                await asyncio.sleep(5)

    async def reprocess_item(self, item: Dict) -> bool:
        """Attempt to reprocess failed item"""
        # Implementation depends on item type
        # This is a placeholder for actual reprocessing logic
        return False
```

### Anomaly Detection

ML-based anomaly detection:

```python
class AnomalyDetector:
    """ML-based anomaly detection for metrics and events"""

    def __init__(self):
        self.models = {}
        self.training_window_ms = 86_400_000  # 24 hours
        self.detection_threshold = 3.0  # Standard deviations

        # Circuit breaker for ML operations
        self.circuit_breaker = ObservabilityCircuitBreaker(
            CircuitBreakerConfig(), "anomaly_detector"
        )

    async def detect_anomalies(
        self,
        metric_name: str,
        data_points: List[DataPoint]
    ) -> List[Anomaly]:
        """Detect anomalies in metric data"""

        return self.circuit_breaker.execute_with_protection(
            self._detect_anomalies_internal,
            "detect_anomalies",
            metric_name,
            data_points
        )

    async def _detect_anomalies_internal(
        self,
        metric_name: str,
        data_points: List[DataPoint]
    ) -> List[Anomaly]:
        """Internal anomaly detection using statistical methods"""

        if len(data_points) < 100:
            return []  # Not enough data for detection

        # Calculate statistics
        values = [dp.value for dp in data_points]
        mean = statistics.mean(values)
        stdev = statistics.stdev(values)

        anomalies = []

        for dp in data_points:
            # Z-score based detection
            z_score = abs((dp.value - mean) / stdev) if stdev > 0 else 0

            if z_score > self.detection_threshold:
                anomalies.append(Anomaly(
                    timestamp=dp.timestamp,
                    metric=metric_name,
                    value=dp.value,
                    expected_value=mean,
                    deviation=z_score,
                    severity=self._calculate_severity(z_score)
                ))

        return anomalies

    def _calculate_severity(self, z_score: float) -> str:
        """Calculate anomaly severity based on deviation"""
        if z_score > 5:
            return "critical"
        elif z_score > 4:
            return "high"
        elif z_score > 3:
            return "medium"
        else:
            return "low"
```

## Alert Rules and Configurations

### Alert Rule Definitions

```yaml
# Alert rule configurations
alert_rules:
  - name: high_error_rate
    condition: rate(errors[5m]) > 0.05
    severity: critical
    annotations:
      summary: "High error rate detected"
      description: "Error rate exceeds 5% threshold"
    routing:
      - channel: pagerduty
        if: severity == "critical"
      - channel: slack
        if: severity in ["warning", "critical"]

  - name: memory_leak_detected
    condition: delta(memory_usage[1h]) > 1GB
    severity: high
    annotations:
      summary: "Potential memory leak"
      description: "Memory usage increased by >1GB in 1 hour"
    routing:
      - channel: email
        to: ["ops-team@example.com"]

  - name: circuit_breaker_open
    condition: circuit_breaker_state == "open"
    severity: high
    annotations:
      summary: "Circuit breaker opened"
      description: "Component {{ .component }} circuit breaker is open"
    routing:
      - channel: slack
        immediate: true

  - name: slo_budget_low
    condition: error_budget_remaining < 10
    severity: warning
    annotations:
      summary: "SLO error budget low"
      description: "{{ .slo_name }} has <10% error budget remaining"
    routing:
      - channel: email
        to: ["sre-team@example.com"]

  - name: conservative_mode_active
    condition: conservative_mode != "normal"
    severity: warning
    annotations:
      summary: "System in conservative mode"
      description: "Operating in {{ .mode }} mode due to resource pressure"
    routing:
      - channel: slack
        channel: "#ops-alerts"
```

### Notification Channel Configuration

```python
class NotificationChannelConfig:
    """Configuration for notification channels"""

    def __init__(self):
        self.channels = {
            "email": {
                "smtp_host": "smtp.example.com",
                "smtp_port": 587,
                "from_address": "nissa-alerts@example.com",
                "rate_limit": 100  # per hour
            },
            "slack": {
                "webhook_url": "https://hooks.slack.com/services/...",
                "default_channel": "#alerts",
                "rate_limit": 500  # per hour
            },
            "pagerduty": {
                "integration_key": "YOUR_INTEGRATION_KEY",
                "api_url": "https://events.pagerduty.com/v2/enqueue",
                "rate_limit": 100  # per hour
            },
            "webhook": {
                "endpoints": [
                    "https://example.com/webhook/alerts",
                    "https://backup.example.com/webhook/alerts"
                ],
                "timeout_ms": 5000,
                "retry_attempts": 3
            }
        }
```

## Testing and Validation

### Load Testing for Alerting

```python
class NissaC016LoadTests:
    """Comprehensive load testing with C-016 specific scenarios"""

    async def test_circuit_breaker_behavior(self):
        """Test circuit breaker protection under failures"""

        # Simulate failure conditions
        failure_injection = FailureInjector()

        # Test circuit breaker opening
        await failure_injection.inject_failures("event_processing", rate=0.8)

        # Verify circuit breaker opens
        assert self.nissa.circuit_breaker.state == CircuitBreakerState.OPEN

        # Test fallback responses
        result = await self.nissa.process_events([test_event])
        assert result["status"] == "circuit_open"

        # Test recovery
        await failure_injection.stop_failures("event_processing")
        await asyncio.sleep(31)  # Wait for recovery timeout

        result = await self.nissa.process_events([test_event])
        assert result["status"] == "success"

    async def test_memory_management_effectiveness(self):
        """Test TTL-based memory cleanup prevents leaks"""

        initial_memory = self._get_memory_usage()

        # Generate high-volume data with TTL
        for epoch in range(100):
            for request_id in range(1000):
                await self.nissa.memory_manager.store_with_ttl(
                    "event", epoch, f"req_{request_id}",
                    {"data": "test" * 100},  # 400 bytes per entry
                    custom_ttl_ms=5_000  # 5 second TTL
                )

        # Verify memory growth
        peak_memory = self._get_memory_usage()
        assert peak_memory > initial_memory + 30_000_000  # ~30MB growth

        # Wait for TTL cleanup
        await asyncio.sleep(10)
        await self.nissa.memory_manager.cleanup_expired_entries()

        # Verify memory cleanup
        final_memory = self._get_memory_usage()
        assert final_memory < initial_memory + 5_000_000  # <5MB remaining

    async def test_conservative_mode_transitions(self):
        """Test conservative mode activation and recovery"""

        # Simulate high resource usage
        await self._simulate_resource_pressure(cpu=85, memory=90)

        # Verify transition to conservative mode
        await self.nissa.conservative_mode.evaluate_mode_transition({
            "cpu_usage_percent": 85,
            "memory_usage_percent": 90
        })

        assert self.nissa.conservative_mode.current_mode == ConservativeMode.CONSERVATIVE

        # Verify reduced processing limits
        limits = self.nissa.conservative_mode.get_processing_limits()
        assert limits["batch_size"] == 100  # Reduced from 1000
        assert limits["processing_delay_ms"] > 0

        # Simulate resource recovery
        await self._simulate_resource_pressure(cpu=40, memory=50)
        await asyncio.sleep(301)  # Wait for stability period

        # Verify recovery to normal mode
        await self.nissa.conservative_mode.evaluate_mode_transition({
            "cpu_usage_percent": 40,
            "memory_usage_percent": 50
        })

        assert self.nissa.conservative_mode.current_mode == ConservativeMode.NORMAL
```

## Troubleshooting Guide

### Common Issues and Solutions

```yaml
common_issues:
  circuit_breaker_open:
    symptoms:
      - Operations being rejected
      - Fallback responses returned
      - Circuit breaker OPEN state in logs
    causes:
      - High failure rate in component
      - Cascade failures from dependencies
      - Resource exhaustion
    solutions:
      - Check underlying component health
      - Verify resource availability
      - Manually reset circuit breaker if safe
      - Enable conservative mode if needed

  memory_leak_detected:
    symptoms:
      - Increasing memory usage over time
      - TTL cleanup not reducing memory
      - OOM warnings in logs
    causes:
      - TTL cleanup not running
      - Composite keys not being cleaned
      - Large objects retained beyond TTL
    solutions:
      - Check TTL cleanup process status
      - Verify (epoch, request_id) key format
      - Increase cleanup frequency
      - Reduce TTL values for high-volume data

  slo_error_budget_exhausted:
    symptoms:
      - SLO error budget near zero
      - High burn rate alerts firing
      - System automatically entering conservative mode
    causes:
      - Higher than expected error rates
      - Performance degradation
      - Increased system load
    solutions:
      - Identify sources of errors and latency
      - Implement additional performance optimizations
      - Consider adjusting SLO targets if unrealistic
      - Enable conservative mode to preserve remaining budget

  conservative_mode_stuck:
    symptoms:
      - System remains in conservative mode
      - Performance degraded for extended period
      - Mode transition logs show no recovery
    causes:
      - Persistent resource pressure
      - SLO thresholds set too aggressively
      - Underlying system issues not resolved
    solutions:
      - Check system resource utilization
      - Review conservative mode trigger thresholds
      - Manually force transition if system is healthy
      - Address underlying performance bottlenecks
```

## Performance Metrics

### Alert Processing Performance

- **Alert evaluation frequency**: 1 second
- **Alert routing latency P99**: 10ms
- **Notification delivery latency P99**: 100ms
- **Maximum concurrent alerts**: 10,000
- **DLQ capacity**: 100,000 items

### SLO Tracking Performance

- **SLO calculation frequency**: 1 minute
- **Error budget update latency**: <5 seconds
- **Maximum tracked SLOs**: 1,000
- **Historical data retention**: 90 days

## Related Documentation

- [10 - Nissa Unified Design](10-nissa-unified-design.md)
- [10.1 - Nissa Metrics and Telemetry Design](10.1-nissa-metrics-telemetry.md)
- [10.2 - Nissa Mission Control Design](10.2-nissa-mission-control.md)
- [09 - Oona Message Bus Design](09-oona-unified-design.md)