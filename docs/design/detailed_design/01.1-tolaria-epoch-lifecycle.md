# Tolaria - Epoch Lifecycle and Seed Coordination

**Parent Document**: [01-tolaria-unified-design.md](01-tolaria-unified-design.md)  
**Component Type**: Core Training Loop Management  
**Version**: 3.0 (Split from v2.2)  
**Date**: 2025-01-10

## Overview

This document details Tolaria's epoch lifecycle management, training loop structure, and multi-seed gradient aggregation algorithms. These components form the core temporal framework in which the host model evolves, managing the critical epoch boundaries where morphogenetic operations occur.

## Training Loop Structure

### Core Training Loop

```python
class TolariaTrainer:
    """Master training orchestrator with morphogenetic capabilities"""
    
    def train_epoch(self):
        """Core training loop with multi-seed gradient aggregation"""
        self.model.train()
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            # Standard forward pass through morphogenetic layers
            output = self.model(data)  # KasminaLayers embedded
            host_loss = self.criterion(output, target)
            
            # Collect seed losses for aggregation
            seed_losses = self.collect_seed_losses(batch_idx)
            
            # Multi-seed gradient aggregation (Algorithm below)
            aggregated_loss, telemetry = self.gradient_aggregator.aggregate_losses(
                host_loss=host_loss,
                seed_infos=seed_losses,
                host_batch_size=self.batch_size,
                epoch=self.current_epoch
            )
            
            # Backward pass with conflict resolution
            self.gradient_aggregator.backward_with_aggregation(
                host_model=self.model,
                host_loss=host_loss,
                seed_losses=seed_losses,
                host_batch_size=self.batch_size,
                epoch=self.current_epoch,
                total_epochs=self.total_epochs
            )
            
            # [C-016 FIX] Optimizer step via UnifiedLRController ONLY
            current_lrs = self.unified_lr_controller.step(
                epoch=self.current_epoch,
                metrics={
                    'host_loss': host_loss.item(),
                    'seed_losses': {s.seed_id: s.loss.item() for s in seed_losses}
                }
            )
            self.optimizer.step()
            
            # Async telemetry collection (<2ms guaranteed)
            self.telemetry_collector.collect_step_metrics(
                priority=TelemetryPriority.NORMAL,
                source='tolaria',
                event_type='step_metrics',
                payload={
                    'batch_idx': batch_idx,
                    'loss': aggregated_loss.item(),
                    'host_loss': host_loss.item(),
                    'num_active_seeds': telemetry['num_active_seeds']
                }
            )
        
        # End-of-epoch hook (critical synchronization point)
        self.end_of_epoch_hook()
```

### Key Design Points

1. **Zero Training Disruption**: All morphogenetic operations are non-blocking
2. **Multi-Seed Integration**: Seeds contribute to gradient updates based on lifecycle state
3. **Telemetry Async**: Metrics collection never blocks training (<2ms budget)
4. **LR Control**: All learning rate updates go through UnifiedLRController

## End-of-Epoch Hook

### Implementation with Timeout Protection

**[C-016 FIX] Enhanced with circuit breakers and conservative mode:**

```python
def end_of_epoch_hook(self):
    """Orchestrate epoch boundary operations within 18ms budget"""
    
    start_time = time.perf_counter()
    
    # 1. Validation phase
    val_metrics = self.validate()
    
    # 2. Assemble system state (3.5ms budget) - Protocol Buffer format
    system_state = SystemStatePacket(
        current_epoch=self.current_epoch,
        validation_accuracy=val_metrics['accuracy'],
        validation_loss=val_metrics['loss'],
        hardware_context=self.hardware_context,
        training_metrics=self.epoch_metrics
    )
    
    # Serialize using Protocol Buffer
    serialized_state = self.system_state_serializer.serialize(system_state)
    
    # 3. Invoke strategic controller with timeout wrapper (12ms budget)
    try:
        adaptation_command = self.tamiyo_timeout_wrapper.step(
            system_state=serialized_state,
            timeout_override=2.0  # 2-second maximum
        )
    except TamiyoTimeoutError:
        # Fallback to conservative decision
        adaptation_command = self.generate_fallback_adaptation_command(system_state)
    
    # 4. Process adaptation response (1.5ms budget)
    if adaptation_command.emergency_rollback_required:
        # Trigger fast rollback coordination
        asyncio.create_task(self.fast_rollback_coordinator.initiate_rollback(
            checkpoint_epoch=self.last_stable_checkpoint,
            reason=adaptation_command.metadata.get('reason', 'Unknown'),
            severity='CRITICAL'
        ))
    elif adaptation_command.optimizer_rebuild_required:
        # Coordinate optimizer rebuild with momentum preservation
        self.rebuild_coordinator.handle_parameter_addition(
            layer_id=adaptation_command.metadata.get('layer_id'),
            old_shape=adaptation_command.metadata.get('old_shape'),
            new_shape=adaptation_command.metadata.get('new_shape'),
            parameter_name=adaptation_command.metadata.get('parameter_name')
        )
    
    # 5. Checkpoint if stable (triggered asynchronously to avoid blocking)
    if self.should_checkpoint():
        asyncio.create_task(self.create_checkpoint_with_wal())
    
    # 6. [C-016 FIX] NO scheduler.step() - LR updates via UnifiedLRController only
    
    # Performance validation with circuit breaker
    elapsed_ms = (time.perf_counter() - start_time) * 1000
    MonotonicTimer.within_budget_ms(
        start_time, 
        18.0,  # 18ms budget
        lambda d: self._handle_timing_violation("epoch_boundary", d)
    )
```

### Timing Budget Breakdown

| Operation | Budget | Description |
|-----------|--------|-------------|
| Validation | Variable | Run before timing starts |
| System State Assembly | 3.5ms | Collect metrics and serialize |
| Tamiyo Invocation | 12ms | Strategic decision with timeout |
| Adaptation Processing | 1.5ms | Handle commands |
| Performance Check | 1ms | Validate timing |
| **Total** | **18ms** | Hardware physics limit |

## MonotonicTimer Utilities

**[C-016 FIX] Safe timing validation without system crashes:**

```python
from time import perf_counter
from typing import Callable, Optional

class MonotonicTimer:
    """Monotonic timing with circuit breakers instead of asserts"""
    
    @staticmethod
    def within_budget_ms(
        start: float, 
        budget_ms: float, 
        on_violation: Callable[[float], None]
    ) -> float:
        """Check timing budget without crashing system"""
        duration_ms = (perf_counter() - start) * 1000.0
        if duration_ms > budget_ms:
            on_violation(duration_ms)  # Emit metric, trigger conservative mode
        return duration_ms
        
    @staticmethod
    def start_timer() -> float:
        """Start monotonic timer"""
        return perf_counter()

    @staticmethod  
    def check_budget_with_circuit_breaker(
        start: float,
        budget_ms: float,
        circuit_breaker: 'CircuitBreaker',
        operation_name: str
    ) -> bool:
        """Check timing budget with circuit breaker protection"""
        duration_ms = (perf_counter() - start) * 1000.0
        
        if duration_ms > budget_ms:
            circuit_breaker.record_failure()
            logging.warning(
                f"Timing budget exceeded for {operation_name}: "
                f"{duration_ms:.2f}ms > {budget_ms}ms"
            )
            return False
        
        circuit_breaker.record_success()
        return True
```

## Multi-Seed Gradient Aggregation

### Complete Algorithm Implementation

```python
class MultiSeedGradientAggregator:
    """
    Complete algorithm for aggregating gradients from multiple seeds.
    Handles different lifecycle states, batch normalization, and gradient conflicts.
    """
    
    def __init__(
        self,
        conflict_threshold: float = -0.5,
        max_gradient_norm: float = 10.0,
        stability_epsilon: float = 1e-8,
        conflict_resolution_method: str = 'pcgrad'  # 'pcgrad', 'average', 'priority'
    ):
        self.conflict_threshold = conflict_threshold
        self.max_gradient_norm = max_gradient_norm
        self.epsilon = stability_epsilon
        self.conflict_resolution_method = conflict_resolution_method
        
        # Lifecycle state weights
        self.state_weights = {
            SeedLifecycleState.DORMANT: 0.0,
            SeedLifecycleState.GERMINATED: 0.01,
            SeedLifecycleState.TRAINING: 0.1,
            SeedLifecycleState.GRAFTING: None,  # Uses alpha value
            SeedLifecycleState.STABILIZATION: 0.5,
            SeedLifecycleState.EVALUATING: 0.0,  # No training
            SeedLifecycleState.FINE_TUNING: 1.0,
            SeedLifecycleState.FOSSILIZED: 0.0,
            SeedLifecycleState.CULLED: 0.0
        }
    
    def aggregate_losses(
        self,
        host_loss: torch.Tensor,
        seed_infos: List[SeedGradientInfo],
        host_batch_size: int,
        epoch: int = 0,
        total_epochs: int = 100
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """
        Main entry point for loss aggregation.
        
        Mathematical formulation:
        L_total = L_host + Î£(w_i * L_seed_i)
        
        Where w_i depends on:
        - Seed lifecycle state
        - Batch size normalization
        - Importance sampling weight
        - Alpha blending value (for GRAFTING)
        """
        # Start with host loss
        total_loss = host_loss.clone()
        
        # Telemetry
        telemetry = {
            'host_loss': host_loss.item(),
            'num_active_seeds': 0,
            'total_weight': 1.0,  # Host weight is 1.0
            'conflicts_detected': 0,
            'max_seed_contribution': 0.0
        }
        
        # Process each seed
        active_seeds = []
        for seed_info in seed_infos:
            weight = self._calculate_seed_weight(
                seed_info,
                host_batch_size,
                epoch,
                total_epochs
            )
            
            if weight > 0:
                # Normalize loss by batch size ratio
                batch_ratio = seed_info.batch_size / host_batch_size
                normalized_loss = seed_info.loss * batch_ratio
                
                # Apply weight
                weighted_loss = weight * normalized_loss
                
                # Add to total
                total_loss = total_loss + weighted_loss
                
                # Track active seeds
                active_seeds.append({
                    'seed_id': seed_info.seed_id,
                    'weight': weight,
                    'contribution': weighted_loss.item()
                })
                
                # Update telemetry
                telemetry['num_active_seeds'] += 1
                telemetry['total_weight'] += weight
                telemetry['max_seed_contribution'] = max(
                    telemetry['max_seed_contribution'],
                    weighted_loss.item()
                )
        
        # Apply numerical stability
        total_loss = self._apply_numerical_stability(total_loss, telemetry)
        
        return total_loss, telemetry
```

### Grafting Alpha Calculation

```python
def _calculate_grafting_alpha(
    self,
    epoch: int,
    total_epochs: int,
    temperature: float = 2.0
) -> float:
    """
    Calculate alpha value for GRAFTING state using sigmoid schedule.
    
    Î±(t) = sigmoid((t - t_mid) / Ï)
    
    Properties:
    - Smooth Câ transition
    - Monotonic increase
    - Bounded [0, 1]
    """
    if total_epochs <= 0:
        return 0.5
    
    t = epoch / total_epochs
    t_mid = 0.5  # Midpoint of grafting phase
    
    # Sigmoid function
    alpha = 1.0 / (1.0 + math.exp(-(t - t_mid) * temperature * 2 * math.pi))
    
    return alpha
```

### Gradient Conflict Resolution

```python
def _resolve_gradient_conflicts(
    self,
    host_gradients: Dict[str, torch.Tensor],
    seed_gradients: Dict[str, torch.Tensor],
    conflicts: List[str],
    seed_weight: float
) -> Dict[str, torch.Tensor]:
    """
    Resolve gradient conflicts using specified method.
    
    PCGrad method: Project conflicting gradients to be orthogonal
    """
    resolved = seed_gradients.copy()
    
    if self.conflict_resolution_method == 'pcgrad':
        # PCGrad: Project conflicting gradients
        for name in conflicts:
            host_grad = host_gradients[name]
            seed_grad = seed_gradients[name]
            
            # Project seed gradient to be orthogonal to host
            # g_seed_new = g_seed - (g_seed Â· g_host / ||g_host||Â²) * g_host
            dot_product = (seed_grad * host_grad).sum()
            host_norm_sq = (host_grad * host_grad).sum() + self.epsilon
            projection = (dot_product / host_norm_sq) * host_grad
            
            resolved[name] = seed_grad - projection
    
    return resolved
```

## Seed Lifecycle States and Weights

| State | Weight | Description |
|-------|--------|-------------|
| DORMANT | 0.0 | No gradient contribution |
| GERMINATED | 0.01 | Minimal exploration weight |
| TRAINING | 0.1 | Active training participation |
| GRAFTING | Î±(t) | Sigmoid-scheduled blending |
| STABILIZATION | 0.5 | Significant contribution |
| EVALUATING | 0.0 | No training updates |
| FINE_TUNING | 1.0 | Full weight contribution |
| FOSSILIZED | 0.0 | Frozen, no updates |
| CULLED | 0.0 | Removed from system |

## Performance Requirements

### Critical Timing Constraints

- **Epoch boundary operations**: 18ms total budget
- **Telemetry collection**: <2ms per batch
- **System state assembly**: 3.5ms maximum
- **Tamiyo invocation**: 12ms with 2s timeout fallback
- **Gradient aggregation**: Must complete within batch time

### Memory Management

- **Checkpoint cache**: In-memory for fast rollback
- **Gradient buffers**: Pre-allocated for active seeds
- **Telemetry queue**: Bounded with backpressure handling

## Integration Contract

### Inputs

- **Training data**: Standard PyTorch DataLoader format
- **Model**: Host model with embedded KasminaLayers
- **Seeds**: Active seed configurations and states

### Outputs

- **SystemStatePacket**: Serialized epoch metrics to Tamiyo
- **Telemetry**: Async metrics to Oona message bus
- **Checkpoints**: WAL-protected state snapshots

### Dependencies

- **UnifiedLRController**: All LR updates (see [01.3-tolaria-optimizer-lr.md](01.3-tolaria-optimizer-lr.md))
- **Fast Rollback System**: Emergency recovery (see [01.2-tolaria-rollback-systems.md](01.2-tolaria-rollback-systems.md))
- **Protocol Buffers**: Message serialization (see [01.4-tolaria-integration-protocols.md](01.4-tolaria-integration-protocols.md))

## References

- Parent: [01-tolaria-unified-design.md](01-tolaria-unified-design.md)
- Related: [01.3-tolaria-optimizer-lr.md](01.3-tolaria-optimizer-lr.md) for LR management
- Related: [01.2-tolaria-rollback-systems.md](01.2-tolaria-rollback-systems.md) for emergency procedures
- Integration: [01.4-tolaria-integration-protocols.md](01.4-tolaria-integration-protocols.md) for protocols