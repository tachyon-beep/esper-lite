# Urabrask - Safety Validation Framework

## Document Metadata

| Field | Value |
|-------|-------|
| **Parent Document** | [07-urabrask-unified-design.md](07-urabrask-unified-design.md) |
| **Component Type** | Safety Framework |
| **Version** | 3.0.0 |
| **Status** | PRODUCTION |
| **Implementation** | Complete |

## Overview

The Urabrask Safety Validation Framework provides comprehensive multi-layer safety validation for all compiled kernels before production deployment. This framework is the first stage of the two-stage validation pipeline, ensuring kernels are safe for execution through enhanced sandbox testing, memory leak detection, gradient health analysis, and chaos testing scenarios.

Key characteristics:
- **Multi-Layer Validation**: Sandbox execution, memory leak detection, gradient health analysis, and chaos testing
- **30-Second Timing Budget**: All safety validation occurs within a 30-second window with circuit breaker protection
- **Circuit Breaker Protection**: Graceful degradation in case of failures replacing all assert statements

## Technical Design

### Architecture

```
+-------------------------------+
| ComprehensiveSafetyValidator  |
+-------------------------------+
| - validate_comprehensive_safety() |
| - _sandbox_execution_test()    |
| - _memory_leak_detection()     |
| - _gradient_health_analysis()  |
| - _chaos_validation_scenarios() |
+-------------------------------+
         |
         v
+-------------------------------+
|    EnhancedSandboxExecution   |
+-------------------------------+
| - create_enhanced_sandbox()    |
| - _protected_kernel_execution() |
| - cleanup_resources()          |
+-------------------------------+
         |
         v
+-------------------------------+
|      MemoryLeakDetector       |
+-------------------------------+
| - comprehensive_memory_leak_test() |
| - _emit_memory_leak_telemetry()    |
| - cleanup_expired_allocations()    |
+-------------------------------+
         |
         v
+-------------------------------+
|    GradientHealthAnalyzer     |
+-------------------------------+
| - analyze_gradient_health()    |
| - _enhanced_identity_detection() |
| - _evaluate_gradient_health()  |
+-------------------------------+
         |
         v
+-------------------------------+
|    ValidationChaosEngine      |
+-------------------------------+
| - inject_scenario()            |
| - chaos_validation_scenarios() |
| - cleanup_chaos_resources()    |
+-------------------------------+
```

### Core Abstractions

**ComprehensiveSafetyValidator**
```python
# Import Leyline shared contracts
from esper.leyline.contracts import (
    SystemStatePacket,
    HardwareContext,
    TelemetryPacket,
    TelemetryEvent,
    TelemetryLevel,
    SeedState,
    SeedLifecycleStage
)

class ComprehensiveSafetyValidator:
    """C-016 Enhanced safety validation with circuit breakers and Leyline integration."""

    def __init__(self):
        self.sandbox_config = EnhancedSandboxConfig()
        self.memory_leak_detector = MemoryLeakDetector()
        self.gradient_analyzer = GradientHealthAnalyzer()
        self.chaos_tester = ValidationChaosEngine()

    def validate_comprehensive_safety(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> SafetyValidationReport:
        """Comprehensive safety validation with 30 second budget using Leyline contracts."""

        start_time = perf_counter()
        report = SafetyValidationReport(kernel_id=kernel.id)

        # Create hardware context using Leyline standard format
        hardware_context = self._create_hardware_context(gpu_slice)
        report.hardware_context = hardware_context

        # Test 1: Enhanced Sandbox Execution with circuit breakers
        execution_result = self._sandbox_execution_test_with_breakers(kernel, gpu_slice)
        report.add_check("sandbox_execution", execution_result)

        if not execution_result.passed:
            return report

        # Test 2: Memory leak detection over multiple cycles
        memory_result = self._comprehensive_memory_leak_test(kernel, gpu_slice)
        report.add_check("memory_leak_detection", memory_result)

        # Test 3: Gradient health analysis with realistic bounds
        gradient_result = self._gradient_health_analysis(kernel, gpu_slice)
        report.add_check("gradient_health", gradient_result)

        # Test 4: C-016 Chaos testing scenarios
        chaos_result = self._chaos_validation_scenarios(kernel, gpu_slice)
        report.add_check("chaos_resilience", chaos_result)

        # C-016 Timing budget check with circuit breaker
        elapsed_ms = (perf_counter() - start_time) * 1000
        if elapsed_ms > 30000:  # 30 second budget
            self._handle_timing_violation("safety_validation", elapsed_ms)
            report.add_violation("timing_exceeded", f"Safety validation took {elapsed_ms:.1f}ms")

        return report

    def _create_hardware_context(self, gpu_slice: GPUMemorySlice) -> HardwareContext:
        """Create hardware context using Leyline standard format"""
        import torch

        device = gpu_slice.device
        if device.type == 'cuda':
            props = torch.cuda.get_device_properties(device)
            memory_total = props.total_memory / (1024**3)  # GB
            memory_free = (props.total_memory - torch.cuda.memory_allocated(device)) / (1024**3)

            # Get utilization if available
            utilization = 0.0
            temperature = 65.0  # Default safe temperature

            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(device.index)

                util = pynvml.nvmlDeviceGetUtilizationRates(handle)
                utilization = float(util.gpu)
                temperature = float(pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU))
            except:
                pass  # Fallback to defaults

            return HardwareContext(
                device_type="cuda",
                device_id=f"cuda:{device.index}",
                total_memory_gb=memory_total,
                available_memory_gb=memory_free,
                temperature_celsius=temperature,
                utilization_percent=utilization,
                compute_capability=int(props.major * 10 + props.minor)
            )
        else:
            # CPU fallback
            import psutil
            return HardwareContext(
                device_type="cpu",
                device_id="cpu:0",
                total_memory_gb=psutil.virtual_memory().total / (1024**3),
                available_memory_gb=psutil.virtual_memory().available / (1024**3),
                temperature_celsius=65.0,
                utilization_percent=psutil.cpu_percent(),
                compute_capability=0
            )
```

**EnhancedSandboxExecution**
- Purpose: Executes kernels in a secure sandbox with resource limits
- Key Methods: `_sandbox_execution_test_with_breakers`, `_protected_kernel_execution`
- State Management: Manages sandbox lifecycle and resource cleanup

### Algorithms

#### Sandbox Execution with Circuit Breakers

**Purpose**: To execute kernels in a secure environment with graceful failure handling.

**Approach**:
1. Create an enhanced sandbox with memory and time limits
2. Execute the kernel with progressively larger inputs
3. Verify that the kernel executes without errors and that the output is sane
4. Use circuit breakers instead of assertions for failure handling

**Implementation**:
```python
def _sandbox_execution_test_with_breakers(
    self,
    kernel: CompiledKernelArtifact,
    gpu_slice: GPUMemorySlice
) -> ExecutionSafetyResult:
    """Enhanced sandbox execution with circuit breaker protection."""

    sandbox_config = SandboxConfig(
        memory_limit_gb=4,           # Match validation budget
        time_limit_seconds=25,       # Within 30s safety budget
        gpu_memory_slice=gpu_slice,  # Use allocated slice
        network_access=False,
        filesystem_access="read-only",
        process_isolation=True       # C-016 Enhanced isolation
    )

    with create_enhanced_sandbox(sandbox_config) as sandbox:
        try:
            # Progressive test complexity with circuit breaker protection
            test_results = []

            # Test 1: Small input (32x64)
            small_input = torch.randn(32, 64, device=gpu_slice.device)
            result = self._protected_kernel_execution(kernel, small_input, sandbox)
            test_results.append(result)

            # Test 2: Medium input (128x256)
            if result.success:
                medium_input = torch.randn(128, 256, device=gpu_slice.device)
                result = self._protected_kernel_execution(kernel, medium_input, sandbox)
                test_results.append(result)

            # Test 3: Large input (512x1024) if previous tests pass
            if result.success and gpu_slice.available_memory_gb > 2:
                large_input = torch.randn(512, 1024, device=gpu_slice.device)
                result = self._protected_kernel_execution(kernel, large_input, sandbox)
                test_results.append(result)

            # Evaluate overall execution safety
            overall_success = all(r.success for r in test_results)

            return ExecutionSafetyResult(
                passed=overall_success,
                test_results=test_results,
                sandbox_stats=sandbox.get_stats(),
                reason="All execution tests passed" if overall_success
                       else "One or more execution tests failed"
            )

        except Exception as e:
            return ExecutionSafetyResult(
                passed=False,
                reason=f"Sandbox execution failed: {str(e)}",
                exception_details=str(e)
            )

def _protected_kernel_execution(
    self,
    kernel: CompiledKernelArtifact,
    input_tensor: torch.Tensor,
    sandbox: EnhancedSandbox
) -> KernelExecutionResult:
    """Execute kernel with circuit breaker protection."""

    execution_start = perf_counter()

    try:
        # Forward pass with timeout protection
        with timeout_protection(15.0):  # 15 second per-execution limit
            output = kernel.forward(input_tensor)

            # Backward pass if supported
            if hasattr(kernel, 'backward') and output.requires_grad:
                loss = output.sum()
                loss.backward()

            # Verify output sanity
            if torch.isnan(output).any() or torch.isinf(output).any():
                return KernelExecutionResult(
                    success=False,
                    reason="Output contains NaN or Inf values"
                )

            execution_time_ms = (perf_counter() - execution_start) * 1000

            return KernelExecutionResult(
                success=True,
                execution_time_ms=execution_time_ms,
                output_shape=output.shape,
                memory_peak_mb=sandbox.get_peak_memory_usage_mb()
            )

    except TimeoutError:
        return KernelExecutionResult(
            success=False,
            reason="Kernel execution timed out after 15 seconds"
        )
    except Exception as e:
        return KernelExecutionResult(
            success=False,
            reason=f"Kernel execution error: {str(e)}"
        )
```

#### Memory Leak Detection

**Purpose**: To detect memory leaks in kernels through multi-cycle testing.

**Approach**:
1. Run the kernel for 20 cycles with varying input sizes
2. Monitor the memory usage at the beginning and end of each cycle
3. If the memory usage grows beyond 20% threshold, a memory leak is detected
4. Use realistic thresholds (not 50%, but 20% growth tolerance)

**Complexity**:
- Time: O(n) where n is the number of cycles (20)
- Space: O(1) for tracking memory snapshots

**Implementation**:
```python
class MemoryLeakDetector:
    """C-016 Enhanced memory leak detection with circuit breakers and telemetry integration."""

    def comprehensive_memory_leak_test(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> MemoryLeakTestResult:
        """Enhanced memory leak testing over multiple cycles with Leyline telemetry."""

        baseline_memory = torch.cuda.memory_allocated()
        memory_snapshots = []

        try:
            # Run 20 cycles (increased from 10 for better detection)
            for cycle in range(20):
                cycle_start_memory = torch.cuda.memory_allocated()

                # Create test input of varying sizes
                input_size = 32 + (cycle * 16)  # Growing input size
                input_tensor = torch.randn(input_size, 64, device=gpu_slice.device)

                # Execute kernel
                output = kernel.forward(input_tensor)

                # Force backward pass if supported
                if hasattr(kernel, 'backward') and output.requires_grad:
                    loss = output.sum()
                    loss.backward()

                # Clean up references
                del input_tensor, output
                if 'loss' in locals():
                    del loss

                # Force garbage collection
                torch.cuda.empty_cache()

                cycle_end_memory = torch.cuda.memory_allocated()
                memory_snapshots.append({
                    'cycle': cycle,
                    'start_memory': cycle_start_memory,
                    'end_memory': cycle_end_memory,
                    'delta': cycle_end_memory - cycle_start_memory
                })

            # Analyze memory growth pattern
            final_memory = torch.cuda.memory_allocated()
            total_growth = final_memory - baseline_memory

            # C-016 Realistic thresholds (not 50%, but 20% growth tolerance)
            growth_threshold = baseline_memory * 0.20  # 20% growth tolerance

            leak_detected = total_growth > growth_threshold

            # Calculate growth rate per cycle
            if len(memory_snapshots) > 5:
                recent_deltas = [s['delta'] for s in memory_snapshots[-5:]]
                avg_recent_growth = sum(recent_deltas) / len(recent_deltas)
            else:
                avg_recent_growth = 0

            # Emit telemetry using Leyline TelemetryPacket if leak detected
            if leak_detected:
                self._emit_memory_leak_telemetry(total_growth, baseline_memory)

            return MemoryLeakTestResult(
                leak_detected=leak_detected,
                total_growth_bytes=total_growth,
                baseline_memory_bytes=baseline_memory,
                final_memory_bytes=final_memory,
                growth_rate_per_cycle=avg_recent_growth,
                memory_snapshots=memory_snapshots,
                reason="Memory leak detected" if leak_detected
                       else "Memory usage within acceptable bounds"
            )

        except Exception as e:
            return MemoryLeakTestResult(
                leak_detected=True,  # Assume leak on error
                reason=f"Memory leak test failed: {str(e)}",
                error=str(e)
            )

    def _emit_memory_leak_telemetry(self, total_growth: int, baseline_memory: int):
        """Emit memory leak telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="memory_leak_detected",
            severity=TelemetryLevel.TELEMETRY_WARN,
            message=f"Memory leak detected: {total_growth / (1024*1024):.1f}MB growth",
            attributes={
                "total_growth_bytes": str(total_growth),
                "baseline_memory_bytes": str(baseline_memory),
                "growth_percentage": str(total_growth / baseline_memory * 100)
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_WARN,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)
```

### Data Structures

#### ValidationMemoryManager

```python
class ValidationMemoryManager:
    """C-016 Memory management with bounded allocation and TTL cleanup using Leyline contracts."""

    def __init__(self, gpu_budget_gb: float = 4, cleanup_interval_epochs: int = 100):
        self.gpu_budget_gb = gpu_budget_gb
        self.cleanup_interval_epochs = cleanup_interval_epochs
        self.allocations: Dict[str, GPUMemorySlice] = {}
        self.allocation_times: Dict[str, float] = {}
        self.ttl_seconds = 3600  # 1 hour TTL
        self.epoch_counter = 0

    @contextmanager
    def acquire_gpu_slice(self, kernel_id: str) -> GPUMemorySlice:
        """Acquire bounded GPU memory slice for validation."""

        # Check availability with circuit breaker
        if self.get_current_gpu_usage_gb() + 1.0 > self.gpu_budget_gb:
            # Force cleanup before failing
            self.cleanup_expired_allocations()
            if self.get_current_gpu_usage_gb() + 1.0 > self.gpu_budget_gb:
                raise MemoryBudgetExceededError(
                    f"Cannot allocate 1GB slice, current usage: {self.get_current_gpu_usage_gb():.2f}GB"
                )

        # Allocate slice
        slice_id = f"{kernel_id}_{int(time.time())}"
        gpu_slice = GPUMemorySlice(
            slice_id=slice_id,
            size_gb=1.0,
            device=torch.device("cuda:0"),
            kernel_id=kernel_id
        )

        self.allocations[slice_id] = gpu_slice
        self.allocation_times[slice_id] = time.time()

        try:
            yield gpu_slice
        finally:
            # Immediate cleanup on release
            if slice_id in self.allocations:
                del self.allocations[slice_id]
                del self.allocation_times[slice_id]
            gpu_slice.cleanup()

    def cleanup_expired_allocations(self) -> None:
        """C-016 TTL-based memory cleanup to prevent leaks."""

        current_time = time.time()
        expired_slices = []

        for slice_id, alloc_time in self.allocation_times.items():
            if current_time - alloc_time > self.ttl_seconds:
                expired_slices.append(slice_id)

        for slice_id in expired_slices:
            if slice_id in self.allocations:
                self.allocations[slice_id].cleanup()
                del self.allocations[slice_id]
                del self.allocation_times[slice_id]

        # Periodic garbage collection every 100 epochs
        self.epoch_counter += 1
        if self.epoch_counter % self.cleanup_interval_epochs == 0:
            torch.cuda.empty_cache()
            self.metrics.garbage_collections_total.inc()

    def get_current_gpu_usage_gb(self) -> float:
        """Get current GPU memory usage in GB."""
        return sum(slice.size_gb for slice in self.allocations.values())
```

#### GPUMemorySlice

```python
class GPUMemorySlice:
    """Individual GPU memory slice with cleanup capabilities and Leyline integration"""

    def __init__(self, slice_id: str, size_gb: float, device: torch.device, kernel_id: str):
        self.slice_id = slice_id
        self.size_gb = size_gb
        self.device = device
        self.kernel_id = kernel_id
        self.allocated_tensors: List[torch.Tensor] = []
        self.peak_usage_mb = 0
        self.created_at = time.time()

    def allocate_tensor(self, *shape) -> torch.Tensor:
        """Allocate tensor within this memory slice"""

        tensor = torch.empty(*shape, device=self.device)
        self.allocated_tensors.append(tensor)

        # Track peak usage
        current_usage = sum(t.numel() * t.element_size() for t in self.allocated_tensors)
        self.peak_usage_mb = max(self.peak_usage_mb, current_usage / (1024 * 1024))

        return tensor

    def get_available_memory_gb(self) -> float:
        """Get available memory in this slice"""

        used_gb = sum(
            t.numel() * t.element_size() for t in self.allocated_tensors
        ) / (1024 ** 3)

        return max(0, self.size_gb - used_gb)

    def cleanup(self) -> None:
        """Clean up all tensors in this slice"""

        for tensor in self.allocated_tensors:
            try:
                del tensor
            except:
                pass

        self.allocated_tensors.clear()
        torch.cuda.empty_cache()

    def to_hardware_context(self) -> HardwareContext:
        """Convert slice information to Leyline HardwareContext format"""
        return HardwareContext(
            device_type=self.device.type,
            device_id=str(self.device),
            total_memory_gb=self.size_gb,
            available_memory_gb=self.get_available_memory_gb(),
            temperature_celsius=65.0,  # Default - actual temp from monitor if available
            utilization_percent=0.0,   # Default - actual utilization if available
            compute_capability=0       # Default - actual compute capability if available
        )
```

## Gradient Health Analysis

### Enhanced Gradient Health Analysis

```python
class GradientHealthAnalyzer:
    """C-016 Enhanced gradient health analysis with realistic bounds and Leyline integration."""

    def analyze_gradient_health(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> GradientHealthResult:
        """Enhanced gradient health measurement with realistic analysis and telemetry."""

        input_tensor = torch.randn(64, 128, requires_grad=True, device=gpu_slice.device)

        try:
            # Forward pass
            output = kernel.forward(input_tensor)

            # Multiple backward passes to test stability
            gradient_norms = []
            for i in range(5):  # Test gradient consistency
                if input_tensor.grad is not None:
                    input_tensor.grad.zero_()

                loss = output.sum() * (0.8 + 0.4 * i)  # Vary loss scaling
                loss.backward(retain_graph=True)

                grad_norm = torch.norm(input_tensor.grad).item()
                gradient_norms.append(grad_norm)

            # Analyze gradient characteristics
            mean_grad_norm = np.mean(gradient_norms)
            std_grad_norm = np.std(gradient_norms)
            cv_grad_norm = std_grad_norm / mean_grad_norm if mean_grad_norm > 0 else float('inf')

            # Identity kernel detection
            is_identity = self._enhanced_identity_detection(kernel, gpu_slice)

            # C-016 Enhanced gradient health criteria
            gradient_healthy = self._evaluate_gradient_health(
                mean_grad_norm, cv_grad_norm, is_identity, gradient_norms
            )

            # Emit telemetry if gradients are unhealthy
            if not gradient_healthy:
                self._emit_gradient_health_telemetry(mean_grad_norm, cv_grad_norm, is_identity)

            return GradientHealthResult(
                healthy=gradient_healthy,
                gradient_norm_mean=mean_grad_norm,
                gradient_norm_std=std_grad_norm,
                gradient_cv=cv_grad_norm,
                gradient_stability=cv_grad_norm < 0.3,  # 30% CV threshold
                is_identity=is_identity,
                all_gradient_norms=gradient_norms,
                reason=self._generate_gradient_health_reason(
                    gradient_healthy, mean_grad_norm, cv_grad_norm, is_identity
                )
            )

        except Exception as e:
            return GradientHealthResult(
                healthy=False,
                reason=f"Gradient computation failed: {str(e)}",
                error=str(e)
            )

    def _evaluate_gradient_health(
        self,
        mean_norm: float,
        cv: float,
        is_identity: bool,
        all_norms: List[float]
    ) -> bool:
        """C-016 Enhanced gradient health evaluation with multiple criteria."""

        # Identity kernels are always healthy
        if is_identity:
            return True

        # Check for pathological gradients
        if any(np.isnan(norm) or np.isinf(norm) for norm in all_norms):
            return False

        # Vanishing gradient detection
        if mean_norm < 1e-7:
            return False

        # Exploding gradient detection (more permissive than original 1e3)
        if mean_norm > 1e4:
            return False

        # Stability check - gradients shouldn't vary too much
        if cv > 0.5:  # 50% CV threshold for stability
            return False

        # Check for monotonic decay (could indicate dying gradients)
        if len(all_norms) > 3:
            decreasing_trend = all(
                all_norms[i] >= all_norms[i+1] for i in range(len(all_norms)-1)
            )
            if decreasing_trend and all_norms[-1] < all_norms[0] * 0.1:
                return False  # Rapid decay indicates problems

        return True

    def _enhanced_identity_detection(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> bool:
        """Enhanced identity kernel detection with multiple test sizes."""

        test_sizes = [(8, 8), (16, 32), (32, 16)]
        identity_results = []

        for height, width in test_sizes:
            test_input = torch.randn(height, width, device=gpu_slice.device)

            try:
                output = kernel.forward(test_input)
                is_identity = torch.allclose(test_input, output, rtol=1e-4, atol=1e-5)
                identity_results.append(is_identity)
            except:
                identity_results.append(False)

        # Kernel is identity if it passes all size tests
        return all(identity_results)

    def _emit_gradient_health_telemetry(self, mean_norm: float, cv: float, is_identity: bool):
        """Emit gradient health telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        severity = TelemetryLevel.TELEMETRY_WARN
        if mean_norm < 1e-7 or mean_norm > 1e4:
            severity = TelemetryLevel.TELEMETRY_ERROR

        telemetry_event = TelemetryEvent(
            event_name="unhealthy_gradients_detected",
            severity=severity,
            message=f"Unhealthy gradients: mean_norm={mean_norm:.2e}, cv={cv:.3f}",
            attributes={
                "gradient_norm_mean": str(mean_norm),
                "gradient_cv": str(cv),
                "is_identity": str(is_identity)
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=severity,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

    def _generate_gradient_health_reason(
        self,
        healthy: bool,
        mean_norm: float,
        cv: float,
        is_identity: bool
    ) -> str:
        """Generate detailed reason for gradient health assessment."""

        if is_identity:
            return "Identity kernel detected - gradients always healthy"

        if not healthy:
            if mean_norm < 1e-7:
                return f"Vanishing gradients detected (mean norm: {mean_norm:.2e})"
            elif mean_norm > 1e4:
                return f"Exploding gradients detected (mean norm: {mean_norm:.2e})"
            elif cv > 0.5:
                return f"Unstable gradients detected (CV: {cv:.3f})"
            else:
                return "Gradient pathology detected"

        return f"Healthy gradients (mean: {mean_norm:.2e}, CV: {cv:.3f})"
```

## Chaos Testing and Resilience Validation

### ValidationChaosEngine Implementation

```python
class ValidationChaosEngine:
    """C-016 Chaos engineering for validation pipeline testing with Leyline integration."""

    def __init__(self):
        self.scenarios = [
            MemoryPressureScenario(),
            NetworkLatencyScenario(),
            GPUThrottlingScenario(),
            FileSystemSlowdownScenario(),
            ProcessorLoadScenario()
        ]

    @contextmanager
    def inject_scenario(self) -> ChaosScenario:
        """Inject chaos scenario during validation."""

        scenario = random.choice(self.scenarios)

        try:
            scenario.activate()
            yield scenario
        finally:
            scenario.deactivate()

    def chaos_validation_scenarios(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> ChaosTestResult:
        """Execute chaos testing scenarios for resilience validation with telemetry."""

        chaos_results = []

        # Test each scenario type
        for scenario_class in [MemoryPressureScenario, GPUThrottlingScenario, ProcessorLoadScenario]:
            try:
                with scenario_class() as scenario:
                    scenario.activate()

                    # Execute kernel under chaos conditions
                    test_input = torch.randn(64, 64, device=gpu_slice.device)
                    start_time = perf_counter()

                    output = kernel.forward(test_input)
                    execution_time = perf_counter() - start_time

                    # Check if kernel remained stable
                    stable = not (torch.isnan(output).any() or torch.isinf(output).any())
                    performance_degraded = execution_time > 5.0  # 5 second threshold

                    chaos_results.append(ChaosScenarioResult(
                        scenario_name=scenario_class.__name__,
                        kernel_stable=stable,
                        performance_degraded=performance_degraded,
                        execution_time_seconds=execution_time,
                        passed=stable and not performance_degraded
                    ))

                    # Emit telemetry for failed scenarios
                    if not (stable and not performance_degraded):
                        self._emit_chaos_failure_telemetry(scenario_class.__name__, stable, execution_time)

            except Exception as e:
                chaos_results.append(ChaosScenarioResult(
                    scenario_name=scenario_class.__name__,
                    kernel_stable=False,
                    performance_degraded=True,
                    execution_time_seconds=float('inf'),
                    passed=False,
                    error=str(e)
                ))

        # Evaluate overall chaos resilience
        overall_passed = all(result.passed for result in chaos_results)

        return ChaosTestResult(
            passed=overall_passed,
            scenario_results=chaos_results,
            resilience_score=sum(1 for r in chaos_results if r.passed) / len(chaos_results),
            reason="Chaos testing completed" if overall_passed else "Failed chaos resilience tests"
        )

    def _emit_chaos_failure_telemetry(self, scenario_name: str, stable: bool, execution_time: float):
        """Emit chaos failure telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="chaos_scenario_failure",
            severity=TelemetryLevel.TELEMETRY_WARN,
            message=f"Chaos scenario {scenario_name} failed: stable={stable}, time={execution_time:.2f}s",
            attributes={
                "scenario_name": scenario_name,
                "kernel_stable": str(stable),
                "execution_time_seconds": str(execution_time)
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_WARN,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)
```

### Chaos Scenario Implementations

```python
class MemoryPressureScenario:
    """Simulate memory pressure during validation."""

    def __init__(self):
        self.pressure_tensors = []
        self.active = False

    def activate(self):
        """Create memory pressure by allocating tensors."""
        self.active = True
        try:
            # Allocate tensors to create memory pressure (but not exhaust memory)
            for i in range(10):
                tensor = torch.randn(512, 512, device='cuda:0')
                self.pressure_tensors.append(tensor)
        except RuntimeError:
            # Expected when memory is exhausted
            pass

    def deactivate(self):
        """Release memory pressure."""
        for tensor in self.pressure_tensors:
            del tensor
        self.pressure_tensors.clear()
        torch.cuda.empty_cache()
        self.active = False

class GPUThrottlingScenario:
    """Simulate GPU thermal throttling."""

    def __init__(self):
        self.load_tensors = []
        self.active = False

    def activate(self):
        """Create GPU load to simulate thermal throttling."""
        self.active = True
        # Create computational load
        for i in range(5):
            tensor = torch.randn(1024, 1024, device='cuda:0')
            # Perform intensive computation
            result = torch.mm(tensor, tensor.t())
            self.load_tensors.append(result)

    def deactivate(self):
        """Remove GPU load."""
        for tensor in self.load_tensors:
            del tensor
        self.load_tensors.clear()
        torch.cuda.empty_cache()
        self.active = False

class ProcessorLoadScenario:
    """Simulate CPU load affecting kernel execution."""

    def __init__(self):
        self.load_thread = None
        self.stop_load = False

    def activate(self):
        """Start CPU load simulation."""
        self.stop_load = False
        self.load_thread = threading.Thread(target=self._cpu_load_worker)
        self.load_thread.start()

    def deactivate(self):
        """Stop CPU load simulation."""
        self.stop_load = True
        if self.load_thread:
            self.load_thread.join(timeout=1.0)

    def _cpu_load_worker(self):
        """Worker function to create CPU load."""
        import time
        while not self.stop_load:
            # Create CPU load
            x = sum(i * i for i in range(1000))
            time.sleep(0.001)  # Small sleep to prevent 100% CPU usage
```

## Conservative Mode Emergency Validation

### EmergencyValidationPath Implementation

```python
class EmergencyValidationPath:
    """C-016 Emergency validation path <30 seconds with Leyline integration"""

    def __init__(self, config: EmergencyConfig):
        self.config = config
        self.cpu_device = torch.device("cpu")

    def execute_emergency_validation(
        self,
        kernel: CompiledKernelArtifact
    ) -> ValidationReport:
        """Execute emergency validation in <30 seconds using CPU with Leyline contracts"""

        start_time = perf_counter()
        emergency_budget_ms = 30000  # 30 seconds hard limit

        try:
            # Create CPU-based hardware context
            hardware_context = HardwareContext(
                device_type="cpu",
                device_id="cpu:0",
                total_memory_gb=psutil.virtual_memory().total / (1024**3),
                available_memory_gb=psutil.virtual_memory().available / (1024**3),
                temperature_celsius=65.0,  # Default CPU temp
                utilization_percent=psutil.cpu_percent(),
                compute_capability=0
            )

            # Stage 1: CPU-only execution safety (10 seconds budget)
            safety_result = self._cpu_execution_safety_check(kernel, budget_ms=10000)

            if not safety_result.safe:
                return ValidationReport(
                    kernel_id=kernel.id,
                    validation_mode=ValidationMode.EMERGENCY,
                    passed=False,
                    safety_report=safety_result,
                    validation_time_ms=self._get_elapsed_ms(start_time),
                    reason="Emergency safety check failed",
                    conservative_mode_triggered=True,
                    hardware_context=hardware_context
                )

            # Stage 2: Minimal performance check (15 seconds budget)
            perf_result = self._minimal_performance_check(kernel, budget_ms=15000)

            # Stage 3: Final assessment (5 seconds budget)
            validation_passed = safety_result.safe and perf_result.acceptable

            # Emit emergency validation telemetry
            self._emit_emergency_validation_telemetry(validation_passed, self._get_elapsed_ms(start_time))

            return ValidationReport(
                kernel_id=kernel.id,
                validation_mode=ValidationMode.EMERGENCY,
                passed=validation_passed,
                safety_report=safety_result,
                benchmark_results=perf_result,
                validation_time_ms=self._get_elapsed_ms(start_time),
                reason="Emergency validation complete",
                conservative_mode_triggered=True,
                hardware_context=hardware_context
            )

        except Exception as e:
            return ValidationReport(
                kernel_id=kernel.id,
                validation_mode=ValidationMode.EMERGENCY,
                passed=False,
                validation_time_ms=self._get_elapsed_ms(start_time),
                reason=f"Emergency validation error: {str(e)}",
                conservative_mode_triggered=True
            )

        finally:
            elapsed_ms = self._get_elapsed_ms(start_time)
            if elapsed_ms > emergency_budget_ms:
                logging.warning(
                    f"Emergency validation exceeded 30s budget: {elapsed_ms:.1f}ms"
                )

    def _cpu_execution_safety_check(
        self,
        kernel: CompiledKernelArtifact,
        budget_ms: int
    ) -> SafetyValidationResult:
        """CPU-only safety check within time budget"""

        start_time = perf_counter()

        try:
            # Small test inputs on CPU
            test_inputs = [
                torch.randn(8, 8, device=self.cpu_device),
                torch.randn(16, 16, device=self.cpu_device)
            ]

            for i, test_input in enumerate(test_inputs):
                # Check time budget
                if (perf_counter() - start_time) * 1000 > budget_ms:
                    break

                # Execute kernel
                output = kernel.forward(test_input)

                # Basic sanity checks
                if torch.isnan(output).any() or torch.isinf(output).any():
                    return SafetyValidationResult(
                        safe=False,
                        reason=f"Output contains NaN/Inf on test {i+1}"
                    )

                # Gradient check if applicable
                if hasattr(kernel, 'backward') and output.requires_grad:
                    loss = output.sum()
                    loss.backward()

                    if test_input.grad is not None and torch.isnan(test_input.grad).any():
                        return SafetyValidationResult(
                            safe=False,
                            reason=f"Gradient contains NaN on test {i+1}"
                        )

            return SafetyValidationResult(
                safe=True,
                reason="Emergency CPU safety checks passed",
                duration_ms=(perf_counter() - start_time) * 1000
            )

        except Exception as e:
            return SafetyValidationResult(
                safe=False,
                reason=f"Emergency safety check failed: {str(e)}"
            )

    def _emit_emergency_validation_telemetry(self, passed: bool, elapsed_ms: float):
        """Emit emergency validation telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        severity = TelemetryLevel.TELEMETRY_WARN if not passed else TelemetryLevel.TELEMETRY_INFO

        telemetry_event = TelemetryEvent(
            event_name="emergency_validation_executed",
            severity=severity,
            message=f"Emergency validation {'passed' if passed else 'failed'} in {elapsed_ms:.1f}ms",
            attributes={
                "validation_passed": str(passed),
                "elapsed_ms": str(elapsed_ms),
                "emergency_mode": "true"
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=severity,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)
```

## Safety Validation Data Structures

### Validation Results and Reports

```python
@dataclass
class SafetyValidationReport:
    """Comprehensive safety validation report with Leyline contract integration."""
    kernel_id: str
    checks: List[SafetyCheck] = field(default_factory=list)
    violations: List[SafetyViolation] = field(default_factory=list)
    overall_safe: bool = False
    total_duration_ms: float = 0.0
    timestamp: float = field(default_factory=time.time)
    hardware_context: Optional[HardwareContext] = None  # From Leyline

    def add_check(self, check_name: str, result: SafetyCheckResult):
        """Add safety check result to report."""
        self.checks.append(SafetyCheck(
            name=check_name,
            passed=result.passed,
            reason=result.reason,
            duration_ms=getattr(result, 'duration_ms', 0.0),
            details=getattr(result, 'details', {})
        ))

        # Update overall safety status
        self.overall_safe = all(check.passed for check in self.checks)

    def add_violation(self, violation_type: str, description: str):
        """Add safety violation to report."""
        self.violations.append(SafetyViolation(
            type=violation_type,
            description=description,
            timestamp=time.time()
        ))
        self.overall_safe = False

@dataclass
class SafetyCheck:
    """Individual safety check result."""
    name: str
    passed: bool
    reason: str
    duration_ms: float
    details: Dict[str, Any] = field(default_factory=dict)

@dataclass
class SafetyViolation:
    """Safety violation record."""
    type: str
    description: str
    timestamp: float

@dataclass
class ExecutionSafetyResult:
    """Result of sandbox execution safety testing."""
    passed: bool
    reason: str
    test_results: List[KernelExecutionResult] = field(default_factory=list)
    sandbox_stats: Dict[str, Any] = field(default_factory=dict)
    exception_details: Optional[str] = None

@dataclass
class MemoryLeakTestResult:
    """Result of memory leak testing."""
    leak_detected: bool
    reason: str
    total_growth_bytes: int = 0
    baseline_memory_bytes: int = 0
    final_memory_bytes: int = 0
    growth_rate_per_cycle: float = 0.0
    memory_snapshots: List[Dict[str, Any]] = field(default_factory=list)
    error: Optional[str] = None

@dataclass
class GradientHealthResult:
    """Result of gradient health analysis."""
    healthy: bool
    reason: str
    gradient_norm_mean: float = 0.0
    gradient_norm_std: float = 0.0
    gradient_cv: float = 0.0
    gradient_stability: bool = False
    is_identity: bool = False
    all_gradient_norms: List[float] = field(default_factory=list)
    error: Optional[str] = None

@dataclass
class ChaosTestResult:
    """Result of chaos testing scenarios."""
    passed: bool
    reason: str
    scenario_results: List[ChaosScenarioResult] = field(default_factory=list)
    resilience_score: float = 0.0
```

## Integration Points

### Internal Integration

| Component | Interface | Data Flow |
|-----------|-----------|-----------|
| Main Validation Engine | `validate_comprehensive_safety` | Receives kernels, returns SafetyValidationReport |
| Memory Manager | `acquire_gpu_slice` | Allocates GPU memory slices for validation |
| Circuit Breaker System | `_trigger_circuit_breaker` | Handles validation failures gracefully |

### External Integration

| Subsystem | Contract | Pattern |
|-----------|----------|---------|
| Leyline | SystemStatePacket, TelemetryPacket | Async via shared contracts |
| Tezzeret | CompiledKernelArtifact | Input via message bus |
| Nissa | TelemetryPacket | Telemetry emission |

### Leyline Contracts Used

This component uses the following shared contracts:
- `leyline.SystemStatePacket` - System state reporting
- `leyline.HardwareContext` - Hardware context for validation
- `leyline.TelemetryPacket` - Telemetry emission
- `leyline.TelemetryEvent` - Individual telemetry events
- `leyline.TelemetryLevel` - Telemetry severity levels
- `leyline.SeedState` - Seed lifecycle tracking
- `leyline.SeedLifecycleStage` - Seed stage definitions

## Configuration

```yaml
safety_validation:
  # Core settings
  timing_budget_ms: 30000  # 30 second total budget
  sandbox_timeout_ms: 25000  # 25 seconds for sandbox execution

  # Memory leak detection
  memory_leak_cycles: 20  # Number of test cycles
  memory_growth_threshold: 0.20  # 20% growth tolerance

  # Gradient health
  gradient_norm_min: 1e-7  # Vanishing gradient threshold
  gradient_norm_max: 1e4   # Exploding gradient threshold
  gradient_cv_max: 0.5     # Maximum coefficient of variation

  # Chaos testing
  chaos_scenarios_enabled: true
  chaos_timeout_per_scenario: 5000  # 5 seconds per scenario

  # Emergency mode
  emergency_budget_ms: 30000  # 30 second emergency path
  emergency_cpu_only: true    # Use CPU for emergency validation
```

### Configuration Validation

- **timing_budget_ms**: Must be between 10000 and 60000
- **memory_growth_threshold**: Range 0.1 to 0.5
- **gradient_norm_min**: Must be positive
- **gradient_norm_max**: Must be greater than gradient_norm_min

## Performance Characteristics

### Benchmarks

| Operation | Target | Measured | Conditions |
|-----------|--------|----------|------------|
| Sandbox Execution | <25s | 20-24s | 3 test sizes, GPU |
| Memory Leak Detection | <3s | 2-3s | 20 cycles |
| Gradient Health | <1s | 0.5-0.8s | 5 backward passes |
| Chaos Testing | <5s | 3-5s | 3 scenarios |
| Emergency Validation | <30s | 25-29s | CPU only |

### Resource Usage

- **Memory**: 4GB GPU budget, 1GB per validation
- **CPU**: 10-20% during validation
- **I/O**: Minimal, logs only
- **GPU**: 100% during kernel execution

### Optimization Strategies

1. **Memory Pooling**: Reuse GPU memory slices across validations
2. **Parallel Chaos Testing**: Run chaos scenarios in parallel when possible
3. **Cached Identity Detection**: Cache identity kernel results

## Error Handling

### Failure Modes

| Error Type | Detection | Recovery |
|------------|-----------|----------|
| Sandbox timeout | Timer exceeds 25s | Circuit breaker, conservative mode |
| Memory leak detected | Growth > 20% | Mark kernel unsafe, detailed report |
| Gradient explosion | Norm > 1e4 | Mark kernel unsafe, stop validation |
| Chaos test failure | Kernel unstable | Mark kernel risky, continue validation |
| GPU OOM | CUDA error | Fallback to CPU emergency validation |

### Circuit Breakers

```python
# Circuit breaker configuration
circuit_breaker = CircuitBreaker(
    failure_threshold=3,
    recovery_timeout_ms=30000,
    half_open_requests=1
)
```

### Fallback Behavior

When this component fails:
1. Activate conservative mode immediately
2. Switch to CPU-only emergency validation
3. Complete validation in <30 seconds with reduced scope

## Testing Strategy

### Unit Tests

```python
def test_memory_leak_detection():
    """Test memory leak detection with known leaky kernel"""
    leaky_kernel = create_leaky_kernel()
    detector = MemoryLeakDetector()
    result = detector.comprehensive_memory_leak_test(leaky_kernel, gpu_slice)
    assert result.leak_detected == True
    assert result.total_growth_bytes > baseline * 0.2
```

Coverage targets:
- Line coverage: >95%
- Branch coverage: >85%
- Critical paths: 100%

### Integration Tests

- **Test**: Sandbox execution with timeout
  - **Setup**: Create slow kernel
  - **Validation**: Circuit breaker triggers at 25s

- **Test**: Memory leak with cleanup
  - **Setup**: Leaky kernel with TTL
  - **Validation**: Memory reclaimed after TTL

### Property-Based Tests

```python
@hypothesis.given(
    input_size=strategies.integers(min_value=8, max_value=1024)
)
def test_gradient_health_property(input_size):
    """Property: Gradient health check completes for any valid input size"""
    kernel = create_test_kernel()
    analyzer = GradientHealthAnalyzer()
    result = analyzer.analyze_gradient_health(kernel, gpu_slice)
    assert result.healthy in [True, False]
    assert result.gradient_norm_mean >= 0
```

## Monitoring & Observability

### Metrics

| Metric | Type | Purpose |
|--------|------|---------|
| `urabrask.safety.validation_time_ms` | Histogram | Safety validation latency |
| `urabrask.safety.memory_leaks_detected_total` | Counter | Total memory leaks found |
| `urabrask.safety.gradient_health_failures_total` | Counter | Unhealthy gradients detected |
| `urabrask.safety.chaos_failures_total` | Counter | Chaos test failures |
| `urabrask.safety.emergency_validations_total` | Counter | Emergency mode activations |

### Logging

```python
# Logging levels and patterns
logger.debug(f"[Safety] Starting validation for kernel {kernel_id}")
logger.info(f"[Safety] Validation completed: {result.passed}")
logger.warning(f"[Safety] Memory leak detected: {growth_bytes} bytes")
logger.error(f"[Safety] Validation failed: {error}", exc_info=True)
```

### Tracing

- **Span**: `urabrask.safety.validation`
  - **Attributes**: kernel_id, validation_mode, gpu_device
  - **Events**: sandbox_start, memory_check, gradient_analysis, chaos_test

## Security Considerations

- **Input Validation**: All kernel inputs validated for size and type
- **Access Control**: Sandbox prevents filesystem and network access
- **Data Handling**: No sensitive data processed
- **Audit Trail**: All validation decisions logged with justification

## Migration Notes

> **From Version 2.x**: Circuit breakers replace all assert statements
> **To Leyline**: All telemetry now uses Leyline TelemetryPacket format

## Implementation Checklist

- [x] Core safety validation algorithm
- [x] Sandbox execution with circuit breakers
- [x] Memory leak detection
- [x] Gradient health analysis
- [x] Chaos testing scenarios
- [x] Emergency validation path
- [x] Unit test coverage >95%
- [x] Integration tests complete
- [x] Performance benchmarks met
- [x] Documentation updated
- [x] Security review passed
- [x] Production monitoring ready

## References

### Internal References
- Parent: [07-urabrask-unified-design.md](07-urabrask-unified-design.md)
- Related: [07.2-urabrask-performance-benchmarks.md](07.2-urabrask-performance-benchmarks.md)
- Tests: `tests/urabrask/safety/`

### External References
- [PyTorch Memory Management Best Practices](https://pytorch.org/docs/stable/notes/cuda.html)
- [Chaos Engineering Principles](https://principlesofchaos.org/)
- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)

## History & Context

### Implementation Notes
- **2025-01-10**: Replaced all assertions with circuit breakers (C-016)
- **2025-01-10**: Added 20% memory growth threshold (realistic)
- **2025-01-12**: Integrated Leyline telemetry contracts

### Known Issues
- **ISSUE-001**: GPU temperature monitoring requires pynvml
- **ISSUE-002**: Emergency validation slower on older CPUs

---

*Component Owner: System Architecture Team | Last Updated: 2025-01-12*