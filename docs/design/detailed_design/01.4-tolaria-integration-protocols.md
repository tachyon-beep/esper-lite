# Tolaria - Integration Protocols and Infrastructure

**Parent Document**: [01-tolaria-unified-design.md](01-tolaria-unified-design.md)  
**Component Type**: Integration, Infrastructure, and API Specifications  
**Version**: 3.0 (Split from v2.2)  
**Date**: 2025-01-10

## Overview

This document details Tolaria's integration protocols, shared infrastructure, performance targets, memory management, validation frameworks, and complete API specifications. These components define how Tolaria interfaces with other subsystems and maintains operational excellence.

## Shared Infrastructure

**[C-008 Update] Per C-008 consensus, integration uses shared specifications:**

### Contracts

All cross-subsystem contracts are defined in [Leyline Shared Contracts](../00-leyline-shared-contracts.md):
- `SystemStatePacket` - Training state communication (see [§3.1](../00-leyline-shared-contracts.md#systemstatepacket-optimized))
- `AdaptationCommand` - Control commands (see [§3.2](../00-leyline-shared-contracts.md#adaptationcommand-unified))
- `TelemetryPacket` - Telemetry formats (see [§3.4](../00-leyline-shared-contracts.md#telemetrypacket))
- `HardwareContext` - Hardware state (see [§3.3](../00-leyline-shared-contracts.md#shared-enums-and-constants))

### Configurations  

Leyline (shared contracts) defines unified configuration schemas:
- Circuit breaker settings - Unified circuit breaker configuration
- Timeout values - System-wide timeout specifications
- Backpressure settings - Queue management configuration

### Protocols

Leyline (shared contracts) specifies cross-subsystem protocols:
- Two-tier rollback specification - Emergency recovery protocol
- Four-level emergency protocol - System-wide emergency handling
- Message version management - Cross-subsystem versioning

## Hardware Physics-Compliant Performance Targets

**[C-016 FIX] Updated from sub-millisecond claims to hardware reality:**

### Core Operation Timings

```python
class TolariaPerformanceTargets:
    """Hardware physics-compliant performance targets"""
    
    # [C-016 FIX] Epoch boundary operations (reality-aligned)
    EPOCH_BOUNDARY_OPERATIONS_MS = 18.0
    # Breakdown: Tamiyo.step() GNN inference (12ms) + SystemStatePacket assembly (3.5ms) + processing (2.5ms)
    
    # Training throughput degradation limit
    TRAINING_THROUGHPUT_DEGRADATION_PERCENT = 7.5
    # Justification: 5-8% overhead from morphogenetic extensions and telemetry
    
    # Checkpoint creation timing
    CHECKPOINT_CREATION_SECONDS_1B_PARAMS = 10.0
    # Breakdown: Model serialization (4.2s) + Optimizer state (2.8s) + Others (3.0s)
    
    # [C-008 Update] Two-tier rollback timing
    ROLLBACK_FAST_PATH_MS = 500  # Control plane only
    ROLLBACK_FULL_PATH_SECONDS = 12.0  # Complete system
    # Fast path: 150ms prepare, 250ms commit, 100ms confirm
    
    # Memory overhead limit
    MEMORY_OVERHEAD_PERCENT = 8.5
    # Morphogenetic state tracking + telemetry buffers + checkpoint staging
    
    # Loss explosion threshold
    LOSS_EXPLOSION_MULTIPLIER = 15.0
    # Conservative threshold allowing for adaptation exploration
```

### Component Timing Breakdown

**Epoch Boundary Operations (18ms total):**
- Tamiyo GNN inference: 12.0ms
- SystemStatePacket assembly: 3.5ms
- AdaptationCommand processing: 1.5ms
- Telemetry flush initiation: 1.0ms

**Checkpoint Creation (10s for 1B parameters):**
- Model state_dict serialization: 4.2s
- Optimizer state serialization: 2.8s
- Tamiyo state serialization: 1.5s
- Kasmina states collection: 0.8s
- Atomic write operations: 0.7s

**Emergency Rollback Timing:**

Fast Path (500ms total):
- PREPARE phase: 150ms (gather votes, check readiness)
- COMMIT phase: 250ms (execute rollback from cache)  
- CONFIRM phase: 100ms (verify completion)

Full Path (12s total):
- Checkpoint loading: 3.5s
- Model state restoration: 2.8s
- Optimizer state restoration: 2.2s
- Tamiyo state restoration: 1.5s
- Kasmina seed reset: 1.2s
- System coordination: 0.8s

## Protocol Buffer Message Serialization

### SystemStatePacket Serialization

```python
class SystemStateSerializer:
    """[C-008 Update] Protocol Buffer serialization for SystemStatePacket"""
    
    CURRENT_VERSION = 1
    
    @staticmethod
    def serialize(packet: SystemStatePacket) -> bytes:
        """Serialize to protobuf wire format"""
        # Convert to Protocol Buffer message
        pb_packet = system_state_pb2.SystemStatePacket()
        pb_packet.version = SystemStateSerializer.CURRENT_VERSION
        pb_packet.current_epoch = packet.current_epoch
        pb_packet.validation_accuracy = packet.validation_accuracy
        pb_packet.validation_loss = packet.validation_loss
        pb_packet.timestamp_ns = time.time_ns()
        
        # Hardware context
        pb_packet.hardware_context.device_type = packet.hardware_context.device_type
        pb_packet.hardware_context.device_id = packet.hardware_context.device_id
        pb_packet.hardware_context.total_memory_gb = packet.hardware_context.total_memory_gb
        pb_packet.hardware_context.available_memory_gb = packet.hardware_context.available_memory_gb
        pb_packet.hardware_context.temperature_celsius = packet.hardware_context.temperature_celsius
        
        # Training metrics
        for key, value in packet.training_metrics.items():
            pb_packet.training_metrics[key] = value
        
        # Seed states (if available)
        for seed_info in getattr(packet, 'seed_states', []):
            pb_seed = pb_packet.seed_states.add()
            pb_seed.seed_id = seed_info.seed_id
            pb_seed.gradient_norm = seed_info.gradient_norm
            pb_seed.learning_rate = seed_info.learning_rate  
            pb_seed.layer_depth = seed_info.layer_depth
            for key, value in seed_info.metrics.items():
                pb_seed.metrics[key] = value
        
        return pb_packet.SerializeToString()
```

### Version Compatibility

```python
@staticmethod
def deserialize(data: bytes) -> SystemStatePacket:
    """Deserialize with version compatibility"""
    pb_packet = system_state_pb2.SystemStatePacket()
    pb_packet.ParseFromString(data)
    
    if pb_packet.version > SystemStateSerializer.CURRENT_VERSION:
        raise ValueError(f"Unsupported version: {pb_packet.version}")
    
    # Convert back to Python object
    return SystemStatePacket(
        current_epoch=pb_packet.current_epoch,
        validation_accuracy=pb_packet.validation_accuracy,
        validation_loss=pb_packet.validation_loss,
        hardware_context=HardwareContext(
            device_type=pb_packet.hardware_context.device_type,
            device_id=pb_packet.hardware_context.device_id,
            total_memory_gb=pb_packet.hardware_context.total_memory_gb,
            available_memory_gb=pb_packet.hardware_context.available_memory_gb,
            temperature_celsius=pb_packet.hardware_context.temperature_celsius
        ),
        training_metrics=dict(pb_packet.training_metrics),
        seed_states=[
            SeedGradientInfo(
                seed_id=s.seed_id,
                gradient_norm=s.gradient_norm,
                learning_rate=s.learning_rate,
                layer_depth=s.layer_depth,
                metrics=dict(s.metrics)
            ) for s in pb_packet.seed_states
        ]
    )
```

## Optimizer Rebuild Coordination

**[C-016 FIX] Enhanced with UnifiedLRController integration:**

```python
class OptimizerRebuildCoordinator:
    """
    Coordinates optimizer rebuilding with Kasmina parameter additions.
    Implements two-phase commit pattern for atomic state transitions.
    """
    
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        model: nn.Module,
        unified_lr_controller: 'UnifiedLRController',  # [C-016 FIX]
        max_rebuild_time_ms: int = 10  # 10ms constraint from requirements
    ):
        self.optimizer = optimizer
        self.model = model
        self.unified_lr_controller = unified_lr_controller
        self.max_rebuild_time_ms = max_rebuild_time_ms
        
    async def handle_parameter_addition(
        self,
        layer_id: str,
        old_shape: Tuple[int, ...],
        new_shape: Tuple[int, ...],
        parameter_name: str
    ) -> bool:
        """
        Handle new parameter addition from Kasmina layer expansion.
        Two-phase commit: prepare -> commit/abort
        """
        
        start_time = time.perf_counter()
        
        # Phase 1: PREPARE
        # Lock optimizer state
        old_state = self.optimizer.state_dict()
        
        # Get new parameters from model
        new_params = []
        for name, param in self.model.named_parameters():
            if layer_id in name and parameter_name in name:
                if param.shape == new_shape:
                    new_params.append(param)
        
        if not new_params:
            return False  # Parameters not found
        
        # Phase 2: COMMIT
        try:
            # [C-016 FIX] Add new parameters via UnifiedLRController
            self.unified_lr_controller.add_new_parameters(
                self.optimizer,
                new_params,
                parent_group_name="primary"
            )
            
            # Verify rebuild completed within budget
            elapsed_ms = (time.perf_counter() - start_time) * 1000
            if elapsed_ms > self.max_rebuild_time_ms:
                # Log warning but don't fail - circuit breaker pattern
                logging.warning(
                    f"Optimizer rebuild exceeded budget: "
                    f"{elapsed_ms:.2f}ms > {self.max_rebuild_time_ms}ms"
                )
            
            return True
            
        except Exception as e:
            # ABORT: Restore old state
            self.optimizer.load_state_dict(old_state)
            logging.error(f"Optimizer rebuild failed: {e}")
            return False
```

## Memory Management and Telemetry

### Memory Budget Management

```python
class MemoryBudgetManager:
    """[C-016 FIX] Enhanced memory management with proper garbage collection"""
    
    def __init__(self, total_memory_gb: float):
        self.total_memory_gb = total_memory_gb
        
        # Budget allocation (percentages of total)
        self.budgets = {
            'model': 0.40,      # 40% for model parameters
            'optimizer': 0.25,  # 25% for optimizer state
            'gradients': 0.15,  # 15% for gradient buffers
            'checkpoints': 0.08,  # 8% for checkpoint cache
            'telemetry': 0.05,  # 5% for telemetry buffers
            'morphogenetic': 0.05,  # 5% for seed states
            'emergency': 0.02   # 2% emergency reserve
        }
        
        # [C-016 FIX] Garbage collection intervals
        self.gc_intervals = {
            'checkpoints': 300,  # 5 minutes
            'telemetry': 60,     # 1 minute
            'morphogenetic': 180  # 3 minutes
        }
        
        self.last_gc_times = {k: time.time() for k in self.gc_intervals}
        
    def allocate_budget(self, component: str) -> float:
        """Get memory budget for component in GB"""
        return self.total_memory_gb * self.budgets.get(component, 0.0)
    
    def trigger_gc_if_needed(self, component: str) -> bool:
        """[C-016 FIX] Trigger garbage collection if interval exceeded"""
        if component not in self.gc_intervals:
            return False
            
        current_time = time.time()
        if current_time - self.last_gc_times[component] > self.gc_intervals[component]:
            self._run_gc(component)
            self.last_gc_times[component] = current_time
            return True
        return False
```

### Telemetry Collection

```python
class TelemetryCollector:
    """Asynchronous telemetry collection with backpressure handling"""
    
    def __init__(self, oona_bus, max_queue_size: int = 10000):
        self.oona_bus = oona_bus
        self.queue = asyncio.Queue(maxsize=max_queue_size)
        self.dropped_count = 0
        
    async def collect_telemetry(
        self,
        priority: TelemetryPriority,
        source: str,
        event_type: str,
        payload: Dict[str, Any]
    ):
        """Collect telemetry with priority-based queueing"""
        
        message = TelemetryMessage(
            timestamp_ns=time.time_ns(),
            priority=priority,
            source=source,
            event_type=event_type,
            payload=payload
        )
        
        if priority == TelemetryPriority.EMERGENCY:
            # Emergency messages bypass queue
            await self.oona_bus.send_immediate(message)
        else:
            try:
                # Non-blocking put with timeout
                await asyncio.wait_for(
                    self.queue.put(message),
                    timeout=0.001  # 1ms timeout
                )
            except (asyncio.TimeoutError, asyncio.QueueFull):
                self.dropped_count += 1
                if self.dropped_count % 1000 == 0:
                    logging.warning(f"Dropped {self.dropped_count} telemetry messages")
```

## Validation and Testing Framework

### Integration Test Framework

```python
class TolariaIntegrationTests:
    """Comprehensive integration test suite"""
    
    @pytest.mark.integration
    async def test_epoch_boundary_timing(self):
        """Verify epoch boundary operations complete within 18ms"""
        trainer = TolariaTrainer(config=test_config)
        
        start_time = time.perf_counter()
        await trainer.end_of_epoch_hook()
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        assert elapsed_ms < 18.0, f"Epoch boundary took {elapsed_ms}ms > 18ms"
    
    @pytest.mark.integration
    async def test_fast_rollback_timing(self):
        """Verify fast rollback completes within 500ms"""
        coordinator = FastRollbackCoordinator()
        
        start_time = time.perf_counter()
        success = await coordinator.initiate_fast_rollback(
            checkpoint_epoch=100,
            reason="test"
        )
        elapsed_ms = (time.perf_counter() - start_time) * 1000
        
        assert success, "Fast rollback failed"
        assert elapsed_ms < 500.0, f"Fast rollback took {elapsed_ms}ms > 500ms"
```

### Property-Based Testing

```python
from hypothesis import given, strategies as st

class PropertyTests:
    """Property-based testing for invariants"""
    
    @given(
        old_shape=st.tuples(st.integers(1, 1000), st.integers(1, 1000)),
        expansion=st.integers(1, 100)
    )
    def test_momentum_preservation(self, old_shape, expansion):
        """Verify momentum is preserved during parameter expansion"""
        new_shape = (old_shape[0], old_shape[1] + expansion)
        
        old_momentum = torch.randn(old_shape)
        preserver = DynamicOptimizerStatePreserver(unified_lr_controller)
        
        new_momentum = preserver._transform_momentum_tensor(
            old_momentum,
            ParameterMapping(
                old_id="test",
                new_id="test",
                old_shape=old_shape,
                new_shape=new_shape,
                mapping_type="expand"
            ),
            momentum_type="first_moment"
        )
        
        # Old momentum should be preserved
        assert torch.allclose(
            new_momentum[:old_shape[0], :old_shape[1]],
            old_momentum
        )
```

## Configuration and API

### Main Configuration Schema

```yaml
tolaria:
  # Core settings
  enable_morphogenetic: true
  checkpoint_interval_epochs: 10
  
  # Performance targets
  performance:
    epoch_boundary_ms: 18.0
    training_degradation_percent: 7.5
    memory_overhead_percent: 8.5
    
  # Rollback configuration
  rollback:
    fast_path_enabled: true
    fast_path_timeout_ms: 500
    full_path_timeout_s: 12.0
    checkpoint_cache_size: 5
    
  # UnifiedLRController
  learning_rate:
    enable_runtime_guards: true
    circuit_breaker_threshold: 3
    base_lr: 0.0001
    policy: "cosine"
    warmup_epochs: 10
    
  # Integration
  integration:
    tamiyo_timeout_s: 2.0
    oona_backpressure_limit: 10000
    protobuf_version: 1
```

### Public API

```python
class Tolaria:
    """Main public API for Tolaria training orchestrator"""
    
    def __init__(self, config: Dict[str, Any]):
        """Initialize Tolaria with configuration"""
        
    def train(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int
    ) -> TrainingResult:
        """Execute training with morphogenetic capabilities"""
        
    def checkpoint(self, path: str) -> bool:
        """Create checkpoint at specified path"""
        
    def rollback(self, checkpoint_epoch: int) -> bool:
        """Rollback to specified checkpoint"""
        
    def get_metrics(self) -> Dict[str, float]:
        """Get current training metrics"""
```

## Monitoring and Observability

### Metrics Collection

```python
class TolariaMetrics:
    """Prometheus-compatible metrics"""
    
    # Counters
    epochs_completed = Counter('tolaria_epochs_completed_total')
    rollbacks_initiated = Counter('tolaria_rollbacks_initiated_total')
    rollbacks_succeeded = Counter('tolaria_rollbacks_succeeded_total')
    lr_violations = Counter('tolaria_lr_violations_total')
    
    # Gauges
    current_loss = Gauge('tolaria_current_loss')
    current_accuracy = Gauge('tolaria_current_accuracy')
    memory_usage_gb = Gauge('tolaria_memory_usage_gb')
    
    # Histograms
    epoch_duration_seconds = Histogram('tolaria_epoch_duration_seconds')
    checkpoint_duration_seconds = Histogram('tolaria_checkpoint_duration_seconds')
    rollback_duration_ms = Histogram('tolaria_rollback_duration_ms')
```

### Distributed Tracing

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

class TracedOperations:
    """OpenTelemetry tracing for critical operations"""
    
    @tracer.start_as_current_span("epoch_boundary")
    async def traced_epoch_boundary(self):
        """Traced epoch boundary operations"""
        span = trace.get_current_span()
        span.set_attribute("epoch", self.current_epoch)
        span.set_attribute("validation_loss", self.val_loss)
        
        # Execute epoch boundary
        await self.end_of_epoch_hook()
```

## Implementation Readiness Checklist

### Critical Path Components
- ✅ Training loop with multi-seed aggregation
- ✅ UnifiedLRController with exclusive mutation
- ✅ Two-tier rollback system
- ✅ WAL-protected checkpointing
- ✅ Protocol Buffer serialization
- ✅ Circuit breaker protection
- ✅ Memory management with GC

### Integration Points
- ✅ Kasmina direct function calls
- ✅ Tamiyo timeout wrapper
- ✅ Oona message bus integration
- ✅ Nissa observability hooks
- ✅ Shared infrastructure usage

### Safety Features
- ✅ All asserts replaced with circuit breakers
- ✅ Conservative mode fallbacks
- ✅ Runtime LR integrity checks
- ✅ Timing budget validation
- ✅ Memory leak prevention

## Complete Interface Definitions

### Core Interfaces

```python
from abc import ABC, abstractmethod

class ITrainingOrchestrator(ABC):
    """Interface for training orchestration"""
    
    @abstractmethod
    async def train_epoch(self) -> EpochResult:
        """Execute one training epoch"""
        
    @abstractmethod
    async def validate(self) -> ValidationResult:
        """Run validation"""
        
    @abstractmethod
    async def checkpoint(self, path: str) -> bool:
        """Create checkpoint"""
        
    @abstractmethod
    async def rollback(self, epoch: int) -> bool:
        """Rollback to checkpoint"""

class ILRController(ABC):
    """Interface for learning rate control"""
    
    @abstractmethod
    def step(self, epoch: int, metrics: Dict) -> Dict[str, float]:
        """Update learning rates"""
        
    @abstractmethod
    def register_optimizer(self, name: str, optimizer: Optimizer, config: GroupConfig):
        """Register optimizer"""
        
    @abstractmethod
    def add_new_parameters(self, optimizer: Optimizer, params: List[Parameter]):
        """Add morphogenetic parameters"""
```

### Message Contracts

All cross-subsystem message schemas are defined in the [Leyline Shared Contracts](../00-leyline-shared-contracts.md):

- `SystemStatePacket` - Training state communication (see [Leyline §3.1](../00-leyline-shared-contracts.md#systemstatepacket-optimized))
- `AdaptationCommand` - Control commands (see [Leyline §3.2](../00-leyline-shared-contracts.md#adaptationcommand-unified))
- `HardwareContext` - Hardware state (see [Leyline §3.3](../00-leyline-shared-contracts.md#shared-enums-and-constants))

## References

- Parent: [01-tolaria-unified-design.md](01-tolaria-unified-design.md)
- Related: [01.1-tolaria-epoch-lifecycle.md](01.1-tolaria-epoch-lifecycle.md) for training loop
- Related: [01.2-tolaria-rollback-systems.md](01.2-tolaria-rollback-systems.md) for emergency recovery
- Related: [01.3-tolaria-optimizer-lr.md](01.3-tolaria-optimizer-lr.md) for LR management
- External: C-006, C-008, C-016 conclave specifications