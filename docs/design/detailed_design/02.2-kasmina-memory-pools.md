# Kasmina - Memory Pools and Tensor Lifecycle Management

**Parent Document**: [02-kasmina-unified-design.md](./02-kasmina-unified-design.md)
**Component Type**: System|Memory
**Version**: 3.3
**Status**: PRODUCTION - Critical Safety Controls + Leyline Integration + C-024 KD Memory Management
**C-024 Updates**: Teacher memory pool allocation, checkpoint effectiveness monitoring, emergency cleanup for KD

---

## Overview

This component provides memory pool management and tensor lifecycle controls for Kasmina. It ensures reliable operation through TTL-based garbage collection, memory bounded operations, and efficient tensor caching strategies to prevent the production-killing 24-48hr system failures identified in C-016.

**CRITICAL IMPORTANCE**: These memory management mechanisms prevent:
- Memory exhaustion leading to 24-48hr system failure
- Unbounded memory growth during long training runs
- GPU memory fragmentation causing performance degradation
- C-024: Teacher model OOM on A100 40GB (14GB â†’ 7GB with checkpointing)

**Leyline Integration**: All memory metrics and telemetry use Leyline contracts for optimal performance reporting.

## 1. Memory Management and Garbage Collection

### 1.1 TTL Memory Cache (Critical - 24-48hr failure prevention)

```python
from collections import OrderedDict
import threading
import time
from typing import Dict, Any, Optional, Tuple

class TTLMemoryCache:
    """Memory bounded cache with TTL and LRU eviction"""

    def __init__(self, max_size: int = 10000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache: OrderedDict[Tuple[int, str], Tuple[Any, float]] = OrderedDict()
        self._lock = threading.RLock()

        # Performance tracking for Leyline telemetry
        self.cache_hits = 0
        self.cache_misses = 0
        self.evictions = 0

    def get(self, epoch: int, request_id: str) -> Optional[Any]:
        """Get item with (epoch, request_id) key"""
        key = (epoch, request_id)

        with self._lock:
            if key in self._cache:
                value, timestamp = self._cache[key]

                # Check TTL
                if time.time() - timestamp > self.ttl_seconds:
                    del self._cache[key]
                    self.cache_misses += 1
                    return None

                # Move to end (LRU)
                self._cache.move_to_end(key)
                self.cache_hits += 1
                return value

            self.cache_misses += 1
            return None

    def put(self, epoch: int, request_id: str, value: Any) -> None:
        """Store item with TTL and LRU eviction"""
        key = (epoch, request_id)
        timestamp = time.time()

        with self._lock:
            # Remove if exists
            if key in self._cache:
                del self._cache[key]

            # Add new item
            self._cache[key] = (value, timestamp)

            # Enforce size limit
            while len(self._cache) > self.max_size:
                self._cache.popitem(last=False)  # Remove oldest
                self.evictions += 1

    def cleanup_expired(self) -> int:
        """Remove expired items, return count removed"""
        current_time = time.time()
        expired_keys = []

        with self._lock:
            for key, (_, timestamp) in self._cache.items():
                if current_time - timestamp > self.ttl_seconds:
                    expired_keys.append(key)

            for key in expired_keys:
                del self._cache[key]

        return len(expired_keys)

    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache statistics for monitoring"""
        with self._lock:
            hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0.0

            return {
                'size': len(self._cache),
                'max_size': self.max_size,
                'hit_rate': hit_rate,
                'cache_hits': self.cache_hits,
                'cache_misses': self.cache_misses,
                'evictions': self.evictions,
                'ttl_seconds': self.ttl_seconds
            }
```

### 1.2 Memory Pool Manager with C-024 Teacher Support

```python
class KasminaMemoryPoolManager:
    """Central memory pool management for kernels, blueprints, telemetry, and C-024 teacher models"""

    def __init__(self, config: MemoryPoolConfig):
        self.config = config

        # Separate pools for different data types
        self.kernel_cache = TTLMemoryCache(
            max_size=config.kernel_cache_size,
            ttl_seconds=config.kernel_ttl_seconds
        )
        self.blueprint_cache = TTLMemoryCache(
            max_size=config.blueprint_cache_size,
            ttl_seconds=config.blueprint_ttl_seconds
        )
        self.telemetry_buffer = TTLMemoryCache(
            max_size=config.telemetry_buffer_size,
            ttl_seconds=config.telemetry_ttl_seconds
        )

        # C-024: Teacher model memory tracking
        self.teacher_memory_allocated = False
        self.teacher_memory_gb = 0.0
        self.teacher_checkpoint_enabled = False
        self.checkpoint_effectiveness_metrics = []

        # Garbage collection tracking
        self.gc_counter = 0
        self.gc_frequency = config.gc_frequency  # Run GC every N epochs
        self.last_gc_epoch = 0

        # Leyline integration
        from esper.leyline.contracts import TelemetryPacket
        self.telemetry_reporter = LeylineTelemetryReporter()

    def allocate_teacher_memory(self, checkpoint_enabled: bool = True) -> Tuple[bool, str]:
        """C-024: Pre-allocate memory for teacher model with checkpoint support"""

        if self.teacher_memory_allocated:
            return True, "Teacher memory already allocated"

        # Calculate required memory
        if checkpoint_enabled:
            required_gb = 7.0  # With gradient checkpointing
        else:
            required_gb = 14.0  # Without checkpointing (NOT VIABLE on A100 40GB)

        # Check if allocation is feasible
        current_gpu_memory_gb = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0
        available_memory_gb = 40.0 - current_gpu_memory_gb  # Assume A100 40GB

        if available_memory_gb < required_gb:
            error_msg = f"Insufficient memory for teacher: need {required_gb}GB, have {available_memory_gb:.1f}GB"
            self._report_teacher_memory_failure(error_msg)
            return False, error_msg

        # Track allocation
        self.teacher_memory_gb = required_gb
        self.teacher_checkpoint_enabled = checkpoint_enabled
        self.teacher_memory_allocated = True

        # Report successful allocation
        self._report_teacher_memory_allocated(required_gb, checkpoint_enabled)

        return True, f"Allocated {required_gb}GB for teacher model"

    def track_checkpoint_effectiveness(self, memory_before_gb: float, memory_after_gb: float,
                                      checkpoint_overhead_ms: float) -> None:
        """C-024: Track effectiveness of gradient checkpointing"""

        memory_saved_gb = memory_before_gb - memory_after_gb
        effectiveness_ratio = memory_saved_gb / memory_before_gb if memory_before_gb > 0 else 0

        # Store metrics
        self.checkpoint_effectiveness_metrics.append({
            'memory_before_gb': memory_before_gb,
            'memory_after_gb': memory_after_gb,
            'memory_saved_gb': memory_saved_gb,
            'effectiveness_ratio': effectiveness_ratio,
            'checkpoint_overhead_ms': checkpoint_overhead_ms,
            'timestamp': time.time()
        })

        # Alert if checkpointing not effective
        if memory_saved_gb < 6.0:  # Expected ~7GB savings
            self._report_checkpoint_ineffective(memory_saved_gb)

    def periodic_garbage_collection(self, current_epoch: int) -> Dict[str, int]:
        """Run periodic garbage collection across all pools"""

        if (current_epoch - self.last_gc_epoch) < self.gc_frequency:
            return {'gc_skipped': 1, 'reason': 'frequency_not_met'}

        # Run garbage collection on all pools
        kernel_expired = self.kernel_cache.cleanup_expired()
        blueprint_expired = self.blueprint_cache.cleanup_expired()
        telemetry_expired = self.telemetry_buffer.cleanup_expired()

        self.gc_counter += 1
        self.last_gc_epoch = current_epoch

        gc_results = {
            'kernel_cache_expired': kernel_expired,
            'blueprint_cache_expired': blueprint_expired,
            'telemetry_buffer_expired': telemetry_expired,
            'total_expired': kernel_expired + blueprint_expired + telemetry_expired,
            'gc_epoch': current_epoch,
            'gc_counter': self.gc_counter
        }

        # Report GC metrics to Leyline
        self._report_gc_metrics(gc_results)

        return gc_results

    def get_memory_stats(self) -> Dict[str, Any]:
        """Get comprehensive memory statistics including C-024 teacher metrics"""

        kernel_stats = self.kernel_cache.get_cache_stats()
        blueprint_stats = self.blueprint_cache.get_cache_stats()
        telemetry_stats = self.telemetry_buffer.get_cache_stats()

        stats = {
            # Individual pool stats
            'kernel_cache': kernel_stats,
            'blueprint_cache': blueprint_stats,
            'telemetry_buffer': telemetry_stats,

            # Aggregate metrics
            'total_cached_items': (
                kernel_stats['size'] +
                blueprint_stats['size'] +
                telemetry_stats['size']
            ),
            'total_cache_hits': (
                kernel_stats['cache_hits'] +
                blueprint_stats['cache_hits'] +
                telemetry_stats['cache_hits']
            ),
            'total_cache_misses': (
                kernel_stats['cache_misses'] +
                blueprint_stats['cache_misses'] +
                telemetry_stats['cache_misses']
            ),
            'overall_hit_rate': (
                kernel_stats.get('hit_rate', 0.0) +
                blueprint_stats.get('hit_rate', 0.0) +
                telemetry_stats.get('hit_rate', 0.0)
            ) / 3.0,

            # GC state
            'gc_counter': self.gc_counter,
            'gc_frequency': self.gc_frequency,

            # C-024: Teacher memory stats
            'teacher_memory_allocated': self.teacher_memory_allocated,
            'teacher_memory_gb': self.teacher_memory_gb,
            'teacher_checkpoint_enabled': self.teacher_checkpoint_enabled,
            'checkpoint_effectiveness': self._get_checkpoint_effectiveness_summary()
        }

        return stats

    def _get_checkpoint_effectiveness_summary(self) -> Dict[str, float]:
        """C-024: Summarize checkpoint effectiveness metrics"""
        if not self.checkpoint_effectiveness_metrics:
            return {}

        import numpy as np
        memory_saved = [m['memory_saved_gb'] for m in self.checkpoint_effectiveness_metrics]
        overhead = [m['checkpoint_overhead_ms'] for m in self.checkpoint_effectiveness_metrics]

        return {
            'avg_memory_saved_gb': np.mean(memory_saved),
            'min_memory_saved_gb': np.min(memory_saved),
            'max_memory_saved_gb': np.max(memory_saved),
            'avg_checkpoint_overhead_ms': np.mean(overhead),
            'max_checkpoint_overhead_ms': np.max(overhead),
            'samples': len(self.checkpoint_effectiveness_metrics)
        }

    def _report_teacher_memory_allocated(self, memory_gb: float, checkpoint_enabled: bool) -> None:
        """C-024: Report teacher memory allocation to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_memory_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_memory_allocated"
        event.severity = TelemetryLevel.TELEMETRY_INFO
        event.message = f"Allocated {memory_gb}GB for teacher model"
        event.timestamp.GetCurrentTime()

        event.attributes["memory_gb"] = str(memory_gb)
        event.attributes["checkpoint_enabled"] = str(checkpoint_enabled)
        event.attributes["memory_reduction"] = "50%" if checkpoint_enabled else "0%"

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_checkpoint_ineffective(self, memory_saved_gb: float) -> None:
        """C-024: Alert when checkpointing not achieving expected savings"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"checkpoint_warning_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "checkpoint_ineffective"
        event.severity = TelemetryLevel.TELEMETRY_WARN
        event.message = f"Checkpoint only saved {memory_saved_gb:.1f}GB, expected ~7GB"
        event.timestamp.GetCurrentTime()

        event.attributes["memory_saved_gb"] = str(memory_saved_gb)
        event.attributes["expected_savings_gb"] = "7.0"
        event.attributes["effectiveness"] = str(memory_saved_gb / 7.0)

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_gc_metrics(self, gc_results: Dict[str, int]) -> None:
        """Report garbage collection metrics to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, MetricPoint, MetricType

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"kasmina_gc_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add GC metrics
        for metric_name, value in gc_results.items():
            metric = telemetry.metrics.add()
            metric.name = f"kasmina_gc_{metric_name}"
            metric.value = float(value)
            metric.type = MetricType.METRIC_COUNTER
            metric.timestamp.GetCurrentTime()

            # Use native map for labels (C-018 Option B performance)
            metric.labels["subsystem"] = "kasmina"
            metric.labels["component"] = "memory_management"

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def emergency_memory_cleanup(self, include_teacher: bool = False) -> Dict[str, Any]:
        """Emergency memory cleanup when approaching limits - C-024: optionally clear teacher"""

        # Aggressive cache clearing
        kernel_cleared = len(self.kernel_cache._cache)
        blueprint_cleared = len(self.blueprint_cache._cache)
        telemetry_cleared = len(self.telemetry_buffer._cache)

        self.kernel_cache._cache.clear()
        self.blueprint_cache._cache.clear()
        self.telemetry_buffer._cache.clear()

        # C-024: Clear teacher memory if requested
        teacher_cleared_gb = 0
        if include_teacher and self.teacher_memory_allocated:
            teacher_cleared_gb = self.teacher_memory_gb
            self.teacher_memory_allocated = False
            self.teacher_memory_gb = 0
            # Note: Actual teacher model cleanup handled by KasminaKernelManager

        # Force immediate garbage collection
        import gc
        collected = gc.collect()

        # CUDA cache clear
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        cleanup_results = {
            'emergency_cleanup_triggered': True,
            'kernel_cache_cleared': kernel_cleared,
            'blueprint_cache_cleared': blueprint_cleared,
            'telemetry_cache_cleared': telemetry_cleared,
            'teacher_memory_cleared_gb': teacher_cleared_gb,
            'python_gc_collected': collected,
            'timestamp': time.time()
        }

        # Report emergency cleanup to Leyline
        self._report_emergency_cleanup(cleanup_results)

        return cleanup_results

    def _report_emergency_cleanup(self, cleanup_results: Dict[str, Any]) -> None:
        """Report emergency cleanup to Leyline telemetry"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"kasmina_emergency_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add emergency cleanup event
        event = telemetry.events.add()
        event.event_name = "emergency_memory_cleanup"
        event.severity = TelemetryLevel.TELEMETRY_WARN
        event.message = "Emergency memory cleanup triggered"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        for key, value in cleanup_results.items():
            if key != 'timestamp':  # Skip timestamp to avoid confusion
                event.attributes[key] = str(value)

        self.telemetry_reporter.send_telemetry_async(telemetry)
```

## 2. GPU Memory Pool Management

### 2.1 GPU Tensor Pool with C-024 Teacher Buffer Support

```python
class GPUTensorPool:
    """GPU-resident tensor pool for efficient memory reuse including C-024 teacher buffers"""

    def __init__(self, device: str = 'cuda', initial_pool_size_mb: int = 1024):
        self.device = device
        self.initial_pool_size_mb = initial_pool_size_mb

        # Pool organized by tensor shapes for efficient reuse
        self.tensor_pools: Dict[Tuple[int, ...], List[torch.Tensor]] = {}
        self.pool_usage_stats: Dict[Tuple[int, ...], int] = {}

        # C-024: Teacher model buffer pre-allocation
        self.teacher_buffers_allocated = False
        self.teacher_buffer_size_mb = 0

        # Memory tracking
        self.allocated_memory_mb = 0.0
        self.peak_memory_mb = 0.0
        self.pool_hits = 0
        self.pool_misses = 0

        # Thread safety
        self.pool_lock = threading.RLock()

        # Leyline integration
        from esper.leyline.contracts import TelemetryPacket
        self.telemetry_reporter = LeylineTelemetryReporter()

    def preallocate_teacher_buffers(self, batch_size: int, sequence_length: int,
                                   hidden_dim: int, vocab_size: int) -> bool:
        """C-024: Pre-allocate buffers for teacher model to prevent fragmentation"""

        if self.teacher_buffers_allocated:
            return True

        try:
            # Pre-allocate common teacher tensor shapes
            teacher_shapes = [
                (batch_size, sequence_length, hidden_dim),  # Activations
                (batch_size, sequence_length, vocab_size),  # Logits
                (batch_size, hidden_dim),  # Hidden states
            ]

            total_size_mb = 0
            for shape in teacher_shapes:
                # Pre-allocate and immediately pool
                tensor = torch.zeros(shape, dtype=torch.float16, device=self.device)
                size_mb = tensor.numel() * tensor.element_size() / (1024 * 1024)
                total_size_mb += size_mb

                # Add to pool for reuse
                self.return_tensor(tensor)

            self.teacher_buffer_size_mb = total_size_mb
            self.teacher_buffers_allocated = True

            # Report pre-allocation
            self._report_teacher_buffer_allocation(total_size_mb, teacher_shapes)

            return True

        except torch.cuda.OutOfMemoryError:
            logger.error("Failed to pre-allocate teacher buffers - OOM")
            return False

    def get_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype = torch.float32) -> torch.Tensor:
        """Get tensor from pool or create new one"""

        with self.pool_lock:
            # Check if we have a tensor of this shape in pool
            if shape in self.tensor_pools and len(self.tensor_pools[shape]) > 0:
                tensor = self.tensor_pools[shape].pop()
                self.pool_hits += 1

                # Clear the tensor for reuse
                tensor.zero_()
                return tensor

            # Create new tensor
            tensor = torch.zeros(shape, dtype=dtype, device=self.device)
            self.pool_misses += 1

            # Update memory tracking
            tensor_size_mb = tensor.numel() * tensor.element_size() / (1024 * 1024)
            self.allocated_memory_mb += tensor_size_mb
            self.peak_memory_mb = max(self.peak_memory_mb, self.allocated_memory_mb)

            return tensor

    def return_tensor(self, tensor: torch.Tensor) -> None:
        """Return tensor to pool for reuse"""

        if tensor.device.type != 'cuda':
            return  # Only pool GPU tensors

        shape = tuple(tensor.shape)

        with self.pool_lock:
            if shape not in self.tensor_pools:
                self.tensor_pools[shape] = []
                self.pool_usage_stats[shape] = 0

            # Only keep limited number of tensors per shape to avoid memory bloat
            max_pool_size_per_shape = 10
            if len(self.tensor_pools[shape]) < max_pool_size_per_shape:
                self.tensor_pools[shape].append(tensor)
                self.pool_usage_stats[shape] += 1

    def cleanup_unused_pools(self, min_usage_threshold: int = 5) -> int:
        """Clean up tensor pools that haven't been used much"""

        cleaned_shapes = []
        total_cleaned_tensors = 0

        with self.pool_lock:
            for shape, usage_count in list(self.pool_usage_stats.items()):
                if usage_count < min_usage_threshold:
                    # Remove this shape from pools
                    if shape in self.tensor_pools:
                        tensors_cleaned = len(self.tensor_pools[shape])
                        total_cleaned_tensors += tensors_cleaned
                        del self.tensor_pools[shape]
                        del self.pool_usage_stats[shape]
                        cleaned_shapes.append(shape)

        # Report cleanup to Leyline
        if total_cleaned_tensors > 0:
            self._report_pool_cleanup(cleaned_shapes, total_cleaned_tensors)

        return total_cleaned_tensors

    def get_pool_stats(self) -> Dict[str, Any]:
        """Get tensor pool statistics including C-024 teacher buffer status"""

        with self.pool_lock:
            total_pooled_tensors = sum(len(pool) for pool in self.tensor_pools.values())
            unique_shapes = len(self.tensor_pools)
            hit_rate = self.pool_hits / (self.pool_hits + self.pool_misses) if (self.pool_hits + self.pool_misses) > 0 else 0.0

            return {
                'total_pooled_tensors': total_pooled_tensors,
                'unique_tensor_shapes': unique_shapes,
                'allocated_memory_mb': self.allocated_memory_mb,
                'peak_memory_mb': self.peak_memory_mb,
                'pool_hit_rate': hit_rate,
                'pool_hits': self.pool_hits,
                'pool_misses': self.pool_misses,
                'teacher_buffers_allocated': self.teacher_buffers_allocated,
                'teacher_buffer_size_mb': self.teacher_buffer_size_mb,
                'most_used_shapes': sorted(
                    self.pool_usage_stats.items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]  # Top 5 most used shapes
            }

    def _report_teacher_buffer_allocation(self, size_mb: float, shapes: List[Tuple]) -> None:
        """C-024: Report teacher buffer pre-allocation to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_buffers_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_buffers_allocated"
        event.severity = TelemetryLevel.TELEMETRY_INFO
        event.message = f"Pre-allocated {size_mb:.1f}MB for teacher buffers"
        event.timestamp.GetCurrentTime()

        event.attributes["buffer_size_mb"] = str(size_mb)
        event.attributes["num_shapes"] = str(len(shapes))
        event.attributes["device"] = self.device

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_pool_cleanup(self, cleaned_shapes: List[Tuple[int, ...]], total_cleaned: int) -> None:
        """Report tensor pool cleanup to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"gpu_pool_cleanup_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add cleanup event
        event = telemetry.events.add()
        event.event_name = "gpu_tensor_pool_cleanup"
        event.severity = TelemetryLevel.TELEMETRY_INFO
        event.message = f"Cleaned up {total_cleaned} unused tensors from {len(cleaned_shapes)} shapes"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        event.attributes["total_cleaned_tensors"] = str(total_cleaned)
        event.attributes["cleaned_shape_count"] = str(len(cleaned_shapes))
        event.attributes["device"] = self.device

        self.telemetry_reporter.send_telemetry_async(telemetry)
```

### 2.2 Memory Budget Enforcement with C-024 Teacher Awareness

```python
class MemoryBudgetEnforcer:
    """Enforce memory budgets to prevent OOM conditions - C-024: includes teacher model budget"""

    def __init__(self, max_gpu_memory_gb: float = 40.0, warning_threshold: float = 0.85):
        self.max_gpu_memory_gb = max_gpu_memory_gb  # C-024: Updated for A100 40GB
        self.warning_threshold = warning_threshold
        self.max_memory_bytes = int(max_gpu_memory_gb * 1024 * 1024 * 1024)
        self.warning_threshold_bytes = int(self.max_memory_bytes * warning_threshold)

        # C-024: Memory budget breakdown (A100 40GB)
        self.kasmina_budget_gb = 32.0  # Base Kasmina operations
        self.teacher_budget_gb = 7.0   # Teacher with checkpointing
        self.buffer_budget_gb = 1.0    # Safety buffer

        # Memory tracking
        self.current_allocation_bytes = 0
        self.allocation_requests = 0
        self.allocation_denials = 0
        self.warning_count = 0

        # C-024: Teacher allocation tracking
        self.teacher_allocated = False
        self.teacher_memory_bytes = 0

        # Leyline integration
        from esper.leyline.contracts import TelemetryPacket
        self.telemetry_reporter = LeylineTelemetryReporter()

    def reserve_teacher_memory(self, checkpoint_enabled: bool = True) -> Tuple[bool, str]:
        """C-024: Reserve memory budget for teacher model"""

        if self.teacher_allocated:
            return True, "Teacher memory already reserved"

        # Calculate teacher memory requirement
        if checkpoint_enabled:
            required_gb = 7.0  # With gradient checkpointing
        else:
            required_gb = 14.0  # Without checkpointing

        required_bytes = int(required_gb * 1024 * 1024 * 1024)

        # Check if we can accommodate teacher
        current_gpu_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        projected_usage = current_gpu_memory + required_bytes

        if projected_usage > self.max_memory_bytes:
            reason = f"Cannot allocate teacher: would exceed {self.max_gpu_memory_gb}GB limit"
            self._report_teacher_allocation_denial(required_gb, reason)
            return False, reason

        # Reserve the memory
        self.teacher_memory_bytes = required_bytes
        self.teacher_allocated = True

        # Update warning threshold to account for teacher
        remaining_budget_bytes = self.max_memory_bytes - self.teacher_memory_bytes
        self.warning_threshold_bytes = int(remaining_budget_bytes * self.warning_threshold)

        return True, f"Reserved {required_gb}GB for teacher model"

    def check_allocation_allowed(self, requested_bytes: int, is_teacher_request: bool = False) -> Tuple[bool, str]:
        """Check if memory allocation is allowed within budget - C-024: distinguish teacher requests"""

        self.allocation_requests += 1

        # Check current GPU memory usage
        current_gpu_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0

        # C-024: Account for teacher reservation
        effective_limit = self.max_memory_bytes
        if self.teacher_allocated and not is_teacher_request:
            effective_limit = self.max_memory_bytes - self.teacher_memory_bytes

        projected_usage = current_gpu_memory + requested_bytes

        # Hard limit check
        if projected_usage > effective_limit:
            self.allocation_denials += 1
            reason = f"Allocation would exceed budget: {projected_usage / (1024**3):.2f}GB > {effective_limit / (1024**3):.2f}GB"
            self._report_allocation_denial(requested_bytes, reason)
            return False, reason

        # Warning threshold check
        if projected_usage > self.warning_threshold_bytes:
            self.warning_count += 1
            warning_msg = f"Memory usage approaching limit: {projected_usage / (1024**3):.2f}GB / {self.max_gpu_memory_gb:.2f}GB"
            self._report_memory_warning(projected_usage, warning_msg)

        return True, "allocation_approved"

    def update_allocation(self, allocated_bytes: int) -> None:
        """Update allocation tracking"""
        self.current_allocation_bytes += allocated_bytes

    def update_deallocation(self, deallocated_bytes: int) -> None:
        """Update deallocation tracking"""
        self.current_allocation_bytes = max(0, self.current_allocation_bytes - deallocated_bytes)

    def get_budget_stats(self) -> Dict[str, Any]:
        """Get memory budget statistics including C-024 teacher allocation"""

        current_gpu_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        max_gpu_memory = torch.cuda.max_memory_allocated() if torch.cuda.is_available() else 0

        usage_ratio = current_gpu_memory / self.max_memory_bytes if self.max_memory_bytes > 0 else 0.0
        approval_rate = (self.allocation_requests - self.allocation_denials) / self.allocation_requests if self.allocation_requests > 0 else 1.0

        return {
            'max_budget_gb': self.max_gpu_memory_gb,
            'current_gpu_usage_gb': current_gpu_memory / (1024**3),
            'peak_gpu_usage_gb': max_gpu_memory / (1024**3),
            'budget_usage_ratio': usage_ratio,
            'warning_threshold': self.warning_threshold,
            'allocation_requests': self.allocation_requests,
            'allocation_denials': self.allocation_denials,
            'approval_rate': approval_rate,
            'warning_count': self.warning_count,
            # C-024: Teacher budget tracking
            'teacher_allocated': self.teacher_allocated,
            'teacher_budget_gb': self.teacher_memory_bytes / (1024**3) if self.teacher_allocated else 0,
            'kasmina_budget_gb': self.kasmina_budget_gb,
            'remaining_budget_gb': (self.max_memory_bytes - current_gpu_memory) / (1024**3)
        }

    def _report_teacher_allocation_denial(self, requested_gb: float, reason: str) -> None:
        """C-024: Report teacher allocation denial to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"teacher_denied_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        event = telemetry.events.add()
        event.event_name = "teacher_allocation_denied"
        event.severity = TelemetryLevel.TELEMETRY_ERROR
        event.message = f"Cannot allocate teacher model: {reason}"
        event.timestamp.GetCurrentTime()

        event.attributes["requested_gb"] = str(requested_gb)
        event.attributes["denial_reason"] = reason
        event.attributes["max_budget_gb"] = str(self.max_gpu_memory_gb)

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_allocation_denial(self, requested_bytes: int, reason: str) -> None:
        """Report allocation denial to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"allocation_denied_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add denial event
        event = telemetry.events.add()
        event.event_name = "memory_allocation_denied"
        event.severity = TelemetryLevel.TELEMETRY_WARN
        event.message = f"Memory allocation denied: {reason}"
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        event.attributes["requested_bytes"] = str(requested_bytes)
        event.attributes["requested_gb"] = str(requested_bytes / (1024**3))
        event.attributes["denial_reason"] = reason
        event.attributes["current_gpu_usage_gb"] = str(torch.cuda.memory_allocated() / (1024**3) if torch.cuda.is_available() else 0)

        self.telemetry_reporter.send_telemetry_async(telemetry)

    def _report_memory_warning(self, projected_usage: int, warning_msg: str) -> None:
        """Report memory warning to Leyline"""
        from esper.leyline.contracts import TelemetryPacket, TelemetryEvent, TelemetryLevel

        telemetry = TelemetryPacket()
        telemetry.packet_id = f"memory_warning_{int(time.time())}"
        telemetry.source_subsystem = "kasmina"
        telemetry.timestamp.GetCurrentTime()

        # Add warning event
        event = telemetry.events.add()
        event.event_name = "memory_usage_warning"
        event.severity = TelemetryLevel.TELEMETRY_WARN
        event.message = warning_msg
        event.timestamp.GetCurrentTime()

        # Use native map for attributes
        event.attributes["projected_usage_gb"] = str(projected_usage / (1024**3))
        event.attributes["warning_threshold"] = str(self.warning_threshold)
        event.attributes["max_budget_gb"] = str(self.max_gpu_memory_gb)

        self.telemetry_reporter.send_telemetry_async(telemetry)
```

## 3. Integration Contract

This component integrates with the main Kasmina architecture by providing:

### 3.1 Memory Management Interface with C-024 Extensions

```python
@dataclass
class MemoryPoolContract:
    """Interface contract for memory pool management component with C-024 teacher support"""

    # Memory pool methods
    def get_memory_stats(self) -> Dict[str, Any]
    def periodic_garbage_collection(self, current_epoch: int) -> Dict[str, int]
    def emergency_memory_cleanup(self, include_teacher: bool = False) -> Dict[str, Any]

    # C-024: Teacher memory methods
    def allocate_teacher_memory(self, checkpoint_enabled: bool = True) -> Tuple[bool, str]
    def track_checkpoint_effectiveness(self, memory_before_gb: float, memory_after_gb: float,
                                      checkpoint_overhead_ms: float) -> None
    def reserve_teacher_memory(self, checkpoint_enabled: bool = True) -> Tuple[bool, str]

    # GPU tensor pool methods
    def get_tensor(self, shape: Tuple[int, ...], dtype: torch.dtype = torch.float32) -> torch.Tensor
    def return_tensor(self, tensor: torch.Tensor) -> None
    def cleanup_unused_pools(self, min_usage_threshold: int = 5) -> int
    def get_pool_stats(self) -> Dict[str, Any]
    def preallocate_teacher_buffers(self, batch_size: int, sequence_length: int,
                                   hidden_dim: int, vocab_size: int) -> bool

    # Memory budget methods
    def check_allocation_allowed(self, requested_bytes: int, is_teacher_request: bool = False) -> Tuple[bool, str]
    def update_allocation(self, allocated_bytes: int) -> None
    def update_deallocation(self, deallocated_bytes: int) -> None
    def get_budget_stats(self) -> Dict[str, Any]

    # Leyline integration methods
    def report_to_leyline_telemetry(self, event_data: Dict[str, Any]) -> None
```

### 3.2 Component Coordination

This component coordinates with other Kasmina components:

- **[02.1-kasmina-kernel-execution.md](./02.1-kasmina-kernel-execution.md)**: GPU memory coordination and kernel cache management + C-024 teacher loading
- **[02.3-kasmina-parameter-registration.md](./02.3-kasmina-parameter-registration.md)**: Memory allocation tracking for registered parameters + C-024 teacher params
- **[02.4-kasmina-safety-mechanisms.md](./02.4-kasmina-safety-mechanisms.md)**: Circuit breaker integration for memory limit violations + C-024 OOM protection
- **[02.5-kasmina-performance-validation.md](./02.5-kasmina-performance-validation.md)**: Memory performance benchmarking and validation + C-024 checkpoint monitoring

## 4. References

- **Parent Document**: [02-kasmina-unified-design.md](./02-kasmina-unified-design.md)
- **Kernel Execution**: [02.1-kasmina-kernel-execution.md](./02.1-kasmina-kernel-execution.md)
- **Parameter Registration**: [02.3-kasmina-parameter-registration.md](./02.3-kasmina-parameter-registration.md)
- **Safety Mechanisms**: [02.4-kasmina-safety-mechanisms.md](./02.4-kasmina-safety-mechanisms.md)
- **Performance Validation**: [02.5-kasmina-performance-validation.md](./02.5-kasmina-performance-validation.md)
- **Leyline Contracts**: [00-leyline-shared-contracts.md](./00-leyline-shared-contracts.md)
- **C-018 Final Consensus**: Option B (Performance-First) implementation
- **C-024 KD Amendment**: Teacher memory management with gradient checkpointing
- **Conclave C-016**: Emergency Production Safety Response

---

**COMPONENT STATUS**: COMPLETE - Critical Memory Safety Controls + Leyline Integrated + C-024 KD Support
**Production Safety**: TTL-based garbage collection prevents 24-48hr system failures
**Memory Management**: Comprehensive pool management with budget enforcement
**GPU Optimization**: Efficient tensor reuse and memory tracking
**C-024 Enhancements**: Teacher memory allocation (7GB with checkpointing), effectiveness monitoring
**Integration**: Fully coordinated with kernel execution and safety systems