# Urabrask - Performance Benchmarking Suite

## Document Metadata

| Field | Value |
|-------|-------|
| **Parent Document** | [07-urabrask-unified-design.md](07-urabrask-unified-design.md) |
| **Component Type** | Performance Framework |
| **Version** | 3.0.0 |
| **Status** | PRODUCTION |
| **Implementation** | Complete |

## Overview

The Urabrask Performance Benchmarking Suite provides comprehensive hardware-aware performance validation for compiled kernels that have passed safety validation. This framework is the second stage of the two-stage validation pipeline, ensuring kernels meet performance requirements through realistic benchmarking, chaos injection, and conservative mode assessment.

Key characteristics:
- **Hardware-Aware Benchmarking**: Tailors benchmarks to the target hardware (H100 vs A100 vs older GPUs)
- **Realistic Variance Targets**: 12-18% CV targets reflect hardware physics realities, not artificial zero-variance
- **Chaos Engineering**: Injects failures to test resilience under production conditions

## Technical Design

### Architecture

```
+-------------------------------+
| EnhancedBenchmarkSuite        |
+-------------------------------+
| - run_enhanced_benchmarks()   |
| - _calculate_realistic_stats() |
| - _apply_hardware_aware_validation() |
| - _hardware_aware_warmup()     |
+-------------------------------+
         |
         v
+-------------------------------+
| HardwareAwareLatencyBenchmark |
+-------------------------------+
| - _configure_test_sizes()     |
| - measure()                   |
+-------------------------------+
         |
         v
+-------------------------------+
| RealisticGradientBenchmark    |
+-------------------------------+
| - measure()                   |
| - _evaluate_gradient_health() |
| - _enhanced_identity_detection() |
+-------------------------------+
         |
         v
+-------------------------------+
| ThermalThrottlingBenchmark    |
+-------------------------------+
| - measure()                   |
| - _measure_baseline_performance() |
| - _analyze_thermal_stability() |
+-------------------------------+
         |
         v
+-------------------------------+
| BenchmarkChaosEngine          |
+-------------------------------+
| - inject_scenario()           |
| - _emit_chaos_telemetry()     |
+-------------------------------+
```

### Core Abstractions

**EnhancedBenchmarkSuite**
```python
# Import Leyline shared contracts
from esper.leyline.contracts import (
    SystemStatePacket,
    HardwareContext,
    TelemetryPacket,
    TelemetryEvent,
    TelemetryLevel,
    EventEnvelope,
    MessagePriority,
    ValidationMode
)

class EnhancedBenchmarkSuite:
    """C-016 Enhanced benchmarking with realistic variance targets, hardware awareness, and Leyline integration."""

    def __init__(self, hardware_context: HardwareContext):
        self.hardware_context = hardware_context
        self.benchmarks = {
            'forward_latency': HardwareAwareLatencyBenchmark(hardware_context),
            'memory_usage': EnhancedMemoryBenchmark(),
            'gradient_flow': RealisticGradientBenchmark(),
            'thermal_stability': ThermalThrottlingBenchmark()  # C-016 New
        }
        self.chaos_engine = BenchmarkChaosEngine()

    def run_enhanced_benchmarks(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice,
        mode: ValidationMode
    ) -> BenchmarkResults:
        """C-016 Enhanced benchmarking with realistic variance, chaos testing, and Leyline integration."""

        results = BenchmarkResults(
            kernel_id=kernel.id,
            mode=mode,
            hardware_context=self.hardware_context
        )

        # Determine run count based on mode
        run_counts = {
            ValidationMode.EMERGENCY: 3,      # Fast path
            ValidationMode.STANDARD: 10,     # Standard path
            ValidationMode.COMPREHENSIVE: 25 # Full characterization
        }
        runs = run_counts[mode]

        # Hardware-specific warmup using Leyline HardwareContext
        self._hardware_aware_warmup(kernel, gpu_slice)

        # Run each benchmark with chaos injection
        for benchmark_name, benchmark in self.benchmarks.items():
            measurements = []
            chaos_results = []

            for run in range(runs):
                # Every 5th run includes chaos injection for standard+
                inject_chaos = (run % 5 == 0 and mode != ValidationMode.EMERGENCY)

                try:
                    if inject_chaos:
                        with self.chaos_engine.inject_scenario() as scenario:
                            measurement = benchmark.measure(kernel, gpu_slice)
                            chaos_results.append(scenario.get_result())
                    else:
                        measurement = benchmark.measure(kernel, gpu_slice)

                    measurements.append(measurement)

                except Exception as e:
                    # Record failure but continue
                    measurements.append(BenchmarkFailure(str(e)))
                    # Emit telemetry for benchmark failures
                    self._emit_benchmark_failure_telemetry(benchmark_name, str(e))

            # C-016 Realistic statistics with hardware-aware thresholds
            stats = self._calculate_realistic_stats(measurements, benchmark_name)
            results.add_benchmark(benchmark_name, stats, chaos_results)

        # C-016 Hardware-aware validation using Leyline HardwareContext
        results = self._apply_hardware_aware_validation(results)

        return results

    def _calculate_realistic_stats(
        self,
        measurements: List[float],
        benchmark_name: str
    ) -> BenchmarkStatistics:
        """C-016 Realistic statistics with proper variance handling and Leyline integration."""

        # Filter out failures
        valid_measurements = [m for m in measurements if isinstance(m, (int, float))]

        if len(valid_measurements) < 3:
            return BenchmarkStatistics(
                valid=False,
                reason="Insufficient valid measurements"
            )

        import numpy as np

        stats = BenchmarkStatistics(
            mean=float(np.mean(valid_measurements)),
            std=float(np.std(valid_measurements)),
            min=float(np.min(valid_measurements)),
            max=float(np.max(valid_measurements)),
            median=float(np.median(valid_measurements)),
            p95=float(np.percentile(valid_measurements, 95)),
            coefficient_of_variation=float(np.std(valid_measurements) / np.mean(valid_measurements)),
            sample_count=len(valid_measurements),
            failure_count=len(measurements) - len(valid_measurements)
        )

        # C-016 Realistic variance thresholds by benchmark type
        variance_thresholds = {
            'forward_latency': 0.18,     # 18% CV max for latency
            'memory_usage': 0.12,        # 12% CV max for memory
            'gradient_flow': 0.25,       # 25% CV max for gradients (most variable)
            'thermal_stability': 0.15    # 15% CV max for thermal
        }

        threshold = variance_thresholds.get(benchmark_name, 0.20)  # Default 20%
        stats.variance_acceptable = stats.coefficient_of_variation <= threshold

        # Emit telemetry for high variance measurements
        if not stats.variance_acceptable:
            self._emit_variance_warning_telemetry(benchmark_name, stats.coefficient_of_variation, threshold)

        return stats

    def _apply_hardware_aware_validation(self, results: BenchmarkResults) -> BenchmarkResults:
        """C-016 Hardware-aware performance validation using Leyline HardwareContext."""

        # Hardware-specific performance targets based on compute capability from Leyline
        if self.hardware_context.compute_capability >= 90:  # H100 class
            latency_budget_ms = 50   # Aggressive target for H100
            memory_efficiency = 0.85  # High efficiency expected
        elif self.hardware_context.compute_capability >= 80:  # A100 class
            latency_budget_ms = 100   # Standard target
            memory_efficiency = 0.75
        else:
            latency_budget_ms = 200   # Conservative for older hardware
            memory_efficiency = 0.65

        # Apply hardware-aware thresholds
        if 'forward_latency' in results.benchmarks:
            latency_stats = results.benchmarks['forward_latency']
            latency_stats.hardware_compliant = latency_stats.mean <= latency_budget_ms

        if 'memory_usage' in results.benchmarks:
            memory_stats = results.benchmarks['memory_usage']
            # Use total memory from Leyline HardwareContext
            memory_budget_mb = self.hardware_context.total_memory_gb * 1024 * memory_efficiency
            memory_stats.hardware_compliant = memory_stats.mean <= memory_budget_mb

        return results

    def _hardware_aware_warmup(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> None:
        """Hardware-specific warmup using Leyline HardwareContext to ensure consistent benchmarking."""

        # Use compute capability from Leyline to determine warmup runs
        warmup_runs = 3 if self.hardware_context.compute_capability >= 80 else 5
        warmup_input = torch.randn(32, 32, device=gpu_slice.device)

        for _ in range(warmup_runs):
            try:
                _ = kernel.forward(warmup_input)
                torch.cuda.synchronize()  # Ensure completion
            except:
                break  # Skip warmup if kernel fails

        # Allow GPU to stabilize
        time.sleep(0.1)

    def _emit_benchmark_failure_telemetry(self, benchmark_name: str, error_message: str):
        """Emit benchmark failure telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="benchmark_failure",
            severity=TelemetryLevel.TELEMETRY_WARN,
            message=f"Benchmark {benchmark_name} failed: {error_message}",
            attributes={
                "benchmark_name": benchmark_name,
                "error_message": error_message[:200]  # Truncate long messages
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_WARN,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

    def _emit_variance_warning_telemetry(self, benchmark_name: str, cv: float, threshold: float):
        """Emit variance warning telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="high_benchmark_variance",
            severity=TelemetryLevel.TELEMETRY_WARN,
            message=f"High variance in {benchmark_name}: CV={cv:.3f} exceeds threshold {threshold:.3f}",
            attributes={
                "benchmark_name": benchmark_name,
                "coefficient_of_variation": str(cv),
                "variance_threshold": str(threshold)
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_WARN,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)
```

**HardwareAwareLatencyBenchmark**
- Purpose: Measures forward pass latency with hardware-aware inputs
- Key Methods: `measure`, `_configure_test_sizes`
- State Management: Configures test sizes based on hardware capability

### Algorithms

#### Hardware-Aware Latency Benchmarking

**Purpose**: To measure kernel execution latency tailored to specific hardware capabilities.

**Approach**:
1. Configure test input sizes based on GPU compute capability
2. Run multiple measurements with varying input sizes
3. Use median instead of mean for robustness against outliers
4. Apply hardware-specific performance targets

**Complexity**:
- Time: O(n × m) where n is number of test sizes, m is measurements per size
- Space: O(n) for storing measurements

**Implementation**:
```python
class HardwareAwareLatencyBenchmark:
    """Hardware-specific latency benchmarking with realistic targets using Leyline HardwareContext."""

    def __init__(self, hardware_context: HardwareContext):
        self.hardware_context = hardware_context
        self.test_sizes = self._configure_test_sizes()

    def _configure_test_sizes(self) -> List[Tuple[int, int]]:
        """Configure test input sizes based on Leyline HardwareContext capabilities."""

        # Use compute capability from Leyline to determine test sizes
        if self.hardware_context.compute_capability >= 90:  # H100 class
            # H100 can handle larger test sizes
            return [(64, 128), (128, 256), (256, 512), (512, 1024)]
        elif self.hardware_context.compute_capability >= 80:  # A100 class
            # A100 standard test sizes
            return [(32, 64), (64, 128), (128, 256), (256, 512)]
        else:
            # Conservative sizes for older hardware
            return [(16, 32), (32, 64), (64, 128), (128, 256)]

    def measure(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure forward pass latency with hardware-aware inputs and Leyline integration."""

        latency_measurements = []

        for height, width in self.test_sizes:
            input_tensor = torch.randn(height, width, device=gpu_slice.device)

            # Multiple measurements for statistical accuracy
            for _ in range(5):
                torch.cuda.synchronize()
                start_time = perf_counter()

                try:
                    output = kernel.forward(input_tensor)
                    torch.cuda.synchronize()

                    elapsed_ms = (perf_counter() - start_time) * 1000
                    latency_measurements.append(elapsed_ms)

                except Exception:
                    # Skip failed measurements
                    continue

        if not latency_measurements:
            raise BenchmarkFailure("All latency measurements failed")

        # Return median latency (more robust than mean)
        return float(np.median(latency_measurements))

class EnhancedMemoryBenchmark:
    """Enhanced memory usage benchmarking with efficiency tracking and Leyline integration."""

    def measure(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure memory usage efficiency with telemetry integration."""

        # Test with progressively larger inputs
        test_sizes = [(32, 32), (64, 64), (128, 128), (256, 256)]
        memory_measurements = []

        for height, width in test_sizes:
            # Clear memory before measurement
            torch.cuda.empty_cache()
            baseline_memory = torch.cuda.memory_allocated()

            input_tensor = torch.randn(height, width, device=gpu_slice.device)

            try:
                # Measure peak memory during forward pass
                output = kernel.forward(input_tensor)
                peak_memory = torch.cuda.memory_allocated()

                memory_used_mb = (peak_memory - baseline_memory) / (1024 * 1024)

                # Calculate memory efficiency (output size / memory used)
                output_size_mb = output.numel() * output.element_size() / (1024 * 1024)
                efficiency = output_size_mb / memory_used_mb if memory_used_mb > 0 else 0

                memory_measurements.append(memory_used_mb)

                # Clean up
                del input_tensor, output

            except Exception:
                continue

        if not memory_measurements:
            raise BenchmarkFailure("All memory measurements failed")

        # Return average memory usage
        return float(np.mean(memory_measurements))
```

#### Realistic Gradient Flow Benchmarking

**Purpose**: To measure gradient flow health and stability with realistic variance bounds.

**Approach**:
1. Forward pass with varying loss scaling
2. Multiple backward passes to test gradient consistency
3. Coefficient of variation (CV) as primary stability metric
4. Identity kernel detection for special cases

**Implementation**:
```python
class RealisticGradientBenchmark:
    """C-016 Enhanced gradient health benchmark with realistic bounds and Leyline integration."""

    def measure(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Enhanced gradient health measurement with realistic analysis and telemetry."""

        input_tensor = torch.randn(64, 128, requires_grad=True, device=gpu_slice.device)

        try:
            # Forward pass
            output = kernel.forward(input_tensor)

            # Multiple backward passes to test stability
            gradient_norms = []
            for i in range(5):  # Test gradient consistency
                if input_tensor.grad is not None:
                    input_tensor.grad.zero_()

                loss = output.sum() * (0.8 + 0.4 * i)  # Vary loss scaling
                loss.backward(retain_graph=True)

                grad_norm = torch.norm(input_tensor.grad).item()
                gradient_norms.append(grad_norm)

            # Analyze gradient characteristics
            mean_grad_norm = np.mean(gradient_norms)
            std_grad_norm = np.std(gradient_norms)
            cv_grad_norm = std_grad_norm / mean_grad_norm if mean_grad_norm > 0 else float('inf')

            # Identity kernel detection
            is_identity = self._enhanced_identity_detection(kernel, gpu_slice)

            # C-016 Enhanced gradient health criteria
            gradient_healthy = self._evaluate_gradient_health(
                mean_grad_norm, cv_grad_norm, is_identity, gradient_norms
            )

            # Emit telemetry for unhealthy gradients
            if not gradient_healthy:
                self._emit_gradient_health_telemetry(mean_grad_norm, cv_grad_norm, is_identity)

            # Return CV as benchmark metric (lower is better)
            return cv_grad_norm

        except Exception as e:
            # Emit telemetry for gradient computation failures
            self._emit_gradient_failure_telemetry(str(e))
            raise BenchmarkFailure(f"Gradient computation failed: {str(e)}")

    def _evaluate_gradient_health(
        self,
        mean_norm: float,
        cv: float,
        is_identity: bool,
        all_norms: List[float]
    ) -> bool:
        """C-016 Enhanced gradient health evaluation with multiple criteria."""

        # Identity kernels are always healthy
        if is_identity:
            return True

        # Check for pathological gradients
        if any(np.isnan(norm) or np.isinf(norm) for norm in all_norms):
            return False

        # Vanishing gradient detection
        if mean_norm < 1e-7:
            return False

        # Exploding gradient detection (more permissive than original 1e3)
        if mean_norm > 1e4:
            return False

        # Stability check - gradients shouldn't vary too much
        if cv > 0.5:  # 50% CV threshold for stability
            return False

        # Check for monotonic decay (could indicate dying gradients)
        if len(all_norms) > 3:
            decreasing_trend = all(
                all_norms[i] >= all_norms[i+1] for i in range(len(all_norms)-1)
            )
            if decreasing_trend and all_norms[-1] < all_norms[0] * 0.1:
                return False  # Rapid decay indicates problems

        return True

    def _enhanced_identity_detection(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> bool:
        """Enhanced identity kernel detection with multiple test sizes."""

        test_sizes = [(8, 8), (16, 32), (32, 16)]
        identity_results = []

        for height, width in test_sizes:
            test_input = torch.randn(height, width, device=gpu_slice.device)

            try:
                output = kernel.forward(test_input)
                is_identity = torch.allclose(test_input, output, rtol=1e-4, atol=1e-5)
                identity_results.append(is_identity)
            except:
                identity_results.append(False)

        # Kernel is identity if it passes all size tests
        return all(identity_results)

    def _emit_gradient_health_telemetry(self, mean_norm: float, cv: float, is_identity: bool):
        """Emit gradient health telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        severity = TelemetryLevel.TELEMETRY_WARN
        if mean_norm < 1e-7 or mean_norm > 1e4:
            severity = TelemetryLevel.TELEMETRY_ERROR

        telemetry_event = TelemetryEvent(
            event_name="unhealthy_gradients_detected",
            severity=severity,
            message=f"Unhealthy gradients in benchmark: mean_norm={mean_norm:.2e}, cv={cv:.3f}",
            attributes={
                "gradient_norm_mean": str(mean_norm),
                "gradient_cv": str(cv),
                "is_identity": str(is_identity)
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=severity,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

    def _emit_gradient_failure_telemetry(self, error_message: str):
        """Emit gradient failure telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="gradient_benchmark_failure",
            severity=TelemetryLevel.TELEMETRY_ERROR,
            message=f"Gradient benchmark failed: {error_message}",
            attributes={"error_message": error_message[:200]},  # Truncate long messages
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_ERROR,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)
```

### Data Structures

#### Thermal Stability Benchmarking

```python
class ThermalThrottlingBenchmark:
    """C-016 Thermal stability benchmarking for production readiness with Leyline integration."""

    def __init__(self):
        self.temperature_monitor = GPUTemperatureMonitor()
        self.performance_baseline = None

    def measure(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure performance stability under thermal stress with telemetry."""

        # Establish performance baseline
        baseline_latency = self._measure_baseline_performance(kernel, gpu_slice)

        # Create thermal stress and measure performance degradation
        thermal_results = []

        try:
            # Create sustained GPU load to increase temperature
            stress_tensors = self._create_thermal_stress(gpu_slice)

            # Monitor temperature rise
            initial_temp = self.temperature_monitor.get_gpu_temperature()

            # Wait for temperature to stabilize (up to 60 seconds)
            for _ in range(60):
                time.sleep(1)
                current_temp = self.temperature_monitor.get_gpu_temperature()

                # Test performance at current temperature
                try:
                    stressed_latency = self._measure_performance_under_stress(
                        kernel, gpu_slice
                    )

                    degradation_percent = (
                        (stressed_latency - baseline_latency) / baseline_latency * 100
                    )

                    thermal_results.append(ThermalMeasurement(
                        temperature_celsius=current_temp,
                        latency_ms=stressed_latency,
                        degradation_percent=degradation_percent,
                        timestamp=time.time()
                    ))

                    # Stop if temperature gets too high (safety)
                    if current_temp > 85:  # Conservative thermal limit
                        self._emit_thermal_warning_telemetry(current_temp)
                        break

                except Exception:
                    continue

            # Analyze thermal stability
            stability_analysis = self._analyze_thermal_stability(
                thermal_results, baseline_latency
            )

            # Emit thermal stability telemetry
            self._emit_thermal_stability_telemetry(
                stability_analysis.stable,
                max(r.temperature_celsius for r in thermal_results) if thermal_results else 0,
                max(r.degradation_percent for r in thermal_results) if thermal_results else 0
            )

            # Return maximum degradation percentage as benchmark metric
            max_degradation = max(r.degradation_percent for r in thermal_results) if thermal_results else 0
            return max_degradation

        finally:
            # Clean up thermal stress
            self._cleanup_thermal_stress(stress_tensors)

    def _measure_baseline_performance(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure baseline performance at normal temperature."""

        # Wait for GPU to cool down
        while self.temperature_monitor.get_gpu_temperature() > 70:
            time.sleep(5)

        # Measure baseline latency
        latencies = []
        test_input = torch.randn(128, 128, device=gpu_slice.device)

        for _ in range(10):
            torch.cuda.synchronize()
            start_time = perf_counter()

            _ = kernel.forward(test_input)
            torch.cuda.synchronize()

            latency_ms = (perf_counter() - start_time) * 1000
            latencies.append(latency_ms)

        return float(np.median(latencies))

    def _measure_performance_under_stress(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure performance under thermal stress conditions."""

        test_input = torch.randn(128, 128, device=gpu_slice.device)

        torch.cuda.synchronize()
        start_time = perf_counter()

        _ = kernel.forward(test_input)
        torch.cuda.synchronize()

        return (perf_counter() - start_time) * 1000

    def _create_thermal_stress(self, gpu_slice: GPUMemorySlice) -> List[torch.Tensor]:
        """Create sustained GPU load for thermal testing."""

        stress_tensors = []

        # Create multiple large tensors for sustained computation
        for i in range(5):
            try:
                tensor_a = torch.randn(512, 512, device=gpu_slice.device)
                tensor_b = torch.randn(512, 512, device=gpu_slice.device)

                # Perform matrix multiplication to generate heat
                result = torch.mm(tensor_a, tensor_b)
                stress_tensors.extend([tensor_a, tensor_b, result])

            except RuntimeError:
                # Stop if we run out of memory
                break

        return stress_tensors

    def _cleanup_thermal_stress(self, stress_tensors: List[torch.Tensor]) -> None:
        """Clean up thermal stress tensors."""

        for tensor in stress_tensors:
            try:
                del tensor
            except:
                pass

        torch.cuda.empty_cache()

        # Wait for GPU to cool down
        time.sleep(10)

    def _analyze_thermal_stability(
        self,
        measurements: List[ThermalMeasurement],
        baseline: float
    ) -> ThermalStabilityAnalysis:
        """Analyze thermal stability from measurements."""

        if not measurements:
            return ThermalStabilityAnalysis(
                stable=False,
                reason="No thermal measurements collected"
            )

        max_degradation = max(m.degradation_percent for m in measurements)
        avg_degradation = np.mean([m.degradation_percent for m in measurements])

        # Thermal stability criteria
        stable = (
            max_degradation < 50.0 and  # Max 50% performance loss
            avg_degradation < 20.0 and  # Average degradation < 20%
            all(m.temperature_celsius < 90 for m in measurements)  # Safe temperature
        )

        if stable:
            reason = f"Thermally stable (max degradation: {max_degradation:.1f}%)"
        else:
            reason = f"Thermal instability detected (max degradation: {max_degradation:.1f}%)"

        return ThermalStabilityAnalysis(stable=stable, reason=reason)

    def _emit_thermal_warning_telemetry(self, temperature: float):
        """Emit thermal warning telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="thermal_limit_reached",
            severity=TelemetryLevel.TELEMETRY_WARN,
            message=f"GPU temperature reached {temperature:.1f}°C, stopping thermal test",
            attributes={"temperature_celsius": str(temperature)},
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_WARN,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

    def _emit_thermal_stability_telemetry(self, stable: bool, max_temp: float, max_degradation: float):
        """Emit thermal stability telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        severity = TelemetryLevel.TELEMETRY_INFO if stable else TelemetryLevel.TELEMETRY_WARN

        telemetry_event = TelemetryEvent(
            event_name="thermal_stability_result",
            severity=severity,
            message=f"Thermal stability {'passed' if stable else 'failed'}: max_temp={max_temp:.1f}°C, max_degradation={max_degradation:.1f}%",
            attributes={
                "thermal_stable": str(stable),
                "max_temperature_celsius": str(max_temp),
                "max_degradation_percent": str(max_degradation)
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=severity,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

class GPUTemperatureMonitor:
    """GPU temperature monitoring for thermal testing."""

    def get_gpu_temperature(self) -> float:
        """Get current GPU temperature in Celsius."""

        try:
            import pynvml
            pynvml.nvmlInit()

            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)

            return float(temp)

        except Exception:
            # Fallback: use nvidia-smi command
            try:
                import subprocess
                result = subprocess.run([
                    'nvidia-smi',
                    '--query-gpu=temperature.gpu',
                    '--format=csv,noheader,nounits'
                ], capture_output=True, text=True)

                return float(result.stdout.strip())

            except Exception:
                # Return default safe temperature if monitoring fails
                return 65.0
```

## Chaos Engineering for Performance Testing

### BenchmarkChaosEngine Implementation

```python
class BenchmarkChaosEngine:
    """Chaos engineering engine for performance benchmarking with Leyline integration."""

    def __init__(self):
        self.scenarios = [
            MemoryPressureChaos(),
            NetworkLatencyChaos(),
            GPUThrottlingChaos(),
            FileSystemSlowdownChaos(),
            ProcessorLoadChaos()
        ]
        self.active_scenario = None

    @contextmanager
    def inject_scenario(self) -> BenchmarkChaosScenario:
        """Inject chaos scenario during benchmarking with telemetry."""

        scenario = random.choice(self.scenarios)
        self.active_scenario = scenario

        # Emit scenario activation telemetry
        self._emit_chaos_activation_telemetry(scenario.scenario_name)

        try:
            scenario.activate()
            yield scenario
        finally:
            scenario.deactivate()
            self.active_scenario = None
            # Emit scenario deactivation telemetry
            self._emit_chaos_deactivation_telemetry(scenario.scenario_name)

    def _emit_chaos_activation_telemetry(self, scenario_name: str):
        """Emit chaos scenario activation telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="chaos_scenario_activated",
            severity=TelemetryLevel.TELEMETRY_INFO,
            message=f"Chaos scenario {scenario_name} activated for benchmarking",
            attributes={"scenario_name": scenario_name},
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_INFO,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

    def _emit_chaos_deactivation_telemetry(self, scenario_name: str):
        """Emit chaos scenario deactivation telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        telemetry_event = TelemetryEvent(
            event_name="chaos_scenario_deactivated",
            severity=TelemetryLevel.TELEMETRY_INFO,
            message=f"Chaos scenario {scenario_name} deactivated",
            attributes={"scenario_name": scenario_name},
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=TelemetryLevel.TELEMETRY_INFO,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

class MemoryPressureChaos:
    """Memory pressure chaos scenario for benchmarking."""

    def __init__(self):
        self.pressure_tensors = []
        self.scenario_name = "memory_pressure"
        self.active = False

    def activate(self):
        """Create memory pressure during benchmarking."""
        self.active = True

        try:
            # Allocate tensors to create memory pressure
            available_memory = torch.cuda.get_device_properties(0).total_memory
            pressure_size = int(available_memory * 0.3 / (1024 * 1024))  # 30% of GPU memory

            tensor_size = min(pressure_size // 10, 512)  # Limit individual tensor size

            for i in range(10):
                try:
                    tensor = torch.randn(tensor_size, tensor_size, device='cuda:0')
                    self.pressure_tensors.append(tensor)
                except RuntimeError:
                    break

        except Exception:
            pass

    def deactivate(self):
        """Release memory pressure."""
        for tensor in self.pressure_tensors:
            try:
                del tensor
            except:
                pass

        self.pressure_tensors.clear()
        torch.cuda.empty_cache()
        self.active = False

    def get_result(self) -> ChaosScenarioResult:
        """Get chaos scenario result."""
        return ChaosScenarioResult(
            scenario_name=self.scenario_name,
            active_during_measurement=self.active,
            impact_level="medium",
            success=True
        )

class GPUThrottlingChaos:
    """GPU throttling chaos scenario with Leyline integration."""

    def __init__(self):
        self.load_tensors = []
        self.scenario_name = "gpu_throttling"
        self.active = False
        self.worker_thread = None
        self.stop_load = False

    def activate(self):
        """Create GPU computational load."""
        self.active = True
        self.stop_load = False

        # Start background GPU load
        self.worker_thread = threading.Thread(target=self._gpu_load_worker)
        self.worker_thread.start()

    def deactivate(self):
        """Stop GPU load."""
        self.stop_load = True

        if self.worker_thread:
            self.worker_thread.join(timeout=2.0)

        # Clean up load tensors
        for tensor in self.load_tensors:
            try:
                del tensor
            except:
                pass

        self.load_tensors.clear()
        torch.cuda.empty_cache()
        self.active = False

    def _gpu_load_worker(self):
        """Worker thread to create sustained GPU load."""

        try:
            # Create sustained computational load
            tensor_a = torch.randn(256, 256, device='cuda:0')
            tensor_b = torch.randn(256, 256, device='cuda:0')

            while not self.stop_load:
                # Perform matrix multiplications to create load
                result = torch.mm(tensor_a, tensor_b)

                # Update one tensor to prevent optimization
                tensor_a = result + 0.01

                time.sleep(0.01)  # Small sleep to prevent complete saturation

        except Exception:
            pass

    def get_result(self) -> ChaosScenarioResult:
        """Get chaos scenario result."""
        return ChaosScenarioResult(
            scenario_name=self.scenario_name,
            active_during_measurement=self.active,
            impact_level="high",
            success=True
        )

class ProcessorLoadChaos:
    """CPU load chaos scenario with Leyline integration."""

    def __init__(self):
        self.scenario_name = "processor_load"
        self.active = False
        self.load_threads = []
        self.stop_load = False

    def activate(self):
        """Create CPU load."""
        self.active = True
        self.stop_load = False

        # Start multiple CPU load threads
        num_threads = min(4, os.cpu_count())

        for _ in range(num_threads):
            thread = threading.Thread(target=self._cpu_load_worker)
            thread.start()
            self.load_threads.append(thread)

    def deactivate(self):
        """Stop CPU load."""
        self.stop_load = True

        for thread in self.load_threads:
            thread.join(timeout=1.0)

        self.load_threads.clear()
        self.active = False

    def _cpu_load_worker(self):
        """Worker function to create CPU load."""
        while not self.stop_load:
            # Create CPU-intensive computation
            x = sum(i * i for i in range(10000))
            time.sleep(0.005)  # Small sleep to prevent 100% CPU usage

    def get_result(self) -> ChaosScenarioResult:
        """Get chaos scenario result."""
        return ChaosScenarioResult(
            scenario_name=self.scenario_name,
            active_during_measurement=self.active,
            impact_level="medium",
            success=True
        )
```

## Conservative Mode Performance Assessment

### Conservative Performance Validation

```python
class ConservativePerformanceValidator:
    """Performance validation under conservative mode constraints with Leyline integration."""

    def __init__(self, conservative_config: ConservativeConfig):
        self.config = conservative_config
        self.reduced_benchmarks = {
            'essential_latency': EssentialLatencyBenchmark(),
            'memory_efficiency': BasicMemoryBenchmark()
        }

    def validate_conservative_performance(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> ConservativePerformanceResult:
        """Validate performance under conservative mode constraints with telemetry."""

        start_time = perf_counter()
        results = {}

        # Run essential benchmarks with reduced scope
        for benchmark_name, benchmark in self.reduced_benchmarks.items():
            try:
                # Reduced number of runs for conservative mode
                measurements = []
                for _ in range(3):  # Only 3 runs instead of 10+
                    measurement = benchmark.measure(kernel, gpu_slice)
                    measurements.append(measurement)

                # Calculate basic statistics
                if measurements:
                    results[benchmark_name] = {
                        'mean': np.mean(measurements),
                        'min': np.min(measurements),
                        'max': np.max(measurements),
                        'passed': self._evaluate_conservative_threshold(
                            benchmark_name, measurements
                        )
                    }

            except Exception as e:
                results[benchmark_name] = {
                    'error': str(e),
                    'passed': False
                }

        # Overall conservative performance assessment
        overall_passed = all(
            result.get('passed', False) for result in results.values()
        )

        validation_time_ms = (perf_counter() - start_time) * 1000

        # Emit conservative validation telemetry
        self._emit_conservative_validation_telemetry(overall_passed, validation_time_ms)

        return ConservativePerformanceResult(
            passed=overall_passed,
            results=results,
            validation_time_ms=validation_time_ms,
            conservative_mode=True,
            reason="Conservative performance validation complete" if overall_passed
                   else "Failed conservative performance thresholds"
        )

    def _evaluate_conservative_threshold(
        self,
        benchmark_name: str,
        measurements: List[float]
    ) -> bool:
        """Evaluate performance against conservative thresholds."""

        mean_value = np.mean(measurements)

        # Conservative thresholds (more lenient than standard mode)
        conservative_thresholds = {
            'essential_latency': 1000.0,    # 1 second max latency
            'memory_efficiency': 2048.0     # 2GB max memory usage
        }

        threshold = conservative_thresholds.get(benchmark_name, float('inf'))
        return mean_value <= threshold

    def _emit_conservative_validation_telemetry(self, passed: bool, elapsed_ms: float):
        """Emit conservative validation telemetry using Leyline TelemetryPacket"""
        from google.protobuf import timestamp_pb2

        severity = TelemetryLevel.TELEMETRY_INFO if passed else TelemetryLevel.TELEMETRY_WARN

        telemetry_event = TelemetryEvent(
            event_name="conservative_performance_validation",
            severity=severity,
            message=f"Conservative performance validation {'passed' if passed else 'failed'} in {elapsed_ms:.1f}ms",
            attributes={
                "validation_passed": str(passed),
                "elapsed_ms": str(elapsed_ms),
                "conservative_mode": "true"
            },
            timestamp=timestamp_pb2.Timestamp()
        )

        telemetry_packet = TelemetryPacket(
            packet_id=str(uuid.uuid4()),
            timestamp=timestamp_pb2.Timestamp(),
            source_subsystem="urabrask",
            level=severity,
            events=[telemetry_event]
        )

        # Emit via Nissa observability
        self.telemetry_client.emit_telemetry_packet(telemetry_packet)

class EssentialLatencyBenchmark:
    """Essential latency benchmark for conservative mode."""

    def measure(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure essential latency with minimal overhead."""

        # Single small test for conservative mode
        test_input = torch.randn(16, 16, device=gpu_slice.device)

        torch.cuda.synchronize()
        start_time = perf_counter()

        output = kernel.forward(test_input)
        torch.cuda.synchronize()

        latency_ms = (perf_counter() - start_time) * 1000

        # Verify output sanity
        if torch.isnan(output).any() or torch.isinf(output).any():
            raise BenchmarkFailure("Invalid output in conservative latency test")

        return latency_ms

class BasicMemoryBenchmark:
    """Basic memory benchmark for conservative mode."""

    def measure(
        self,
        kernel: CompiledKernelArtifact,
        gpu_slice: GPUMemorySlice
    ) -> float:
        """Measure basic memory usage."""

        torch.cuda.empty_cache()
        baseline_memory = torch.cuda.memory_allocated()

        # Single test with small input
        test_input = torch.randn(32, 32, device=gpu_slice.device)
        output = kernel.forward(test_input)

        peak_memory = torch.cuda.memory_allocated()
        memory_used_mb = (peak_memory - baseline_memory) / (1024 * 1024)

        return memory_used_mb
```

## Performance Data Structures and Results

### Benchmark Results and Statistics

```python
@dataclass
class BenchmarkResults:
    """Comprehensive benchmark results container with Leyline integration."""
    kernel_id: str
    mode: ValidationMode  # From Leyline
    benchmarks: Dict[str, BenchmarkStatistics] = field(default_factory=dict)
    chaos_results: List[ChaosScenarioResult] = field(default_factory=list)
    hardware_context: Optional[HardwareContext] = None  # From Leyline
    timestamp: float = field(default_factory=time.time)
    total_duration_ms: float = 0.0

    def add_benchmark(
        self,
        name: str,
        stats: BenchmarkStatistics,
        chaos_results: List[ChaosScenarioResult] = None
    ):
        """Add benchmark results."""
        self.benchmarks[name] = stats
        if chaos_results:
            self.chaos_results.extend(chaos_results)

@dataclass
class BenchmarkStatistics:
    """Statistical analysis of benchmark measurements with Leyline integration."""
    mean: float = 0.0
    std: float = 0.0
    min: float = 0.0
    max: float = 0.0
    median: float = 0.0
    p95: float = 0.0
    coefficient_of_variation: float = 0.0
    sample_count: int = 0
    failure_count: int = 0
    variance_acceptable: bool = False
    hardware_compliant: bool = False
    valid: bool = True
    reason: Optional[str] = None

@dataclass
class ThermalStabilityResult:
    """Result of thermal stability testing."""
    stable: bool
    baseline_latency_ms: float
    max_temperature_celsius: float
    max_degradation_percent: float
    thermal_measurements: List[ThermalMeasurement] = field(default_factory=list)
    reason: str = ""

@dataclass
class ThermalMeasurement:
    """Individual thermal measurement point."""
    temperature_celsius: float
    latency_ms: float
    degradation_percent: float
    timestamp: float

@dataclass
class ConservativePerformanceResult:
    """Result of conservative mode performance validation."""
    passed: bool
    results: Dict[str, Any] = field(default_factory=dict)
    validation_time_ms: float = 0.0
    conservative_mode: bool = True
    reason: str = ""

@dataclass
class ChaosScenarioResult:
    """Result of individual chaos scenario."""
    scenario_name: str
    active_during_measurement: bool = False
    impact_level: str = "medium"  # low, medium, high
    success: bool = True
    error: Optional[str] = None

@dataclass
class BenchmarkFailure:
    """Represents a failed benchmark measurement."""
    error_message: str

    def __init__(self, message: str):
        self.error_message = message
```

## Integration Points

### Internal Integration

| Component | Interface | Data Flow |
|-----------|-----------|-----------|
| Main Validation Engine | `run_enhanced_benchmarks` | Receives kernels, returns BenchmarkResults |
| Safety Validation | Prerequisites | Must pass safety before benchmarking |
| Conservative Mode | `validate_conservative_performance` | Reduced benchmark scope |

### External Integration

| Subsystem | Contract | Pattern |
|-----------|----------|---------|
| Leyline | HardwareContext, TelemetryPacket | Async via shared contracts |
| Tezzeret | CompiledKernelArtifact | Input via message bus |
| Nissa | TelemetryPacket | Telemetry emission |

### Leyline Contracts Used

This component uses the following shared contracts:
- `leyline.SystemStatePacket` - System state reporting
- `leyline.HardwareContext` - Hardware-aware benchmark configuration
- `leyline.TelemetryPacket` - Performance metrics emission
- `leyline.TelemetryEvent` - Individual benchmark events
- `leyline.TelemetryLevel` - Event severity levels
- `leyline.EventEnvelope` - Result publishing
- `leyline.MessagePriority` - Message routing priority
- `leyline.ValidationMode` - Benchmark mode selection

## Configuration

```yaml
performance_benchmarking:
  # Core settings
  standard_runs: 10       # Number of runs for standard mode
  comprehensive_runs: 25  # Number of runs for comprehensive mode
  emergency_runs: 3       # Number of runs for emergency mode

  # Variance thresholds (coefficient of variation)
  latency_cv_threshold: 0.18      # 18% max CV for latency
  memory_cv_threshold: 0.12       # 12% max CV for memory
  gradient_cv_threshold: 0.25     # 25% max CV for gradients
  thermal_cv_threshold: 0.15      # 15% max CV for thermal

  # Hardware-specific targets
  h100_latency_budget_ms: 50      # H100 latency target
  a100_latency_budget_ms: 100     # A100 latency target
  default_latency_budget_ms: 200  # Older GPU latency target

  # Thermal testing
  thermal_limit_celsius: 85       # Maximum safe temperature
  thermal_degradation_max: 50     # Max 50% performance loss

  # Chaos injection
  chaos_injection_rate: 0.2       # 20% of runs include chaos
  chaos_scenarios:
    - memory_pressure
    - gpu_throttling
    - processor_load

  # Conservative mode
  conservative_latency_max_ms: 1000    # 1 second max
  conservative_memory_max_mb: 2048     # 2GB max
```

### Configuration Validation

- **standard_runs**: Must be between 5 and 50
- **latency_cv_threshold**: Range 0.05 to 0.30
- **thermal_limit_celsius**: Range 70 to 95
- **chaos_injection_rate**: Range 0.0 to 0.5

## Performance Characteristics

### Benchmarks

| Operation | Target | Measured | Conditions |
|-----------|--------|----------|------------|
| Latency Benchmarking | <100ms | 80-95ms | A100, standard mode |
| Memory Benchmarking | <1s | 0.5-0.8s | 4 test sizes |
| Gradient Benchmarking | <0.5s | 0.3-0.4s | 5 backward passes |
| Thermal Testing | <60s | 45-55s | Temperature stabilization |
| Conservative Mode | <30s | 20-28s | Reduced scope |

### Resource Usage

- **Memory**: 4GB GPU budget for testing
- **CPU**: 20-30% during benchmarking
- **I/O**: Minimal, metrics only
- **GPU**: 100% during kernel execution

### Optimization Strategies

1. **Test Size Adaptation**: Hardware-specific test sizes
2. **Warmup Optimization**: Fewer warmup runs for modern GPUs
3. **Median vs Mean**: Use median for robustness against outliers

## Error Handling

### Failure Modes

| Error Type | Detection | Recovery |
|------------|-----------|----------|
| Benchmark timeout | Timer exceeds limit | Skip measurement, continue |
| High variance | CV > threshold | Rerun with more samples |
| Thermal throttling | Temperature > 85°C | Stop thermal test |
| Chaos failure | Kernel crash during chaos | Record failure, continue |
| GPU OOM | CUDA error | Reduce test size, retry |

### Circuit Breakers

```python
# Benchmark-specific circuit breaker
circuit_breaker = CircuitBreaker(
    failure_threshold=5,
    recovery_timeout_ms=60000,
    half_open_requests=2
)
```

### Fallback Behavior

When this component fails:
1. Switch to conservative benchmarking mode
2. Reduce test sizes and run counts
3. Skip non-essential benchmarks (thermal, chaos)

## Testing Strategy

### Unit Tests

```python
def test_realistic_variance_calculation():
    """Test realistic variance calculation with CV thresholds"""
    measurements = [100, 105, 95, 102, 98]  # 5% variation
    suite = EnhancedBenchmarkSuite(hardware_context)
    stats = suite._calculate_realistic_stats(measurements, 'forward_latency')
    assert stats.coefficient_of_variation < 0.18  # Within 18% CV threshold
    assert stats.variance_acceptable == True
```

Coverage targets:
- Line coverage: >90%
- Branch coverage: >80%
- Critical paths: 100%

### Integration Tests

- **Test**: Hardware-aware benchmark adaptation
  - **Setup**: Different GPU configurations
  - **Validation**: Correct test sizes selected

- **Test**: Chaos injection during benchmarking
  - **Setup**: Enable chaos scenarios
  - **Validation**: Benchmarks complete despite chaos

### Property-Based Tests

```python
@hypothesis.given(
    run_count=strategies.integers(min_value=3, max_value=50)
)
def test_benchmark_statistics_property(run_count):
    """Property: Statistics valid for any run count >= 3"""
    kernel = create_test_kernel()
    suite = EnhancedBenchmarkSuite(hardware_context)
    results = suite.run_enhanced_benchmarks(kernel, gpu_slice, ValidationMode.STANDARD)
    assert all(stat.valid for stat in results.benchmarks.values())
    assert results.total_duration_ms > 0
```

## Monitoring & Observability

### Metrics

| Metric | Type | Purpose |
|--------|------|---------|
| `urabrask.benchmark.latency_ms` | Histogram | Forward pass latency distribution |
| `urabrask.benchmark.memory_usage_mb` | Histogram | Memory usage distribution |
| `urabrask.benchmark.variance_cv` | Gauge | Coefficient of variation per benchmark |
| `urabrask.benchmark.chaos_injections_total` | Counter | Total chaos scenarios injected |
| `urabrask.benchmark.thermal_degradation_percent` | Gauge | Performance degradation under thermal stress |

### Logging

```python
# Logging levels and patterns
logger.debug(f"[Benchmark] Starting {benchmark_name} for kernel {kernel_id}")
logger.info(f"[Benchmark] Results: mean={stats.mean:.2f}ms, CV={stats.cv:.3f}")
logger.warning(f"[Benchmark] High variance detected: CV={cv:.3f}")
logger.error(f"[Benchmark] Failed: {error}", exc_info=True)
```

### Tracing

- **Span**: `urabrask.benchmark.execution`
  - **Attributes**: kernel_id, benchmark_name, validation_mode
  - **Events**: warmup_start, measurement_start, chaos_injection, completion

## Security Considerations

- **Input Validation**: Test input sizes validated against hardware limits
- **Access Control**: Internal service only
- **Data Handling**: No sensitive data in benchmarks
- **Audit Trail**: All benchmark results logged with parameters

## Migration Notes

> **From Version 2.x**: Realistic variance targets (12-18% CV) replace zero-variance goals
> **To Leyline**: All hardware configuration via HardwareContext

## Implementation Checklist

- [x] Core benchmarking algorithm
- [x] Hardware-aware latency benchmarking
- [x] Memory efficiency benchmarking
- [x] Gradient flow benchmarking
- [x] Thermal stability testing
- [x] Chaos engineering integration
- [x] Conservative mode validation
- [x] Unit test coverage >90%
- [x] Integration tests complete
- [x] Performance targets met
- [x] Documentation updated
- [x] Security review passed
- [x] Production monitoring ready

## References

### Internal References
- Parent: [07-urabrask-unified-design.md](07-urabrask-unified-design.md)
- Related: [07.1-urabrask-safety-validation.md](07.1-urabrask-safety-validation.md)
- Tests: `tests/urabrask/benchmarks/`

### External References
- [CUDA Performance Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)
- [PyTorch Benchmarking Guide](https://pytorch.org/tutorials/recipes/recipes/benchmark.html)
- [Thermal Design Guidelines](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf)

## History & Context

### Implementation Notes
- **2025-01-10**: Added realistic variance targets (C-016)
- **2025-01-10**: Integrated thermal stability testing
- **2025-01-12**: Added chaos engineering scenarios

### Known Issues
- **ISSUE-001**: Thermal monitoring requires pynvml or nvidia-smi
- **ISSUE-002**: Chaos scenarios may interfere with concurrent processes

---

*Component Owner: System Architecture Team | Last Updated: 2025-01-12*