---
title: BIBLIOGRAPHY / READING LIST
split_mode: consolidated
appendix: "C"
coauthors:
  - John Morrissey
  - Codex CLI (OpenAI)
generated_by: scripts/split_paper.py
---

# Appendix C: Bibliography / Reading List
This appendix provides a consolidated list of all references from the original research notes for further reading and to acknowledge the broader literature that informed this work.

1. Beaulieu, S., Frasca, F., Xu, Y., Goyal, S., Pal, C., & Larochelle, H. (2020). Learning sparse representations in reinforcement learning with the successor features. In Advances in Neural Information Processing Systems (NeurIPS).
2. Bengio, Y., & LeCun, Y. (2007). Scaling learning algorithms towards AI. In Large-scale kernel machines (Vol. 34, pp. 321–360).
3. Elsken, T., Metzen, J. H., & Hutter, F. (2019). Neural architecture search: A survey. Journal of Machine Learning Research, 20(55), 1–21.
4. Goyal, A., Lamb, A. M., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., & Schölkopf, B. (2021). Inductive biases, pretraining and fine-tuning for transformer-based geometric reasoning. arXiv preprint arXiv:2110.06091.
5. Han, S., Pool, J., Tran, J., & Dally, W. (2015). Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems (NeurIPS).
6. Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:1503.02531.
7. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., ... & Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning (ICML).
8. Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., & Aila, T. (2020). Training generative adversarial networks with limited data. arXiv preprint arXiv:2006.06676.1
9. Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hadsell, R. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13), 3521–3526.
10. Mallya, A., & Lazebnik, S. (2018). Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In ECCV.
11. Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. (2019). Continual lifelong learning with neural networks: A review. Neural Networks, 113, 54–71.
12. Rosenbaum, C., Klinger, T., & Riemer, M. (2019). Routing networks: Adaptive selection of non-linear functions for multi-task learning. In ICLR.
13. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., ... & Hadsell, R. (2016). Progressive neural networks. arXiv preprint arXiv:1606.04671.
14. Schick, T., & Schütze, H. (2020). It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).
15. Alet, F., et al. (2023). Modular Deep Learning. arXiv preprint arXiv:2302.11529v2.
16. Anthropic. (2024). Model Stitching by Functional Latent Alignment. arXiv: 2505.20142.
19. Chen, C., et al. (2021). Neural Network Surgery: Injecting Data Patterns. ACL Anthology.
20. Chen, R., et al. (2020). Accurate Neural Network Computer Vision Without The 'Black Box'. Duke Today.
21. Du, J., et al. (2025). Knowledge Grafting of Large Language Models. arXiv preprint arXiv:2505.18502v1.
22. Hadsell, R. (2014). What is Catastrophic Forgetting?. IBM.
23. He, S., et al. (2025). Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models. arXiv preprint arXiv:2504.20020v1.
24. Jin, X., et al. (2025). ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates. arXiv preprint arXiv:2505.12242v1.
25. Lansdell, B., & Kording, K. (2023). Feature alignment as a generative process. PMC.
26. Le, T., et al. (2024). MergeKD: an empirical framework for combining knowledge distillation with model fusion using BERT model. ScholarSpace.
27. Li, Z., et al. (2024). Training Independent Subnetworks for Structural Ensembling. OpenReview.
28. Lu, C., et al. (2024). Dynamic Neural Network Structure: A Review for Its Theories and Applications. ResearchGate.
29. Ma, X., et al. (2024). Cross-Silo Feature Space Alignment for Federated Learning on Clients with Imbalanced Data. AAAI Conference on Artificial Intelligence.
30. Peters, B. (2025). Dynamic neural networks: advantages and challenges. National School of Development, Peking University.
31. Shao, D., et al. (2024). Prompt-Based Distribution Alignment for Unsupervised Domain Adaptation. AAAI Conference on Artificial Intelligence.
32. Sun, Q., et al. (2024). DeepArc: Modularizing neural networks for the model maintenance. <InK@SMU.edu.sg>.
33. Wortsman, M., et al. (2024). Aligning latent representations of neural activity. PMC.
34. Wu, P., et al. (2024). On the Direct Alignment of Latent Spaces. OpenReview.
35. Wikipedia contributors. (2024). Modular neural network. Wikipedia.
36. Zhang, C., et al. (2024). Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. arXiv preprint arXiv:2401.13721v1.
37. Zhuang, F., et al. (2016). Transfer Learning across Feature-Rich Heterogeneous Feature Spaces via Feature-Space Remapping (FSR). PMC.
38. Zador, A. (2024). Latent Space Translation via Semantic Alignment. OpenReview.
