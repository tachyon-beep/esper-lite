# Finding Ticket: Compiled Train Step Caches Fallback Permanently

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B8-PT-03` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 8 |
| **Agent** | `pytorch` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/helpers.py` |
| **Line(s)** | `146-179` |
| **Function/Class** | `_get_compiled_train_step()` |

---

## Summary

**One-line summary:** `functools.cache` means if compilation fails once, all subsequent calls use uncompiled path for process lifetime.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [x] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
@functools.cache
def _get_compiled_train_step(use_compile: bool = True) -> Callable[...]:
    if use_compile:
        try:
            return torch.compile(_train_step_impl, mode="default", dynamic=True)
        except Exception as e:
            logger.warning(...)
            return _train_step_impl  # Cached forever!
```

If compilation fails due to a transient issue (e.g., GPU memory pressure, driver glitch), the fallback is cached permanently. All subsequent training runs in the same process will use the uncompiled path, potentially 2-5x slower.

### Impact

- **Performance degradation**: Entire process runs slower after one transient failure
- **Silent slowdown**: Only a warning is logged, easy to miss
- **No recovery**: Restart required to attempt recompilation

---

## Recommended Fix

**Option 1 - Don't cache failures:**
```python
_compiled_step = None

def _get_compiled_train_step(use_compile: bool = True) -> Callable[...]:
    global _compiled_step
    if not use_compile:
        return _train_step_impl
    if _compiled_step is not None:
        return _compiled_step
    try:
        _compiled_step = torch.compile(_train_step_impl, mode="default", dynamic=True)
        return _compiled_step
    except Exception as e:
        logger.warning(...)
        return _train_step_impl  # Not cached, will retry next call
```

**Option 2 - Retry with backoff:**
Track failure count, retry compilation after N calls or time delay.

---

## Verification

### How to Verify the Fix

- [ ] Modify caching to not persist failures
- [ ] Add metric for compiled vs uncompiled path usage
- [ ] Test recovery after simulated compilation failure

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch8-pytorch.md`
**Section:** "P2 - Compiled train step caches fallback permanently"
