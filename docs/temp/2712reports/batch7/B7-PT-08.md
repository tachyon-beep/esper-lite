# Finding Ticket: Gradient Norm Averaging is Non-Standard

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B7-PT-08` |
| **Severity** | `P4` |
| **Status** | `open` |
| **Batch** | 7 |
| **Agent** | `pytorch` |
| **Domain** | `simic/telemetry` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/telemetry/gradient_collector.py` |
| **Line(s)** | `211` |
| **Function/Class** | `SeedGradientCollector.materialize()` |

---

## Summary

**One-line summary:** Dividing total L2 norm by parameter count is non-standard metric.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
gradient_norm = (total_squared_norm ** 0.5) / n_grads
```

This computes "average per-parameter-tensor norm" which is:
- `sqrt(sum(||g_i||^2)) / n` where `n` is number of parameter tensors

This is NOT the same as:
1. **Total L2 norm:** `sqrt(sum(||g_i||^2))` (standard for gradient clipping)
2. **Per-element L2 norm:** `sqrt(sum(g^2)) / num_elements`
3. **Mean gradient magnitude:** `mean(|g|)`

The metric is non-standard and may confuse users expecting typical gradient norm behavior.

---

## Recommended Fix

Either document the metric clearly or use standard L2 norm:

**Option 1 - Document:**
```python
# Per-tensor average L2 norm: sqrt(sum(||g_i||^2)) / num_tensors
# This gives average gradient magnitude per parameter tensor, not total norm.
gradient_norm = (total_squared_norm ** 0.5) / n_grads
```

**Option 2 - Use standard total norm:**
```python
# Total L2 norm (same as clip_grad_norm_)
gradient_norm = total_squared_norm ** 0.5
```

---

## Verification

### How to Verify the Fix

- [ ] Document the metric formula clearly
- [ ] Consider renaming to `avg_per_tensor_norm` for clarity
- [ ] Or change to standard total L2 norm

---

## Related Findings

None.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `REFINE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Metric (total_L2 / n_tensors) is intentional for telemetry - normalizes across models with different param counts. Standard total norm would make comparisons meaningless.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `REFINE` |
| **Reviewer** | PyTorch Specialist |

**Evaluation:** Not a bug. Different from clip_grad_norm_ but appropriate - telemetry wants comparable values across architectures.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Fix is documentation only - rename to `avg_per_tensor_norm` or add formula comment. Valid P4 naming/clarity issue.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch7-pytorch.md`
**Section:** "Gradient norm averaging is non-standard"

**Report file:** `docs/temp/2712reports/batch7-drl.md`
**Section:** "Gradient norm averaging formula may be unconventional"
