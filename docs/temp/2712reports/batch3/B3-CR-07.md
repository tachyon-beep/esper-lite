# Finding Ticket: force_alpha Not DDP-Safe

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-CR-07` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/slot.py` |
| **Line(s)** | `1127-1185` |
| **Function/Class** | `SeedSlot.force_alpha()` |

---

## Summary

**One-line summary:** `force_alpha()` is documented as NOT thread-safe and NOT DDP-safe - ensure callers are aware of these constraints.

**Category:**
- [ ] Correctness bug
- [x] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `force_alpha()` context manager mutates local state without DDP synchronization. It's correctly documented as unsafe, but callers might not realize the implications.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/slot.py:1127

@contextmanager
def force_alpha(self, value: float) -> Generator[None, None, None]:
    """Temporarily override alpha.

    NOT thread-safe. NOT DDP-safe.
    """
```

### Current Mitigation

It's documented correctly and used only for counterfactual evaluation in eval mode.

---

## Recommended Fix

Add runtime check or assertion when DDP is initialized:

```python
@contextmanager
def force_alpha(self, value: float) -> Generator[None, None, None]:
    if torch.distributed.is_initialized():
        warnings.warn(
            "force_alpha() is not DDP-safe. Ensure symmetric calls across ranks.",
            stacklevel=2
        )
    ...
```

---

## Verification

### How to Verify the Fix

- [ ] Add assertion or warning when called under DDP
- [ ] Verify counterfactual evaluation doesn't use DDP

---

## Related Findings

- B3-PT-10: force_alpha compile-safety undocumented

---

## Cross-Review: PyTorch Specialist

| Verdict | Evaluation |
|---------|------------|
| **NEUTRAL** | Already well-documented with clear warnings. The suggested runtime warning under `torch.distributed.is_initialized()` adds noise since counterfactual eval typically runs post-training in single-process mode. Keep docs, skip runtime check. |

---

## Cross-Review: DRL Specialist

| Verdict | Evaluation |
|---------|------------|
| **ENDORSE** | The `force_alpha` method is correctly scoped to counterfactual evaluation (differential validation in eval mode) - this measures seed contribution post-training, not during gradient updates. DDP-unsafe mutations are acceptable here. Existing TODO documents path to DDP-safe counterfactual forward if needed. |

## Cross-Review: Code Review Specialist

| Verdict | Evaluation |
|---------|------------|
| **NEUTRAL** | Documentation is comprehensive (lines 1139-1151 have explicit thread-safety/DDP warnings, TODO at 1187-1192 documents future alternatives). Runtime warning would add noise for legitimate single-rank eval. Existing docstring warnings are sufficient. |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-codereview.md`
**Section:** "P2 - Performance/Resource" (SLOT-3)
