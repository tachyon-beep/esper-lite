# Finding Ticket: DDP Gate Synchronization Ordering Assumption

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B2-CR-03` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 2 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/slot.py` |
| **Line(s)** | `(DDP integration)` |
| **Function/Class** | `_sync_gate_decision()` |

---

## Summary

**One-line summary:** `_sync_gate_decision()` broadcasts rank-0's decision, but assumes slots are processed in consistent order across ranks.

**Category:**
- [ ] Correctness bug
- [x] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

`_sync_gate_decision()` broadcasts rank-0's gate decision to all ranks. If slots are processed in different orders across ranks (e.g., due to hash-based dict ordering or non-deterministic iteration), deadlock or data corruption could occur.

### Current Mitigation

`self.seed_slots` is an `nn.ModuleDict` which maintains insertion order in Python 3.7+. As long as slot creation order is deterministic (it is - from the `slots` list), this is safe.

### Risk

No explicit verification of this assumption. If a future change breaks ordering determinism, DDP sync would break silently.

---

## Recommended Fix

Add defensive logging/assertion to detect ordering issues:

```python
def _sync_gate_decision(self, slot_id: str, decision: bool) -> bool:
    """Synchronize gate decision across DDP ranks.

    INVARIANT: All ranks must call this with the same slot_id sequence.
    Mismatched ordering causes deadlock.
    """
    if dist.is_initialized() and dist.get_world_size() > 1:
        # Log slot_id for debugging ordering issues
        logger.debug(f"Rank {dist.get_rank()} syncing gate for {slot_id}")
    # ...
```

---

## Verification

### How to Verify the Fix

- [ ] Add DDP integration test with multiple slots
- [ ] Verify all ranks process slots in same order

---

## Related Findings

| Ticket ID | Relationship | Notes |
|-----------|--------------|-------|
| `B2-DRL-05` | `related` | Alpha controller sync also relies on ordering |

---

## Cross-Review

| Agent | Verdict | Evaluation |
|-------|---------|------------|
| **PyTorch** | ENDORSE | DDP collective ordering is critical - mismatched call sequences cause NCCL hangs that are notoriously hard to debug. Recommend adding dist.monitored_barrier() with timeout in debug builds, or using torch.distributed.collective_fingerprint (2.0+) to detect ordering mismatches before deadlock. |
| **CodeReview** | ENDORSE | DDP ordering assumptions are fragile and notoriously hard to debug when broken. The current reliance on nn.ModuleDict insertion order is correct but undocumented - adding the INVARIANT comment and debug logging is low-cost, high-value defensive documentation. |
| **DRL** | ENDORSE | Gate decisions determine which seeds participate in the forward pass and thus which policies contribute gradients. Cross-rank ordering mismatch causes some ranks to blend seed A while others blend seed B - this breaks the policy gradient estimator since log-probabilities and advantages no longer correspond to the same policy mixture across the all-reduce. |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch2-codereview.md`
**Section:** "Cross-Cutting Integration Risks" - "DDP Gate Synchronization Ordering"
