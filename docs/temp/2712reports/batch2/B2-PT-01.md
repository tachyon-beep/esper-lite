# Finding Ticket: Thread-Local Cache Not Cleared on Device Move

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B2-PT-01` |
| **Severity** | `P1` |
| **Status** | `open` |
| **Batch** | 2 |
| **Agent** | `pytorch` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/blending.py` |
| **Line(s)** | `76` |
| **Function/Class** | `BlendAlgorithm._get_cached_alpha_tensor()` |

---

## Summary

**One-line summary:** Thread-local alpha tensor cache may hold stale device reference when module is moved via `.to(device)`.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [x] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `threading.local()` instance is created per `BlendAlgorithm` instance to cache alpha tensors per thread (for DataParallel safety). However, when a `GatedBlend` is moved to a new device mid-training via `.to(device)`, the cache holds a stale device reference until the next cache miss.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/blending.py:62-104

def __init__(self) -> None:
    self._alpha_cache_local = threading.local()  # Per-thread cache

def _get_cached_alpha_tensor(self, batch_size: int, device: torch.device, dtype: torch.dtype) -> torch.Tensor:
    cache = getattr(self._alpha_cache_local, 'cache', None)  # L76
    if cache is not None and cache.shape[0] >= batch_size and cache.device == device and cache.dtype == dtype:
        return cache[:batch_size]
    # ... reallocate on cache miss
```

### Natural Mitigation

Cache invalidation happens naturally via device mismatch check at line 79. If the cached tensor is on the wrong device, a new tensor is allocated. The old tensor is not explicitly released, but Python GC will handle it when no references remain.

### Why This Is Still P1

1. **Memory leak potential:** The old tensor lingers briefly until overwritten
2. **Strict cleanup violation:** Best practice is to clear caches on device moves
3. **DataParallel accumulation:** In long-running training with persistent workers, each thread accumulates cache entries

---

## Recommended Fix

### Option A: Clear cache in module movement

Add explicit cache clearing when device changes:

```python
def _clear_cache(self) -> None:
    """Clear thread-local alpha tensor cache."""
    if hasattr(self._alpha_cache_local, 'cache'):
        del self._alpha_cache_local.cache

def _modules_hooks(self):
    # Register hook to clear cache on device move
    pass
```

### Option B: Document current behavior

If the memory impact is truly negligible (single scalar tensor):

```python
def _get_cached_alpha_tensor(...):
    """...

    Note: Cache is invalidated on device mismatch but old tensor
    may linger briefly until garbage collected. This is intentional
    to avoid hook complexity for minimal memory savings.
    """
```

### Verdict

**Recommend Option B** given the natural mitigation - document the behavior rather than adding cleanup complexity.

---

## Verification

### How to Verify the Fix

- [ ] Add test for device move behavior: move model to GPU, then CPU, verify no stale tensors
- [ ] Profile memory usage across device moves in DataParallel training

---

## Related Findings

| Ticket ID | Relationship | Notes |
|-----------|--------------|-------|
| `B2-CR-01` | `duplicate` | CodeReview perspective - thread-local cache accumulation |
| `B2-DRL-07` | `related` | DRL perspective - cache never cleaned in practice |
| `B2-CR-05` | `related` | Uses getattr() for cache access (defensive pattern) |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch2-pytorch.md`
**Section:** "File-by-File Analysis" - blending.py - P1 finding

---

## Cross-Review

| Verdict | Reviewer | Domain |
|---------|----------|--------|
| **OBJECT** | DRL Specialist | Deep RL |
| **OBJECT** | PyTorch Specialist | PyTorch Engineering |

**DRL Evaluation:** P1 severity is excessive; `reset_cache()` already exists (L93-104) and cache auto-invalidates on device mismatch (L79).
Recommend downgrade to P3/P4 with Option B - document that `reset_cache()` should be called at epoch boundaries.

**PyTorch Evaluation:** The code already has `reset_cache()` at line 93-104 with clear docstring recommending epoch-boundary calls. The cache stores a single scalar tensor (~8 bytes) and invalidates naturally on device mismatch (line 79). P1 implies correctness/crash risk - this is at worst a minor memory inefficiency during rare device moves. Recommend downgrade to P3 and document existing `reset_cache()` usage pattern in training loop documentation.
