# Finding Ticket: load_state_dict Device Handling Documentation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-CR-07` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 5 |
| **Agent** | `codereview` |
| **Domain** | `simic/control` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/control/normalization.py` |
| **Line(s)** | `160-164` |
| **Function/Class** | `RunningMeanStd.load_state_dict()` |

---

## Summary

**One-line summary:** `load_state_dict` moves tensors to `self._device` which may not match saved device - undocumented.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

When loading state, tensors are moved to `self._device`:

```python
def load_state_dict(self, state: dict[str, torch.Tensor]) -> None:
    self.mean = state["mean"].to(self._device)
    self.var = state["var"].to(self._device)
    self.count = state["count"].to(self._device)
```

If state was saved from a GPU normalizer and loaded into a CPU-initialized normalizer, the tensors stay on CPU. This is probably correct behavior (move to current device), but could be surprising.

---

## Recommended Fix

Document that `load_state_dict` moves tensors to the normalizer's current device, not the saved device:

```python
def load_state_dict(self, state: dict[str, torch.Tensor]) -> None:
    """Restore statistics from a checkpoint.

    Note:
        Loaded tensors are moved to this normalizer's device, not the
        device they were saved from. Use .to(device) before loading
        if a specific target device is needed.
    """
    self.mean = state["mean"].to(self._device)
    ...
```

---

## Verification

### How to Verify the Fix

- [ ] Add docstring note about device behavior
- [ ] No functional change needed

---

## Related Findings

- B5-CR-04: Redundant _device field

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-codereview.md`
**Section:** "P3 - Code Quality" (ID 9)

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** The current behavior (moving tensors to `self._device`) is the correct PyTorch idiom for checkpointing. This matches standard PyTorch patterns like `model.load_state_dict()` where you load to CPU then call `.to(device)`. Documenting this explicitly is valuable because cross-device checkpoint loading is a common source of subtle bugs, especially when mixing CPU training with GPU inference or vice versa.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** RunningMeanStd is fundamental for observation normalization in RL (Schulman et al., 2017 recommend it for PPO). The device-move-on-load behavior is actually the correct pattern for checkpoint resumption across heterogeneous hardware. However, documenting this is important because RL practitioners frequently save checkpoints on GPU clusters and load on CPU for inference or debugging. The proposed docstring addition prevents confusion without changing the correct behavior.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** The device handling behavior is correct (move loaded tensors to the normalizer's current device), but undocumented. Per CLAUDE.md's legitimate uses list, "Device type normalization" is explicitly allowed - this is not defensive programming, it's proper checkpoint restoration. Adding the docstring note clarifying the device behavior is a valid P3 documentation improvement that prevents user confusion when loading checkpoints across devices.
