# Finding Ticket: Tuple Keys in Dicts May Cause torch.compile Graph Breaks

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B8-PT-06` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 8 |
| **Agent** | `pytorch` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/parallel_env_state.py` |
| **Line(s)** | `73-74, 131-137` |
| **Function/Class** | `ParallelEnvState` |

---

## Summary

**One-line summary:** Using `tuple[int, int]` as dictionary keys could cause graph breaks if accessed inside torch.compile regions.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [x] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Line 73-74
cf_pair_accums: dict[tuple[int, int], torch.Tensor] = field(default_factory=dict)

# Line 131-137 (in init_accumulators)
self.cf_pair_accums = {
    pair: torch.zeros(1, device=self.env_device)
    for pair in self.counterfactual_pairs
}
```

Using `tuple[int, int]` as dictionary keys:
1. Is fine for eager execution
2. Can cause graph breaks if accessed inside torch.compile regions
3. Dynamic key access patterns are hard for the compiler to trace

### Impact

- **Graph breaks**: Compilation may fail or fall back to eager mode
- **Performance**: Loss of compilation benefits in affected regions
- **Workaround needed**: May need @torch.compiler.disable on accessors

---

## Recommended Fix

**Option 1 - Use flat indices:**
```python
# Convert (i, j) pairs to flat indices
def pair_to_idx(i: int, j: int, n: int) -> int:
    return i * n + j

cf_pair_accums: dict[int, torch.Tensor] = field(default_factory=dict)
```

**Option 2 - Ensure access is outside compiled regions:**
Document that cf_pair_accums should not be accessed inside torch.compile scope.

---

## Verification

### How to Verify the Fix

- [ ] Profile with TORCH_LOGS=graph_breaks to detect issues
- [ ] Either flatten keys or ensure access is outside compiled regions
- [ ] Add test for torch.compile compatibility

---

## Related Findings

- B8-PT-05: Fragile locals() check (also torch.compile concern)
- B8-CR-02: Repeated cast() in hot path (also compilation concern)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch8-pytorch.md`
**Section:** "P3 - cf_pair_accums uses tuple[int, int] keys"
