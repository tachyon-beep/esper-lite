# Finding Ticket: Per-Batch env_states Recreation Causes Memory Churn

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B8-PT-04` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 8 |
| **Agent** | `pytorch` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | `1700` |
| **Function/Class** | `train_ppo_vectorized()` |

---

## Summary

**One-line summary:** Each batch creates new `env_states` with models, optimizers, and CUDA streams that are never explicitly freed.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [x] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Line 1700
env_states = [create_env_state(i, base_seed) for i in range(envs_this_batch)]
# ... entire batch processes ...
# env_states goes out of scope - Python GC will eventually clean up
```

Each batch creates:
- N model instances (large GPU memory)
- N optimizer instances (GPU memory for momentum buffers)
- N CUDA streams
- N GradScaler instances

While Python GC will eventually clean these up, the lack of explicit cleanup means:
1. Memory fragmentation on GPU
2. Peak memory higher than necessary
3. Potential OOM if batches are large

### Impact

- **Memory fragmentation**: Allocation patterns vary batch-to-batch
- **Peak memory**: Higher than if objects were reused or explicitly freed
- **GC pressure**: Large object graphs take time to collect

---

## Recommended Fix

Add explicit cleanup at batch end:

```python
# After batch completes:
for env_state in env_states:
    # Clear CUDA resources
    if env_state.stream is not None:
        env_state.stream.synchronize()
    # Clear optimizers
    env_state.host_optimizer = None
    env_state.seed_optimizers.clear()
    # Clear model reference
    env_state.model = None

del env_states
torch.cuda.empty_cache()  # Optional: defragment GPU memory
```

Or consider pooling env_states across batches.

---

## Verification

### How to Verify the Fix

- [ ] Add explicit cleanup at batch end
- [ ] Profile memory usage across batches
- [ ] Compare peak memory with/without cleanup
- [ ] Consider env_state pooling for reuse

---

## Related Findings

- B8-DRL-06: GradScaler divergence on same GPU (related to per-env allocation)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch8-pytorch.md`
**Section:** "P2 - env_states created per batch but never explicitly freed"
