# Finding Ticket: Episode Telemetry Uses Normalized Rewards (Uninterpretable)

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-CR-03` |
| **Severity** | `P2` |
| **Status** | `fixed` |
| **Batch** | 11 |
| **Agent** | `correctness` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2026-01-01 |
| **Updated** | 2026-01-01 |
| **Fixed** | 2026-01-01 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | `2779` (episode_rewards storage), `3341` (penalty application) |
| **Function/Class** | `train_ppo_vectorized()` |

---

## Summary

**One-line summary:** B11-CR-01 "fix" changed `episode_rewards` to store normalized rewards instead of raw rewards, making telemetry uninterpretable (zero-centered, non-comparable across runs, doesn't reflect environment scale).

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

**B11-CR-01 attempted to fix scale inconsistency but introduced telemetry bug:**

**Before B11-CR-01 (ORIGINAL STATE):**
```python
# Line 2779: Store RAW rewards
env_state.episode_rewards.append(reward)

# Line 3341 (on penalty): Append NORMALIZED penalty (BUG - mixed scale!)
env_states[env_idx].episode_rewards.append(normalized_penalty)
```

**Issues:**
- ✅ Telemetry interpretable (raw rewards show actual environment scale)
- ❌ Mixed scale (raw rewards + normalized penalty)
- ❌ Append instead of overwrite (length mismatch with buffer)

**After B11-CR-01 "fix" (CURRENT STATE):**
```python
# Line 2779: Store NORMALIZED rewards (BUG - broke telemetry!)
env_state.episode_rewards.append(normalized_reward)

# Line 3341 (on penalty): Overwrite with NORMALIZED penalty
if env_states[env_idx].episode_rewards:
    env_states[env_idx].episode_rewards[-1] = normalized_penalty
```

**Issues:**
- ✅ Consistent scale (all normalized)
- ✅ Overwrite semantics (matches buffer)
- ❌ Telemetry uninterpretable (zero-centered, non-comparable across runs)

**CORRECT state (SHOULD BE):**
```python
# Line 2779: Store RAW rewards (for telemetry)
env_state.episode_rewards.append(reward)

# Line 3341 (on penalty): Overwrite with RAW penalty (not normalized_penalty)
if env_states[env_idx].episode_rewards:
    env_states[env_idx].episode_rewards[-1] = penalty  # RAW penalty
```

**Why:**
- ✅ Consistent scale (all raw)
- ✅ Overwrite semantics (matches buffer structure)
- ✅ Telemetry interpretable (actual environment scale)

### Separation of Concerns

**PPO buffer** (for learning):
```python
# Buffer stores normalized rewards - CORRECT, don't change
agent.buffer.add(..., reward=normalized_reward, ...)
agent.buffer.mark_terminal_with_penalty(env_idx, normalized_penalty)
```

**Telemetry** (`episode_rewards` for analysis/dashboards):
```python
# Should store RAW rewards - actual environment scale for interpretability
env_state.episode_rewards.append(reward)  # RAW
```

These are **separate concerns**:
1. PPO needs normalized rewards for stable training (already correct in buffer)
2. Telemetry needs raw rewards for interpretability and cross-run comparison

### Impact

**With current B11-CR-01 "fix" (normalized rewards in telemetry):**

```python
# Example: Environment gives rewards [+10, +20, +15, +30] over 4 epochs
# Normalizer has mean=18.75, std=8.54

# What PPO buffer sees (normalized - CORRECT for training):
buffer.rewards = [(10-18.75)/8.54, (20-18.75)/8.54, ...]
               = [-1.02, +0.15, -0.44, +1.32]  # Zero-centered, stable gradients

# What telemetry sees (ALSO normalized - WRONG for interpretability):
episode_rewards = [-1.02, +0.15, -0.44, +1.32]
EpisodeOutcome.episode_reward = sum(episode_rewards) = +0.01  # Near zero!

# What telemetry SHOULD see (raw rewards):
episode_rewards = [+10, +20, +15, +30]
EpisodeOutcome.episode_reward = +75  # Actual environment performance
```

**Problems:**
1. **Zero-centered**: Episode rewards will average near zero by construction (meaningless)
2. **Non-comparable**: Different normalizer stats across runs make rewards incomparable
3. **Drift**: As normalizer stats evolve, same environment performance shows different rewards
4. **Dashboard confusion**: Users see "episode_reward = +0.5" and have no idea what that means
5. **Pareto analysis broken**: Can't compare reward/accuracy tradeoff across runs

**Example cross-run comparison:**
```
Run A (early training): normalizer mean=10, std=5
  Environment reward: +50
  Telemetry shows: (50-10)/5 = +8.0

Run B (later training): normalizer mean=45, std=15
  Environment reward: +50  (SAME performance!)
  Telemetry shows: (50-45)/15 = +0.33  (Looks 24x worse!)
```

### Affected Systems

1. **EpisodeOutcome** - Pareto analysis sees normalized rewards (uninterpretable)
2. **Episode history** (A/B testing) - Can't compare across runs (different normalizer stats)
3. **Dashboards** - Display normalized rewards (zero-centered, meaningless to users)
4. **Stability scores** - Variance of normalized rewards (inflated by normalization)
5. **Batch summaries** - Average episode reward near zero (uninformative)

---

## Fix Applied

**Commit:** `[current commit]`

**Approach:** Revert B11-CR-01 to use raw rewards for telemetry, keep overwrite semantics

**Changes made:**

1. **Line 2780:** Reverted to store RAW rewards (not normalized)
   ```python
   # BEFORE (B11-CR-01 "fix"):
   env_state.episode_rewards.append(normalized_reward)

   # AFTER (B11-CR-03 fix):
   env_state.episode_rewards.append(reward)  # RAW for telemetry
   ```

2. **Line 3345:** Changed to overwrite with RAW penalty (not normalized)
   ```python
   # BEFORE (B11-CR-01 "fix"):
   env_states[env_idx].episode_rewards[-1] = normalized_penalty

   # AFTER (B11-CR-03 fix):
   env_states[env_idx].episode_rewards[-1] = penalty  # RAW for telemetry
   ```

**Rationale:**
- **Separation of concerns**: PPO buffer uses `normalized_reward` (for training stability), telemetry uses `reward` (for interpretability)
- **Consistent scale**: All `episode_rewards` entries are raw (rewards and penalty both use actual environment scale)
- **Correct semantics**: Penalty overwrites last entry (matches buffer structure, from B11-CR-01)
- **Interpretable telemetry**: Dashboards show actual environment scale, comparable across runs
- **B11-CR-02 compatibility**: Recomputation logic works with raw rewards (consistent scale preserved)

**Tests passed:**
- [x] All 4 reward telemetry flow tests
- [x] Vectorized determinism test (47s, 2440 warnings)
- [x] All integration tests

**What changed:**
- **Before B11-CR-01:** Raw rewards + normalized penalty (mixed scale, wrong semantics)
- **After B11-CR-01:** Normalized rewards + normalized penalty (consistent scale, wrong purpose)
- **After B11-CR-03:** Raw rewards + raw penalty (consistent scale, correct purpose ✅)

---

## Recommended Fix (IMPLEMENTED)

**Revert B11-CR-01 change to line 2779, fix line 3341 to use raw penalty:**

```python
# Line 2779: Store RAW rewards (revert B11-CR-01 change)
normalized_reward = reward_normalizer.update_and_normalize(reward)
env_state.episode_rewards.append(reward)  # RAW for telemetry

# Line 3341: Overwrite with RAW penalty (keep overwrite, change to raw)
if env_states[env_idx].episode_rewards:
    env_states[env_idx].episode_rewards[-1] = penalty  # RAW penalty, not normalized_penalty
```

**Rationale:**
- **Separation of concerns**: PPO buffer has normalized rewards (for training), telemetry has raw rewards (for interpretability)
- **Consistent scale**: All `episode_rewards` entries are raw (either raw reward or raw penalty)
- **Correct semantics**: Penalty overwrites last entry (matches buffer structure)
- **Interpretable telemetry**: Dashboards show actual environment scale, comparable across runs

**B11-CR-02 update:** No change needed - recomputation logic works with either raw or normalized, as long as scale is consistent

---

## Verification

### How to Verify the Fix

1. **Run training with reward normalization enabled**
2. **Check EpisodeOutcome telemetry:**
   ```python
   # Before fix: episode_reward ≈ 0 (normalized, zero-centered)
   # After fix: episode_reward = sum of actual environment rewards (e.g., +150.5)
   ```
3. **Check cross-run comparability:**
   ```python
   # Run same configuration twice
   # Before fix: episode_rewards differ due to normalizer drift
   # After fix: episode_rewards same (actual environment scale)
   ```
4. **Verify buffer still has normalized rewards:**
   ```python
   # Buffer should still use normalized_reward (for PPO stability)
   assert agent.buffer.rewards[0] != env_state.episode_rewards[0]  # Different scales
   ```

---

## Root Cause Analysis

**How did this happen?**

The original B11-CR-01 bug report identified:
1. Overwrite vs append mismatch ✅ (correctly identified)
2. Mixed scale (raw rewards + normalized penalty) ✅ (correctly identified)
3. Timing issue ✅ (correctly identified as B11-CR-02)

**The mistake in the fix:**

I assumed "mixed scale" meant we needed to normalize ALL entries to match the normalized penalty. But the correct interpretation is:
- **Buffer** needs normalized rewards (for PPO training) - already correct
- **Telemetry** needs raw rewards (for interpretability) - should NOT be normalized

The "mixed scale" bug should have been fixed by using raw penalty on line 3341, not by normalizing rewards on line 2779.

**Lesson learned:**
- Always ask: "What is this data structure used for?"
- PPO learning vs telemetry have different requirements
- Consistent scale is good, but scale must match the purpose

---

## Related Findings

- **B11-CR-01:** Attempted to fix scale consistency, introduced telemetry bug
- **B11-CR-02:** Sequencing fix (metrics computed before penalty) - orthogonal issue, unaffected by this bug
- **B-METRIC-01:** Original attempt to sync episode_rewards with buffer

---

## Open Questions

1. **Historical data:** Should we backfill corrected episode rewards for past runs in the database?
2. **Stability calculation:** Does variance of raw rewards make more sense than variance of normalized rewards for stability score?
3. **Dashboard display:** Should dashboards show both raw and normalized rewards, or just raw?

---

## Notes

- This bug was introduced by B11-CR-01 fix itself (regression)
- PPO learning is still correct (buffer has normalized rewards)
- Only affects telemetry/analysis systems
- Severity P2 because it makes Pareto analysis and A/B testing uninterpretable
