# Bug Ticket: AMP Breaks Gradient Flow to Action Heads

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-PT-01` |
| **Severity** | `P0` |
| **Status** | `open` |
| **Batch** | 11 |
| **Agent** | `pytorch` |
| **Domain** | `simic/agent`, `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2026-01-02 |
| **Updated** | 2026-01-02 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `src/esper/tamiyo/policy/action_masks.py`, `src/esper/simic/training/vectorized.py` |
| **Line(s)** | `action_masks.py:497-498`, `vectorized.py:366-369` |
| **Function/Class** | `MaskedCategorical.__init__()`, vectorized training loop |

---

## Summary

**One-line summary:** Under AMP (Automatic Mixed Precision), gradient flow to all action heads is completely broken, causing all head gradient norms to be NaN in Sanctum UX.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [x] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Reproduction

**Command:**
```bash
uv run python -m esper.scripts.train ppo \
    --devices cuda:0 cuda:1 \
    --telemetry-dir ./telemetry \
    --sanctum \
    --telemetry-level debug \
    --config-json config.json \
    --gpu-preload \
    --rounds 2000 \
    --envs 64 \
    --experimental-gpu-preload-gather \
    --task cifar_impaired
```

**Expected:** After Tamiyo's first episode, Sanctum UX shows average gradient norms for all action heads (finite values).

**Actual:** All action head gradient averages show `NaN` in Sanctum UX.

---

## Root Cause Analysis

### The Problem

When training runs under AMP (`torch.amp.autocast`), the network forward pass outputs logits in `float16`. These float16 logits are passed directly to `MaskedCategorical`, which creates a `Categorical` distribution and computes `log_prob` in float16.

The log-softmax operation inside `Categorical.log_prob()` is numerically unstable in float16:
- Small differences in logits become indistinguishable
- The gradient path through log-softmax can produce NaN/Inf
- Backward propagation fails, leaving head parameters with `grad = None`

### Evidence

Debug probes show:
```
Compiled network params with grad: 9/49
slot_head: None=4, NaN/Inf=0, Finite=0 (of 4 params)
op_head: None=4, NaN/Inf=0, Finite=0 (of 4 params)
value_head: None=4, NaN/Inf=0, Finite=0 (of 4 params)
```

All head parameters have `grad = None` after `loss.backward()` under AMP. The 9 parameters with gradients are from the feature network and LSTM (which don't go through `log_prob`).

### Code Path

1. `vectorized.py:366-369`: `agent.update()` runs inside `torch.amp.autocast(device_type="cuda", dtype=dtype)`
2. Network forward produces float16 logits
3. `factored_lstm.py:891`: `MaskedCategorical(logits=logits_flat, mask=mask_flat)`
4. `action_masks.py:497-498`: `self.masked_logits = logits.masked_fill(~mask, MASKED_LOGIT_VALUE)` stays float16
5. `Categorical(logits=self.masked_logits)` operates in float16
6. `log_prob()` computes log-softmax in float16 - gradients break

---

## Proposed Fix

Cast logits to float32 before creating the Categorical distribution:

```python
# In MaskedCategorical.__init__():
# CRITICAL: Upcast to float32 for AMP stability
# log_softmax in float16 causes gradient flow to break
logits_f32 = logits.float()
self.masked_logits = logits_f32.masked_fill(~mask, MASKED_LOGIT_VALUE)
self._dist = Categorical(logits=self.masked_logits)
```

This ensures loss computations happen in float32 (PyTorch AMP best practice) while the forward pass still benefits from float16 speed.

---

## Regression Test

A regression test should:
1. Create a PPO agent with compiled network
2. Run `agent.update()` inside `torch.amp.autocast(dtype=torch.float16)`
3. Assert that all head gradient norms in the returned metrics are finite (not NaN)

Test location: `tests/simic/test_ppo_amp_gradients.py`

---

## Impact

- **Severity:** P0 - Training telemetry is completely broken under AMP (default for CUDA)
- **Scope:** All CUDA training runs with AMP enabled
- **User Impact:** Cannot monitor gradient health during training; no visibility into head learning
- **Data Impact:** Telemetry data contains NaN values, degrading analysis capabilities

---

## References

- PyTorch AMP documentation recommends float32 for loss computation
- Similar issues: https://discuss.pytorch.org/t/nan-gradients-with-amp/
- Related ticket: `B10-PT-01` (inference_mode tensors)
