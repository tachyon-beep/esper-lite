# Finding Ticket: LSTM Hidden State Reset Before Bootstrap Computation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-DRL-01` |
| **Severity** | `P0` |
| **Status** | `fixed` |
| **Batch** | 11 |
| **Agent** | `drl` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2026-01-01 |
| **Updated** | 2026-01-01 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | `3117-3132` (pre-fix), `3301-3307` |
| **Function/Class** | `train_ppo_vectorized()` |

---

## Summary

**One-line summary:** LSTM hidden state was reset to initial_hidden() when `done=True`, but then used for bootstrap V(s_{t+1}) computation, biasing GAE for truncated episodes.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

**Execution flow (PRE-FIX):**
```
1. epoch == max_epochs → done=True, truncated=True (line 3074-3075)
2. done=True triggers hidden state reset to initial_hidden() (line 3117-3132)
3. Truncated state captured for bootstrap (line 3148-3169)
4. Bootstrap V(s_{t+1}) computed using RESET hidden state (line 3301-3307) ← BUG
```

**Why this is wrong:**

For LSTM policies, `V(s, h) = f(observation, hidden_state)` where `h` encodes episode history.

The bootstrap value `V(s_{t+1})` for truncated episodes should use:
- **Correct:** The carried episode hidden state (representing accumulated episode memory)
- **Actual:** A fresh/zero hidden state (representing no history)

This biases the GAE computation for the final step of every truncated episode, since:
```python
A_t = r_t + γ * V(s_{t+1}) - V(s_t)
```

If `V(s_{t+1})` is computed with a "memory-wiped" agent, the advantage estimate is incorrect.

**Why the reset existed:**

Line 3121 comment said "Reset this environment's hidden state in the batch for the next episode."

But there IS no next episode:
- Each batch runs exactly ONE episode per environment (line 1741-1743)
- Episode ends when `epoch == max_epochs`
- Bootstrap computation happens AFTER the epoch loop
- Next rollout (next batch) reinitializes hidden states anyway (line 1747)

### Impact

- **Training correctness**: Biased advantage estimates for final step of truncated episodes
- **LSTM policies only**: Non-recurrent policies unaffected
- **Silent**: No error, just incorrect gradients flowing through value network
- **Every episode**: All episodes hit `epoch == max_epochs` and are truncated

---

## Fix Applied

**Commit:** `[current commit]`

**Change:** Removed the hidden state reset entirely.

```python
# OLD (line 3117-3132)
if done:
    agent.buffer.end_episode(env_id=env_idx)
    if batched_lstm_hidden is not None:
        # Reset this environment's hidden state in the batch for the next episode.
        init_hidden = agent.policy.initial_hidden(1)
        # ... clone and replace logic ...
        batched_lstm_hidden = (new_h, new_c)

# NEW (line 3117-3123)
if done:
    agent.buffer.end_episode(env_id=env_idx)
    # NOTE: Do NOT reset batched_lstm_hidden here. The bootstrap value computation
    # (after the epoch loop) requires the carried episode hidden state to correctly
    # estimate V(s_{t+1}) for truncated episodes. Resetting to initial_hidden() would
    # bias the GAE computation by computing V(s_{t+1}) with a "memory-wiped" agent.
    # The next rollout will initialize fresh hidden states anyway (line 1747).
```

**Rationale:**
1. **No functional need:** Next rollout reinitializes anyway
2. **Semantic correctness:** Hidden state should persist until after bootstrap
3. **Minimal change:** No additional state tracking required

---

## Verification

### Tests Passed

- [x] `tests/integration/test_vectorized_determinism.py::TestVectorizedDeterminism::test_end_to_end_determinism`
- [x] All PPO/LSTM unit tests (`tests/simic/test_ppo.py`, `tests/simic/test_tamiyo_*.py`)
- [x] All PPO/reward integration tests

### Semantic Question Resolved

**User asked:** "Should `max_epochs` be terminal vs truncation?"

**Current semantics (CORRECT):**
```python
done = epoch == max_epochs  # Episode ends
truncated = done            # But it's a time-limit truncation, not natural terminal
```

This means: "The episode is cut short due to time limits, but the task continues. Bootstrap V(s_{t+1}) to estimate future return."

**Recommendation:** Keep current semantics. `max_epochs` is a computational limit, not a task-defined terminal state. The fix ensures the implementation matches the intended semantics.

---

## Related Findings

- B11-CR-01: Death-penalty bookkeeping inconsistency (metrics issue, not RL correctness)
- B11-DRL-02: Adaptive entropy floor inert (architectural jank, but not harmful)

---

## Appendix

### Key Insight: LSTM Policies and Bootstrap

For recurrent policies, the value function depends on hidden state:
```
V(s_t, h_t) where h_t = LSTM(s_0, ..., s_t)
```

Bootstrap computation for truncated episodes requires:
```
V(s_{T+1}, h_{T+1}) where h_{T+1} = LSTM(s_0, ..., s_T, s_{T+1})
```

If we reset `h` to zeros before computing `V(s_{T+1})`, we get:
```
V(s_{T+1}, h_0)  ← WRONG! This is not the agent's actual value estimate
```

This is analogous to asking "what would the agent think of this state if it forgot everything?"
