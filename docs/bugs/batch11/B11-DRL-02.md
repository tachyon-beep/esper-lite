# Finding Ticket: Adaptive Entropy Floor Inert Due to Missing Mask Threading

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-DRL-02` |
| **Severity** | `P3` |
| **Status** | `fixed` |
| **Batch** | 11 |
| **Agent** | `drl` |
| **Domain** | `simic/agent` |
| **Assignee** | |
| **Created** | 2026-01-01 |
| **Updated** | 2026-01-01 |
| **Fixed** | 2026-01-01 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/agent/ppo.py`, `/home/john/esper-lite/src/esper/tamiyo/policy/action_masks.py` |
| **Line(s)** | `321-322, 756` (ppo.py), `450-480` (action_masks.py) |
| **Function/Class** | `PPOAgent.get_entropy_floor()`, `PPOAgent.update()`, `MaskedCategorical.entropy()` |

---

## Summary

**One-line summary:** `adaptive_entropy_floor` feature is inert because `get_entropy_coef()` is called without mask info, but the feature is likely redundant anyway since entropy is already normalized by log(num_valid).

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [x] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

**The adaptive entropy floor feature has two issues:**

#### 1. Not Wired: No Mask Threading

**get_entropy_floor() signature (ppo.py:322):**
```python
def get_entropy_floor(self, action_mask: torch.Tensor | None = None) -> float:
    """Get entropy floor, optionally scaled by valid action count."""
    # If adaptive and mask provided, scale by log(num_total) / log(num_valid)
    # ...
```

**get_entropy_coef() signature (ppo.py:313):**
```python
def get_entropy_coef(self, action_mask: torch.Tensor | None = None) -> float:
    # ...
    floor = self.get_entropy_floor(action_mask)
    return max(annealed, floor)
```

**Caller in update() (ppo.py:756):**
```python
entropy_coef = self.get_entropy_coef()  # NO MASK PASSED
```

**Result:** `get_entropy_floor()` always receives `action_mask=None`, so the adaptive scaling is never activated.

**TODO marker (ppo.py:321):**
```python
# TODO: potential dead code - action_mask is not currently threaded through callers.
def get_entropy_floor(self, action_mask: torch.Tensor | None = None) -> float:
```

#### 2. Likely Redundant Anyway

**Entropy is already normalized in MaskedCategorical (action_masks.py:450-480):**

```python
def entropy(self) -> torch.Tensor:
    """Compute normalized entropy over valid actions.

    Returns entropy normalized to [0, 1] by dividing by max entropy
    (log of number of valid actions). This makes exploration incentives
    comparable across states with different action restrictions.
    """
    probs = self._dist.probs
    log_probs = self._dist.logits - self._dist.logits.logsumexp(dim=-1, keepdim=True)
    raw_entropy = -(probs * log_probs * self.mask).sum(dim=-1)
    num_valid = self.mask.sum(dim=-1).clamp(min=1)
    max_entropy = torch.log(num_valid.float())

    normalized_entropy = torch.where(
        num_valid > 1,
        raw_entropy / safe_max_entropy,
        torch.zeros_like(raw_entropy)
    )
    return normalized_entropy
```

**What this means:**
- Entropy is already scaled to [0, 1] relative to the number of valid actions
- A state with 2 valid actions and a state with 10 valid actions both have max entropy = 1.0
- The entropy coefficient is already being applied to normalized entropy

**What adaptive_entropy_floor would do if wired:**
```python
scale_factor = log(num_total) / log(num_valid)
floor = base_floor * scale_factor
```

This would INCREASE the floor when fewer actions are valid, which would INCREASE exploration in heavily-masked states.

**But:**
- Since entropy is already normalized by log(num_valid), the "relative exploration" is already constant across different mask densities
- Scaling the coefficient by mask density would over-amplify exploration in heavily-masked states
- Example: With 2 valid actions, scale_factor = log(8)/log(2) = 3, tripling the floor
- This might cause excessive exploration when action space is heavily restricted

### Impact

**Current behavior:**
- `adaptive_entropy_floor=True` does nothing (mask not threaded)
- Entropy coefficient is constant regardless of action mask density
- Entropy is normalized, so exploration incentive is comparable across different mask densities

**If wired correctly:**
- Entropy floor would scale up in heavily-masked states
- This might cause over-exploration when action space is restricted
- Unclear if this is desirable

**Classification:** Architectural jank / dead code, but not harmful in current form.

---

## Fix Applied

**Commit:** `[current commit]`

**Approach:** Removed adaptive entropy floor feature entirely (Option 2 from recommendations)

**Changes made:**

1. **Deleted `get_entropy_floor()` method** (ppo.py:320-357) - 38 lines removed
2. **Simplified `get_entropy_coef()`** (ppo.py:296-318) - Inlined base floor
3. **Removed `adaptive_entropy_floor` parameter** from:
   - `PPOHyperparameters` dataclass (config.py:76)
   - `PPOAgent.__init__()` (ppo.py:113)
   - `train_ppo_vectorized()` (vectorized.py:521)
   - Checkpoint save/load (ppo.py:893)
   - Config conversion methods (config.py:307, 328)
   - LSTM_EXPERIMENTAL preset (config.py:175)
   - Test assertions (test_config.py:63)

**Rationale:**

1. **Theoretically redundant:** MaskedCategorical already normalizes entropy by `log(num_valid)`, making adaptive scaling redundant
2. **Potentially harmful:** If wired, would over-amplify exploration in masked states (3x higher floor) without empirical validation
3. **Code quality:** Removes 38 lines of unused code, confusing dead parameter
4. **No behavior change:** Feature was inert (never activated), so removal preserves current behavior

**New `get_entropy_coef()` implementation:**

```python
def get_entropy_coef(self, action_mask: torch.Tensor | None = None) -> float:
    """Get current entropy coefficient with annealing and floor.

    Note:
        The adaptive_entropy_floor feature was removed (B11-DRL-02). It scaled
        the floor by mask density, but MaskedCategorical already normalizes
        entropy by log(num_valid), making the adaptive scaling redundant and
        potentially harmful (over-exploration in masked states).
    """
    if self.entropy_anneal_steps == 0:
        return max(self.entropy_coef, self.entropy_coef_min)

    progress = min(1.0, self.train_steps / self.entropy_anneal_steps)
    annealed = self.entropy_coef_start + progress * (self.entropy_coef_end - self.entropy_coef_start)
    return max(annealed, self.entropy_coef_min)
```

**Tests passed:**
- [x] `tests/simic/test_config.py` (all 18 tests)
- [x] `tests/simic/test_ppo.py` (all 16 tests)

**Git history preserved:** If the hypothesis "adaptive floor helps" is later validated, the implementation can be restored from git history with proper A/B testing.

---

## Recommended Fix (IMPLEMENTED: Option 2)

**Option 1: Document as intentionally inert (RECOMMENDED)**

```python
# ppo.py:321-322
def get_entropy_floor(self, action_mask: torch.Tensor | None = None) -> float:
    """Get entropy floor (adaptive scaling currently disabled).

    NOTE: The adaptive_entropy_floor feature scales the floor by mask density,
    but this is likely redundant since MaskedCategorical already normalizes
    entropy by log(num_valid). Scaling the coefficient would cause
    over-exploration in heavily-masked states.

    Current behavior: Returns base_entropy_floor regardless of mask density.
    This is correct given that entropy is pre-normalized.
    """
    # Intentionally ignore action_mask - see docstring
    return self.base_entropy_floor
```

**Pros:**
- Acknowledges the current state
- Explains the reasoning
- No behavior change

**Cons:**
- Leaves dead parameter (`adaptive_entropy_floor`) in config

**Option 2: Remove adaptive_entropy_floor entirely**

```python
# Remove from PPOHyperparameters:
# - adaptive_entropy_floor field
# - get_entropy_floor() method (inline base_entropy_floor)

# ppo.py:313
def get_entropy_coef(self, action_mask: torch.Tensor | None = None) -> float:
    annealed = self.base_entropy_coef * (1.0 - self.get_entropy_annealing_factor())
    return max(annealed, self.base_entropy_floor)  # Direct access
```

**Pros:**
- Clean removal of unused functionality
- No confusing dead parameters

**Cons:**
- Breaking change if anyone set `adaptive_entropy_floor=True` (but it did nothing anyway)

**Option 3: Wire it correctly and evaluate**

```python
# Thread masks through update():
def update(self, ...):
    # ...
    entropy_coef = self.get_entropy_coef(action_mask=head_masks)  # Pass masks
```

**Pros:**
- Feature actually works

**Cons:**
- Requires testing to see if it helps or hurts
- May cause over-exploration in masked states
- Adds complexity for unclear benefit

---

## Verification

### How to Verify the Fix

**Option 1 (Document):**
- [ ] Update docstring to explain why adaptive scaling is disabled
- [ ] No behavior change, no tests needed

**Option 2 (Remove):**
- [ ] Remove `adaptive_entropy_floor` from config
- [ ] Remove `get_entropy_floor()` method
- [ ] Inline `base_entropy_floor` in `get_entropy_coef()`
- [ ] Verify all existing tests still pass

**Option 3 (Wire):**
- [ ] Thread masks through `update()` to `get_entropy_coef()`
- [ ] Add test that entropy coefficient scales with mask density when `adaptive_entropy_floor=True`
- [ ] Run A/B test to compare learning curves with/without adaptive scaling
- [ ] Verify no over-exploration in heavily-masked states

---

## Related Findings

- B11-DRL-01: LSTM bootstrap bug (FIXED, unrelated)
- B11-CR-01: Death-penalty bookkeeping (metrics issue, unrelated)

---

## Appendix

### Example: Entropy Normalization Makes Adaptive Floor Redundant

**Scenario:** State A has 8 valid actions, State B has 2 valid actions.

**Unnormalized entropy (typical PPO):**
```
State A: max_entropy = log(8) ≈ 2.08
State B: max_entropy = log(2) ≈ 0.69

Uniform distribution:
  State A: H = 2.08 (max)
  State B: H = 0.69 (max)

With same entropy coefficient (0.01):
  State A: exploration_bonus = 0.01 * 2.08 = 0.021
  State B: exploration_bonus = 0.01 * 0.69 = 0.007

→ State A gets 3x more exploration incentive!
```

**Normalized entropy (Esper's MaskedCategorical):**
```
State A: normalized_H = 2.08 / log(8) = 1.0 (max)
State B: normalized_H = 0.69 / log(2) = 1.0 (max)

With same entropy coefficient (0.05):
  State A: exploration_bonus = 0.05 * 1.0 = 0.05
  State B: exploration_bonus = 0.05 * 1.0 = 0.05

→ Equal exploration incentive!
```

**With adaptive floor (if wired):**
```
State A: scale_factor = log(8) / log(8) = 1.0
  floor = base_floor * 1.0

State B: scale_factor = log(8) / log(2) = 3.0
  floor = base_floor * 3.0

→ State B would get 3x HIGHER floor, causing more exploration when restricted!
```

This might be desirable for some problems (encourage exploration when choices are limited), but it's unclear if it helps in Esper's domain. Without empirical validation, it's safer to keep it disabled.

### Historical Context

The `adaptive_entropy_floor` feature was likely added with the intention of scaling exploration based on action availability, similar to how some papers suggest scaling learning rates by gradient magnitudes. However:

1. The normalization already handles the "relative exploration" concern
2. The feature was never wired (mask not threaded)
3. No empirical evidence it helps in this domain

Result: Dead code that can be documented or removed.
