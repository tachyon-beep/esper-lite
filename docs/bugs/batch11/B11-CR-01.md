# Finding Ticket: Death-Penalty Bookkeeping Inconsistency

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-CR-01` |
| **Severity** | `P2` |
| **Status** | `fixed` |
| **Batch** | 11 |
| **Agent** | `code-review` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2026-01-01 |
| **Updated** | 2026-01-01 |
| **Fixed** | 2026-01-01 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py`, `/home/john/esper-lite/src/esper/simic/agent/rollout_buffer.py` |
| **Line(s)** | `2777-2778, 3185-3194, 3336-3341` (vectorized.py), `552-581` (rollout_buffer.py) |
| **Function/Class** | `train_ppo_vectorized()`, `TamiyoRolloutBuffer.mark_terminal_with_penalty()` |

---

## Summary

**One-line summary:** Death penalty bookkeeping is inconsistent: buffer overwrites last reward, but `episode_rewards` appends normalized penalty, creating mixed-scale list and length mismatch.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

**Three inconsistencies in death-penalty handling:**

#### 1. Overwrite vs Append Mismatch

**Buffer behavior (rollout_buffer.py:578):**
```python
def mark_terminal_with_penalty(self, env_id: int, penalty: float) -> bool:
    # ...
    last_idx = step_count - 1
    self.rewards[env_id, last_idx] = penalty  # OVERWRITES last reward
    self.dones[env_id, last_idx] = True
    return True
```

**Telemetry behavior (vectorized.py:3341):**
```python
# B-METRIC-01 fix: Reflect penalty in episode_rewards so metrics
# (EpisodeOutcome, A/B history, stability) match what PPO learned.
env_states[env_idx].episode_rewards.append(normalized_penalty)  # APPENDS
```

**Result:** If an episode has N steps, the buffer has N rewards (with the last one overwritten), but `episode_rewards` has N+1 entries (N original + 1 appended penalty).

#### 2. Mixed Scale: Raw + Normalized

**During normal rollout (vectorized.py:2777-2778):**
```python
normalized_reward = reward_normalizer.update_and_normalize(reward)
env_state.episode_rewards.append(reward)  # Stores RAW reward
```

**During governor rollback (vectorized.py:3337-3341):**
```python
penalty = env_states[env_idx].governor.get_punishment_reward()
normalized_penalty = reward_normalizer.normalize_only(penalty)
# ...
env_states[env_idx].episode_rewards.append(normalized_penalty)  # Stores NORMALIZED penalty
```

**Result:** `episode_rewards` contains:
- Steps 1..N: Raw rewards (e.g., -0.05, 0.12, -0.03)
- Final penalty: Normalized reward (e.g., -2.3)

This makes aggregates like `sum(episode_rewards)` nonsensical.

#### 3. Timing: Metrics Computed Before Penalty Applied

**EpisodeOutcome creation (vectorized.py:3185-3194):**
```python
# Inside the epoch loop, when episode completes naturally
recent_ep_rewards = env_state.episode_rewards[-20:]
reward_var = float(np.var(recent_ep_rewards))
stability = 1.0 / (1.0 + reward_var)

episode_outcome = EpisodeOutcome(
    episode_reward=env_total_rewards[env_idx],
    final_accuracy=env_final_accs[env_idx],
    stability_score=stability,
)
```

**Death penalty applied later (vectorized.py:3336-3341):**
```python
# After the epoch loop, during governor rollback
penalty = env_states[env_idx].governor.get_punishment_reward()
normalized_penalty = reward_normalizer.normalize_only(penalty)
agent.buffer.mark_terminal_with_penalty(env_idx, normalized_penalty)
env_states[env_idx].episode_rewards.append(normalized_penalty)
```

**Result:** `EpisodeOutcome` and `stability_score` are computed using `episode_rewards` BEFORE the penalty is appended. But then the penalty is appended to `episode_rewards`, so subsequent episodes' stability scores include the penalty from previous episodes.

### Impact

**NOT an RL correctness issue:**
- PPO learns from the buffer, which correctly has the overwritten penalty
- GAE computation uses the buffer, not `episode_rewards`
- The bug is confined to telemetry/metrics

**IS a telemetry correctness issue:**
- `episode_rewards` list is malformed (mixed scale, wrong length)
- Stability scores are incorrect (use stale `episode_rewards` for current episode, but include previous episode's penalty for future episodes)
- A/B testing reward comparisons may be misleading
- Episode reward summaries don't reflect actual learned rewards

**Classification:** Metrics/telemetry correctness bug, not RL learning bug.

---

## Fix Applied

**Commit:** `[current commit]`

**Approach:** Implemented Option 1 (episode_rewards mirrors buffer semantics)

**Changes made:**

1. **Line 2779:** Changed `episode_rewards` to store normalized rewards instead of raw rewards
   ```python
   # OLD: env_state.episode_rewards.append(reward)
   # NEW: env_state.episode_rewards.append(normalized_reward)
   ```

2. **Lines 3342-3343:** Changed penalty application to OVERWRITE last entry instead of appending
   ```python
   # OLD: env_states[env_idx].episode_rewards.append(normalized_penalty)
   # NEW: if env_states[env_idx].episode_rewards:
   #          env_states[env_idx].episode_rewards[-1] = normalized_penalty
   ```

**Result:**
- ✅ Consistent scale: All entries in `episode_rewards` are normalized
- ✅ Consistent semantics: Penalty overwrites (matching buffer behavior)
- ✅ Correct length: `len(episode_rewards) == buffer.step_counts[env]`
- ✅ Stability scores now computed on normalized rewards (correct variance)
- ✅ `env_total_rewards` now sums normalized rewards (matches what PPO learned)
- ℹ️ `reward_summary_accum` still uses raw rewards (for telemetry breakdowns)

**Tests passed:**
- [x] `tests/integration/test_vectorized_determinism.py`
- [x] `tests/integration/test_reward_telemetry_flow.py`
- [x] `tests/integration/test_governor_rollback.py`
- [x] `tests/simic/test_vectorized.py`

---

## Recommended Fix (IMPLEMENTED)

**Option 1: episode_rewards should mirror buffer (RECOMMENDED)**

```python
# During normal rollout (vectorized.py:2777-2778)
normalized_reward = reward_normalizer.update_and_normalize(reward)
env_state.episode_rewards.append(normalized_reward)  # Change to normalized

# During governor rollback (vectorized.py:3341)
# OVERWRITE last entry instead of appending
if env_states[env_idx].episode_rewards:
    env_states[env_idx].episode_rewards[-1] = normalized_penalty
```

**Pros:**
- Consistent with buffer semantics (overwrite)
- Consistent scale (all normalized)
- Correct length (N entries for N steps)
- Stability score timing is correct

**Cons:**
- Loses raw reward history (but we could add a separate `raw_episode_rewards` if needed)

**Option 2: Separate raw and learned rewards**

```python
# Add new fields:
env_state.raw_episode_rewards  # For telemetry (raw scale)
env_state.learned_episode_rewards  # For RL analysis (normalized scale)

# During rollout:
env_state.raw_episode_rewards.append(reward)
env_state.learned_episode_rewards.append(normalized_reward)

# During rollback:
if env_state.learned_episode_rewards:
    env_state.learned_episode_rewards[-1] = normalized_penalty
# Don't touch raw_episode_rewards (no raw equivalent for penalty)
```

**Pros:**
- Clear separation of concerns
- Both raw and learned rewards available

**Cons:**
- More fields to track
- More complex

---

## Verification

### How to Verify the Fix

- [ ] Add test that verifies `episode_rewards` and buffer have same length after rollback
- [ ] Add test that verifies all entries in `episode_rewards` have consistent scale
- [ ] Verify stability scores use correct reward history (after penalty applied)
- [ ] Check A/B testing metrics match buffer rewards, not mixed-scale `episode_rewards`

---

## Related Findings

- B11-DRL-01: LSTM hidden state bootstrap bug (FIXED, unrelated)
- B-METRIC-01: Previous attempt to sync episode_rewards with buffer (incomplete fix)

---

## Appendix

### Trace of episode_rewards for a rollback episode

**Episode with 3 steps, then rollback:**

```
Step 1: reward = -0.05 (raw)
  normalized_reward = reward_normalizer.update_and_normalize(-0.05) = -0.12
  episode_rewards.append(-0.05)  # RAW
  buffer.rewards[env, 0] = -0.12  # NORMALIZED

Step 2: reward = 0.10 (raw)
  normalized_reward = reward_normalizer.update_and_normalize(0.10) = 0.23
  episode_rewards.append(0.10)  # RAW
  buffer.rewards[env, 1] = 0.23  # NORMALIZED

Step 3: reward = -0.03 (raw)
  normalized_reward = reward_normalizer.update_and_normalize(-0.03) = -0.07
  episode_rewards.append(-0.03)  # RAW
  buffer.rewards[env, 2] = -0.07  # NORMALIZED

Rollback detected:
  penalty = -5.0 (raw)
  normalized_penalty = reward_normalizer.normalize_only(-5.0) = -2.3
  buffer.rewards[env, 2] = -2.3  # OVERWRITES step 3 reward
  episode_rewards.append(-2.3)  # APPENDS

Final state:
  buffer.rewards[env] = [-0.12, 0.23, -2.3]  # length 3, all normalized
  episode_rewards = [-0.05, 0.10, -0.03, -2.3]  # length 4, mixed scale
```

### Why this matters for metrics

**Stability score calculation:**
```python
reward_var = float(np.var(episode_rewards[-20:]))
stability = 1.0 / (1.0 + reward_var)
```

If `episode_rewards` has mixed scale, the variance is meaningless:
```
var([-0.05, 0.10, -0.03, -2.3]) = dominated by -2.3
```

But the agent learned from:
```
[-0.12, 0.23, -2.3]  # Different values, normalized scale
```
