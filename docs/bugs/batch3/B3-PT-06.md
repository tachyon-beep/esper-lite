# Finding Ticket: Alpha Tensor Creation in Forward Path

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-PT-06` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `pytorch` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/slot.py` |
| **Line(s)** | `1975-1983` |
| **Function/Class** | `SeedSlot.forward()` |

---

## Summary

**One-line summary:** Alpha tensor cache invalidation creates new tensors during forward, potentially causing torch.compile recompilation.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [x] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The alpha tensor cache validation happens inside `forward()`. When cache is invalidated (device/dtype change), a new tensor is created, which could cause recompilation if it happens during graph tracing.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/slot.py:1975-1983

if (
    self._cached_alpha_tensor is None
    or self._cached_alpha_tensor.device != host_features.device
    or self._cached_alpha_tensor.dtype != host_features.dtype
):
    self._cached_alpha_tensor = torch.tensor(...)
```

### Why This Is Acceptable

The PERF comment on line 1974 acknowledges this. Cache invalidation only happens at:
- `set_alpha()` calls
- Device changes (`.to()`)

During steady-state forward passes, the cache is valid.

---

## Resolution

**Acceptable as-is** - Document the expected behavior and ensure callers don't change alpha during compiled regions.

---

## Verification

### How to Verify the Fix

- [ ] Confirm cache invalidation only occurs outside compiled forward passes
- [ ] Test torch.compile behavior when alpha changes mid-training

---

## Related Findings

- B3-CR-06: Scattered cache invalidation (related cache management concern)

---

## Cross-Review: DRL Specialist

| Verdict | **ENDORSE** |
|---------|-------------|

**Evaluation:** The alpha tensor cache invalidation is correctly isolated from training-time forward passes. During PPO rollout collection and gradient updates, alpha changes only at epoch boundaries (advance_stage) or via explicit set_alpha calls, not mid-batch. The cache-check-then-create pattern is compile-safe for steady-state training. The P2 severity is appropriate given the compile recompilation risk if misused.

## Cross-Review: PyTorch Specialist

| Verdict | **NEUTRAL** |
|---------|-------------|

**Evaluation:** The cache invalidation only triggers on device/dtype mismatch which is rare in steady-state; however, the PERF comment at line 1974 already acknowledges this concern. The real risk is that `set_alpha()` calls during training invalidate the cache inside forward context, which could cause recompilation. Consider guarding alpha changes to occur only outside compiled regions.

## Cross-Review: Code Review Specialist

| Verdict | **ENDORSE** |
|---------|-------------|

**Evaluation:** The finding is accurate and well-analyzed. The code at lines 1975-1983 does create new tensors on cache miss, but the PERF comment and cache invalidation strategy (set_alpha, device transfer) ensure this only happens during setup, not steady-state training. The ticket correctly concludes this is acceptable as-is.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-pytorch.md`
**Section:** "P2 - Performance"
