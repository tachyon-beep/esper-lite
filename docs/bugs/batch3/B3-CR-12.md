# Finding Ticket: MLP Code Duplication (mlp_small vs mlp)

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-CR-12` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/blueprints/transformer.py` |
| **Line(s)** | Multiple |
| **Function/Class** | `TransformerMLPSmallSeed`, `TransformerMLPSeed` |

---

## Summary

**One-line summary:** `mlp_small` and `mlp` seeds differ only in expansion factor but duplicate most code.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The two MLP seed classes are nearly identical except for the expansion factor (2x vs 4x). This violates DRY and creates maintenance burden.

### Code Evidence

```python
# TransformerMLPSmallSeed: expansion = 2
# TransformerMLPSeed: expansion = 4
# Everything else is identical
```

---

## Recommended Fix

Share a base class or use a factory with expansion parameter:

```python
class TransformerMLPSeed(nn.Module):
    def __init__(self, dim: int, expansion: int = 4, checkpoint: bool = False):
        ...

# Then register with different defaults:
@blueprint("mlp_small", topology="transformer", param_estimate=...)
def create_mlp_small(dim: int, checkpoint: bool = False) -> nn.Module:
    return TransformerMLPSeed(dim, expansion=2, checkpoint=checkpoint)
```

---

## Verification

### How to Verify the Fix

- [ ] Refactor to shared implementation
- [ ] Verify output equivalence

---

## Related Findings

- B3-PT-05: MLP fused GELU (related to MLP implementation)
- B3-PT-08: FlexAttention fallback code duplication

---

## Cross-Review (PyTorch Specialist)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **NEUTRAL** | pytorch-expert | 2024-12-27 |

The duplication is cosmetic (14 lines each) and both classes compile identically under torch.compile with no graph breaks.
Refactoring is fine for maintainability but has zero performance benefit; low priority given no functional or compile impact.

---

## Cross-Review (DRL Specialist)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **NEUTRAL** | drl-expert | 2024-12-27 |

Duplication has no RL training impact - both classes produce identical gradient flow patterns (fc1->GELU->fc2 with zero-init output).
The 2x vs 4x expansion affects capacity and param count (policy complexity budget), but this is a code maintenance concern, not training stability. Refactoring is safe but not urgent.

## Cross-Review (Code Quality)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **NEUTRAL** | cross-review-specialist | 2024-12-27 |

Valid DRY violation but marginal benefit - both classes are already parameterized by `expansion` and duplication is only ~18 lines each.
If refactored, use a single class with factory wrappers; however, separate classes aid profiling/debugging clarity so this is low-priority.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-codereview.md`
**Section:** "P3 - Code Quality" (TRANS-4)
