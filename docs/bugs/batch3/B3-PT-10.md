# Finding Ticket: force_alpha Compile Safety Undocumented

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-PT-10` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `pytorch` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/slot.py` |
| **Line(s)** | `1127-1185` |
| **Function/Class** | `SeedSlot.force_alpha()` |

---

## Summary

**One-line summary:** `force_alpha` context manager mutates module state during forward, which causes torch.compile guard failures if used during compilation.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [x] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `force_alpha` context manager is well-documented as not thread-safe and not DDP-safe. However, it's not documented that it will cause torch.compile recompilation if used during a compiled forward pass.

### Current Documentation

```python
@contextmanager
def force_alpha(self, value: float) -> Generator[None, None, None]:
    """Temporarily override alpha for counterfactual evaluation.

    NOT thread-safe. NOT DDP-safe.
    """
```

### Missing Documentation

No mention of torch.compile implications.

---

## Recommended Fix

Add torch.compile warning to docstring:

```python
@contextmanager
def force_alpha(self, value: float) -> Generator[None, None, None]:
    """Temporarily override alpha for counterfactual evaluation.

    Warnings:
        - NOT thread-safe
        - NOT DDP-safe
        - Will cause torch.compile recompilation if used during a compiled region

    Use only in eval mode for counterfactual evaluation.
    """
```

---

## Verification

### How to Verify the Fix

- [ ] Update docstring
- [ ] Verify counterfactual evaluation uses eval mode (no compile)
- [ ] Add test demonstrating guard failure if misused

---

## Related Findings

- B3-CR-07: force_alpha not DDP-safe (related safety concern)

---

## Cross-Review: DRL Specialist

| Verdict | **OBJECT** |
|---------|-------------|

**Evaluation:** The ticket's premise is outdated - the docstring at lines 1147-1151 already documents torch.compile implications ("will cause graph specialization for the forced-alpha path"). The existing documentation is more precise than the proposed fix, explaining *why* it's acceptable (counterfactual evaluation runs once per epoch in eval mode). Recommend closing as already addressed.

## Cross-Review: PyTorch Specialist

| Verdict | **OBJECT** |
|---------|-------------|

**Evaluation:** The docstring at lines 1147-1151 already documents torch.compile implications: "will cause graph specialization for the forced-alpha path... acceptable as counterfactual evaluation runs once per epoch in eval mode." The ticket's claim of missing documentation is incorrect.

## Cross-Review: Code Review Specialist

| Verdict | **OBJECT** |
|---------|-------------|

**Evaluation:** The finding is outdated. Reviewing lines 1147-1151 shows the docstring already documents torch.compile behavior: "When using torch.compile, this will cause graph specialization for the forced-alpha path..." The existing documentation is adequate and more nuanced than the proposed fix. No action needed - close as already addressed.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-pytorch.md`
**Section:** "P3 - Code Quality"
