# Finding Ticket: Import-Time Flag Capture for FlexAttention

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-CR-11` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/blueprints/transformer.py` |
| **Line(s)** | `17-21` |
| **Function/Class** | Module-level `_HAS_FLEX_ATTENTION` |

---

## Summary

**One-line summary:** `_HAS_FLEX_ATTENTION` is set at import time - if PyTorch is upgraded while process runs (hot reload), this won't update.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The FlexAttention availability check happens at module import time. For hot reload scenarios (development), the flag won't update if PyTorch is upgraded.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/blueprints/transformer.py:17-21

try:
    from torch.nn.attention.flex_attention import flex_attention, create_block_mask
    _HAS_FLEX_ATTENTION = True
except ImportError:
    _HAS_FLEX_ATTENTION = False
```

### Why This Is Acceptable

- Hot reload of PyTorch is extremely rare in production
- Development environments typically restart the process
- Standard Python module behavior

---

## Resolution

**Acceptable as-is** - Document for future maintainers.

---

## Cross-Review (PyTorch Specialist)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **ENDORSE** | pytorch-expert | 2024-12-27 |

Import-time feature detection is the canonical PyTorch pattern (cf. `torch.cuda.is_available()`).
FlexAttention availability is determined by PyTorch version, not runtime state; hot-reloading PyTorch mid-process is undefined behavior anyway.

---

## Cross-Review (DRL Specialist)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **ENDORSE** | drl-expert | 2024-12-27 |

Import-time flag capture is correct and preferred for RL training - checking feature availability once prevents per-forward-pass overhead.
Hot reload is irrelevant for training loops which require process restart anyway (replay buffer, optimizer state). The fallback to SDPA ensures policy checkpoint compatibility.

## Cross-Review (Code Quality)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **ENDORSE** | cross-review-specialist | 2024-12-27 |

Import-time feature detection is idiomatic Python; hot-reloading PyTorch mid-process is unrealistic.
Original assessment correct - this is standard practice and no reasonable alternative exists without runtime overhead.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-codereview.md`
**Section:** "P3 - Code Quality" (TRANS-2)
