# Finding Ticket: Style Mask Clone on Every get_action Call

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B9-PT-01` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 9 |
| **Agent** | `pytorch` |
| **Domain** | `tamiyo/networks` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/networks/factored_lstm.py` |
| **Line(s)** | `499-508` |
| **Function/Class** | `FactoredRecurrentActorCritic.get_action()` |

---

## Summary

**One-line summary:** `.clone()` allocates a new tensor on every `get_action()` call even when not always needed.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 499-508
style_mask_override = masks["style"]
if style_mask_override is None:
    style_mask_override = torch.ones_like(head_logits["style"], dtype=torch.bool)
# Avoid `.any()` (CPU sync) by applying the override unconditionally.
style_mask_override = style_mask_override.clone()  # <-- allocation every call
style_irrelevant = (actions["op"] != LifecycleOp.GERMINATE) & (...)
style_mask_override[style_irrelevant] = False
style_mask_override[style_irrelevant, int(GerminationStyle.SIGMOID_ADD)] = True
```

The `.clone()` is necessary to avoid mutating the input mask, but it allocates a new tensor on every `get_action()` call. In the hot inference path during rollout collection, this adds memory pressure and allocation overhead.

The comment says "avoid .any()" but then does in-place modification anyway - the clone is specifically to protect the input mask.

### Impact

- **Memory churn**: New tensor allocation per step
- **GC pressure**: More objects for garbage collection
- **Inference latency**: Small but cumulative overhead

---

## Recommended Fix

Options:

1. **Pre-allocate buffer** in `__init__` and reuse:
```python
self._style_mask_buffer = None  # Lazy init

def get_action(self, ...):
    if self._style_mask_buffer is None or self._style_mask_buffer.shape != target_shape:
        self._style_mask_buffer = torch.empty(target_shape, dtype=torch.bool, device=device)
    style_mask_override = self._style_mask_buffer
    style_mask_override.copy_(masks["style"] if masks["style"] is not None else ...)
```

2. **Conditional clone** (check if modification needed):
```python
style_irrelevant = (actions["op"] != LifecycleOp.GERMINATE) & (...)
if style_irrelevant.any():  # Worth the sync for the savings
    style_mask_override = style_mask_override.clone()
    # ... modifications
```

3. **Accept current overhead** if profiling shows it's negligible

---

## Verification

### How to Verify the Fix

- [ ] Profile get_action() to measure current allocation overhead
- [ ] Implement fix if overhead is significant (>1% of step time)
- [ ] Benchmark before/after with typical batch sizes

---

## Related Findings

- B9-PT-02: Style mask logic duplication (same code appears in evaluate_actions)

---

## Appendix

### Original Report Reference

**Report files:**
- `docs/temp/2712reports/batch9-codereview.md` Section: P2-B
- `docs/temp/2712reports/batch9-drl.md` Section: N1
