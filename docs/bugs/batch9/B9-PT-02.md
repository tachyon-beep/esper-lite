# Finding Ticket: No Runtime Guard Against Backprop Through inference_mode

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B9-PT-02` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 9 |
| **Agent** | `pytorch` |
| **Domain** | `tamiyo/networks` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/networks/factored_lstm.py` |
| **Line(s)** | `392-396`, `436` |
| **Function/Class** | `FactoredRecurrentActorCritic.get_action()` |

---

## Summary

**One-line summary:** get_action uses inference_mode but no runtime check prevents backprop attempts on returned log_probs.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 392-396 (docstring warning)
"""
    WARNING: log_probs returned from this method are NOT differentiable
    (computed under torch.inference_mode()). For training, use
    evaluate_actions() instead.
"""

# Line 436
with torch.inference_mode():
    output = self.forward(...)
```

The docstring correctly warns that `log_probs` from `get_action()` are not differentiable. However:

1. If someone accidentally uses these log_probs in a loss computation, they get **silent failures** - the gradients just don't flow
2. There's no runtime assertion to catch this mistake
3. Training would appear to work but policy wouldn't learn

### Impact

- **Silent training failure**: Policy gradients would be zero
- **Hard to debug**: No error message, just learning that doesn't happen
- **API misuse risk**: Easy to confuse get_action vs evaluate_actions

---

## Recommended Fix

Add runtime assertion that detects gradient requirement on inputs:

```python
def get_action(self, state: torch.Tensor, ...):
    # Fail fast if caller expects gradients
    if state.requires_grad:
        raise RuntimeError(
            "get_action() is for inference only (no gradients). "
            "Use evaluate_actions() for training with gradients."
        )

    with torch.inference_mode():
        ...
```

Or add a warning when log_probs are created:

```python
# In returned GetActionResult
log_probs.requires_grad = False  # Already true from inference_mode
# Add a custom attribute for detection
log_probs._inference_mode_computed = True
```

---

## Verification

### How to Verify the Fix

- [ ] Add test that attempts backprop through get_action() results
- [ ] Verify assertion or warning is raised
- [ ] Document the inference vs training API split more prominently

---

## Related Findings

- B9-PT-01: Style mask clone overhead (same get_action method)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch9-drl.md`
**Section:** "N2 - No runtime guard against backprop through inference_mode results"
