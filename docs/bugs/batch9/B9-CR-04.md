# Finding Ticket: Duplicate TrainingMetrics Construction Logic

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B9-CR-04` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 9 |
| **Agent** | `codereview` |
| **Domain** | `tamiyo/tracker` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/tracker.py` |
| **Line(s)** | `173-188`, `261-282` |
| **Function/Class** | `SignalTracker.update()`, `SignalTracker.peek()` |

---

## Summary

**One-line summary:** TrainingMetrics construction is duplicated between update() and peek() methods.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `TrainingMetrics` dataclass is constructed in two places with similar logic:

```python
# In update() - Lines 261-282
return TrainingMetrics(
    epoch=epoch,
    global_step=global_step,
    val_loss=val_loss,
    val_accuracy=val_accuracy,
    best_val_accuracy=self._best_accuracy,
    best_val_loss=min(self._loss_history) if self._loss_history else float('inf'),
    # ... more fields
)

# In peek() - Lines 173-188 (similar construction)
return TrainingMetrics(
    epoch=epoch,
    global_step=global_step,
    # ... duplicated field population
)
```

If fields are added to `TrainingMetrics`, both methods must be updated. This creates maintenance burden and divergence risk.

### Impact

- **Maintenance burden**: Changes require two updates
- **Divergence risk**: Easy to update one and forget the other
- **DRY violation**: Repeated code that should be factored out

---

## Recommended Fix

Extract a `_build_metrics()` helper:

```python
def _build_metrics(
    self,
    epoch: int,
    global_step: int,
    val_loss: float,
    val_accuracy: float,
    # ... other params
) -> TrainingMetrics:
    """Construct TrainingMetrics from current tracker state."""
    return TrainingMetrics(
        epoch=epoch,
        global_step=global_step,
        val_loss=val_loss,
        val_accuracy=val_accuracy,
        best_val_accuracy=self._best_accuracy,
        best_val_loss=min(self._loss_history) if self._loss_history else float('inf'),
        # ... all fields
    )

def update(self, ...) -> TrainingSignals:
    # ... update state
    metrics = self._build_metrics(epoch, global_step, val_loss, val_accuracy, ...)
    return self._build_signals(metrics, ...)

def peek(self, ...) -> TrainingSignals:
    # ... no state mutation
    metrics = self._build_metrics(epoch, global_step, val_loss, val_accuracy, ...)
    return self._build_signals(metrics, ...)
```

---

## Verification

### How to Verify the Fix

- [ ] Extract _build_metrics helper
- [ ] Verify update() and peek() produce identical metrics for same inputs
- [ ] Add test comparing update vs peek output equivalence

---

## Related Findings

- B9-CR-02: best_val_loss naming (field in TrainingMetrics)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch9-codereview.md`
**Section:** "P3-C - Duplicate logic in update() and peek()"
