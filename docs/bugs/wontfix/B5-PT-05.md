# Finding Ticket: RewardNormalizer is CPU-Only

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-PT-05` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 5 |
| **Agent** | `pytorch` |
| **Domain** | `simic/control` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/control/normalization.py` |
| **Line(s)** | Entire `RewardNormalizer` class |
| **Function/Class** | `RewardNormalizer` |

---

## Summary

**One-line summary:** `RewardNormalizer` uses Python floats throughout, requiring CPU for each reward normalization.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

`RewardNormalizer` uses Python floats:

```python
self.mean: float = 0.0
self.m2: float = 0.0
self.count: int = 0
```

If called in a hot loop with GPU rewards, each call requires a scalar GPU-to-CPU transfer.

### Current Usage

From `vectorized.py` (L758), it's used for scalar rewards which likely come from Python computation, so this may be acceptable. However, if rewards are computed on GPU and transferred, this creates a sync point.

---

## Recommended Fix

**Investigation needed first:**

1. Profile to determine if this is actually a bottleneck
2. Check how rewards are computed (CPU or GPU)

If bottleneck confirmed:

```python
class RewardNormalizer:
    def __init__(self, device: str | torch.device = "cpu", ...):
        self.mean = torch.tensor(0.0, device=device)
        self.m2 = torch.tensor(0.0, device=device)
        self.count = torch.tensor(0, device=device)
```

---

## Verification

### How to Verify the Fix

- [ ] Profile reward normalization to confirm bottleneck
- [ ] If confirmed, implement GPU-native version
- [ ] If not a bottleneck, document why CPU is acceptable

---

## Related Findings

- B5-DRL-05: RunningMeanStd vs RewardNormalizer differences

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-pytorch.md`
**Section:** "P3 (Code Quality/Maintainability)" (RewardNormalizer CPU-only)

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** The ticket correctly identifies the CPU-only pattern but appropriately flags that profiling is needed before optimization. In RL training loops, reward computation is typically scalar and infrequent compared to forward/backward passes, so GPU-to-CPU sync for a single float is unlikely to be a meaningful bottleneck. The recommended fix using torch.tensor for scalar statistics would actually be less efficient due to kernel launch overhead for trivial scalar ops. Keep as P3 pending profiling evidence.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Reward normalization is called once per environment step, making GPU-CPU sync points a genuine concern for training throughput in vectorized environments. In PPO/A2C pipelines with 8-64 parallel environments, this could introduce measurable latency. However, the ticket correctly notes that profiling should confirm the bottleneck first - rewards often originate from CPU-side environment logic (Gym/Gymnasium), making the normalizer's CPU-native design appropriate. Recommend profiling before implementing the GPU-native version, as premature optimization could add complexity without benefit.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** The observation is technically correct - `RewardNormalizer` uses Python floats while `RunningMeanStd` uses GPU tensors. However, the design is intentional: `RewardNormalizer` handles scalar rewards (one float per step), not batched tensors. GPU tensors for single scalars would add overhead for no benefit. The ticket correctly notes profiling is needed first - this is speculative optimization. Keep open only if profiling reveals actual bottleneck.
