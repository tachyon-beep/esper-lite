# Finding Ticket: Scattered Cache Invalidation for Alpha Tensor

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-CR-06` |
| **Severity** | `P4` |
| **Status** | `closed` |
| **Batch** | 3 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/slot.py` |
| **Line(s)** | Multiple |
| **Function/Class** | `SeedSlot._cached_alpha_tensor` management |

---

## Summary

**One-line summary:** `_cached_alpha_tensor` invalidation is scattered across multiple methods (`set_alpha`, `prune`, `force_alpha`, `to`) - maintenance burden.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The alpha tensor cache must be invalidated when:
- Alpha value changes (`set_alpha`)
- Slot is pruned (`prune`)
- Alpha is temporarily forced (`force_alpha`)
- Device changes (`.to()`)

This logic is scattered across multiple methods.

### Code Evidence

```python
# Multiple locations:
self._cached_alpha_tensor = None  # Scattered invalidation
```

### Why This Matters

- Easy to miss an invalidation site when adding new methods
- Risk of stale cache serving wrong alpha values
- Maintenance burden

---

## Recommended Fix

Centralize cache management:

```python
def _invalidate_alpha_cache(self) -> None:
    """Clear cached alpha tensor. Call when alpha, device, or dtype changes."""
    self._cached_alpha_tensor = None

# Then use consistently:
def set_alpha(self, value: float) -> None:
    self._alpha = value
    self._invalidate_alpha_cache()
```

---

## Verification

### How to Verify the Fix

- [ ] Identify all cache invalidation sites
- [ ] Extract to single method
- [ ] Add test for cache consistency

---

## Related Findings

- B3-PT-06: Alpha tensor creation in forward path

---

## Cross-Review: PyTorch Specialist

| Verdict | Evaluation |
|---------|------------|
| **ENDORSE** | Cache invalidation scattered across 5 sites is a compile-safety hazard; a missed invalidation yields stale device/dtype tensors causing silent wrong results. Centralizing to `_invalidate_alpha_cache()` also documents the compile-visible state mutation. |

---

## Cross-Review: DRL Specialist

| Verdict | Evaluation |
|---------|------------|
| **ENDORSE** | Stale alpha cache is a **silent training bug** - wrong blending coefficients cause policy gradients to be computed against incorrect advantage baselines. The 5-site scattered invalidation is a maintenance hazard. Consider elevating to P1 given RL correctness implications. |

## Cross-Review: Code Review Specialist

| Verdict | Evaluation |
|---------|------------|
| **ENDORSE** | Verified 5 invalidation sites (lines 1113, 1170, 1185, 1582, 2026). Each has inline comments, but the `_invalidate_alpha_cache()` extraction would reduce future omission risk. Low-effort refactor with defensive value. |

---

## Resolution

### Status: WONTFIX

**Closed via Systematic Debugging investigation.**

#### Why This Is Not Worth Fixing

The "scattered invalidation" is **correctly designed**:

| Site | Method | Reason | Documented? |
|------|--------|--------|-------------|
| 1131 | `to()` | Device change - cache has device affinity | ✅ Yes |
| 1188 | `force_alpha()` | Before forcing alpha | ✅ Yes |
| 1203 | `force_alpha()` | Restore original (finally block) | ✅ Yes |
| 1616 | `prune()` | Reset slot state | ✅ Yes |
| 2066 | `set_alpha()` | Alpha value changed | ✅ Yes |

Each invalidation is placed **immediately next to its state mutation** with a comment explaining why. This is the correct pattern - not a bug.

#### Test File Analysis

The tests at `tests/integration/test_alpha_shock.py:71,79` have manual cache invalidation with "CRITICAL FIX" comments. However, this is because the **test bypasses the official API**:

```python
# Test code (bypasses API):
slot.state.alpha = 0.0  # Direct state mutation
slot._cached_alpha_tensor = None  # Manual invalidation required

# Correct approach:
slot.set_alpha(0.0)  # Includes cache invalidation
```

This is a **test quality issue**, not a production code bug.

#### No Production Bugs

- `git log --grep="stale.*alpha"` → No matching commits
- `git log --grep="alpha.*cache"` → Only test fixes and initial implementation
- No evidence of actual stale-cache bugs in production

#### Proposed Fix Adds Marginal Value

The refactor would add:
```python
def _invalidate_alpha_cache(self) -> None:
    self._cached_alpha_tensor = None  # 1 line
```

This doesn't prevent omissions - you still need to remember to call it. The current inline pattern with comments is equally clear and more explicit about placement.

#### Severity Downgrade

- Original: P2 (based on theoretical "maintenance burden")
- Revised: P4 (cosmetic code style preference)
- Resolution: WONTFIX

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-codereview.md`
**Section:** "P2 - Performance/Resource" (SLOT-4)
