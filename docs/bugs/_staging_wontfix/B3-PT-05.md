# Finding Ticket: MLP Seeds Could Benefit from Fused GELU

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-PT-05` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `pytorch` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/blueprints/transformer.py` |
| **Line(s)** | `151, 177` |
| **Function/Class** | `TransformerMLPSmallSeed`, `TransformerMLPSeed` |

---

## Summary

**One-line summary:** MLP seeds use separate GELU and matmul operations; fused GELU could improve performance.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The MLP forward passes use `F.gelu(self.fc1(x))` which creates separate GELU and matmul operations. For large dimensions (e.g., MLP with 4x expansion on dim=384), a fused approach could improve performance.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/blueprints/transformer.py:151,177

return self.fc2(F.gelu(self.fc1(x)))
```

### Why This Matters

- Separate ops prevent kernel fusion in some cases
- Minor optimization opportunity for large MLP dimensions
- TorchInductor may handle this automatically, but explicit hints help

---

## Recommended Fix

Consider using approximate GELU which enables better fusion:

```python
return self.fc2(F.gelu(self.fc1(x), approximate='tanh'))
```

Or rely on TorchInductor's automatic fusion and document the expectation.

---

## Verification

### How to Verify the Fix

- [ ] Profile MLP forward pass with exact vs approximate GELU
- [ ] Verify TorchInductor fusion behavior under torch.compile
- [ ] Check numerical equivalence (tanh approximation)

---

## Related Findings

- B3-CR-12: MLP code duplication (mlp_small vs mlp)
- B3-DRL-25: mlp_small/mlp code duplication

---

## Cross-Review

| Reviewer | Verdict | Date |
|----------|---------|------|
| Code Review Agent | **NEUTRAL** | 2024-12-27 |
| DRL Specialist | **OBJECT** | 2024-12-27 |
| PyTorch Expert | **NEUTRAL** | 2024-12-27 |

**Code Review Evaluation:** Valid optimization suggestion but with tradeoffs. Approximate GELU (`tanh`) introduces numerical differences (~1e-4 max deviation) which may affect training reproducibility. TorchInductor already fuses exact GELU in most cases; benchmark before changing.

**DRL Evaluation:** Approximate GELU (`tanh` approximation) introduces non-negligible numerical differences that can affect gradient flow through the MLP. In RL, small gradient differences compound across thousands of updates and can alter policy convergence behavior. The exact GELU is preferable for training stability; TorchInductor handles fusion automatically. Recommend closing as **WONTFIX** unless profiling shows measurable bottleneck.

**PyTorch Expert Evaluation:** TorchInductor in PyTorch 2.5+ automatically fuses Linear+GELU patterns regardless of exact vs approximate. The `approximate='tanh'` variant provides marginal speedup (~5-10%) but introduces numerical differences (~1e-4) that may compound in RL training. Only pursue if profiling identifies MLP as a bottleneck.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-pytorch.md`
**Section:** "P2 - Performance"
