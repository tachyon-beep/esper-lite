# Finding Ticket: Thread-Local Cache Accumulation in DataParallel

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B2-CR-01` |
| **Severity** | `P2` |
| **Status** | `closed` |
| **Batch** | 2 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/blending.py` |
| **Line(s)** | `62-104` |
| **Function/Class** | `BlendAlgorithm.__init__()`, `reset_cache()` |

---

## Summary

**One-line summary:** `_alpha_cache_local` is thread-local but `reset_cache()` only clears the calling thread's cache - other DataParallel workers accumulate entries.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [x] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The thread-local pattern for alpha caching is correct for DataParallel safety, but the cleanup mechanism only clears the current thread's cache. In long-running DataParallel training with persistent workers, other threads accumulate cache entries that never get cleared.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/blending.py:62-63, 93-104

def __init__(self) -> None:
    self._alpha_cache_local = threading.local()  # Per-thread cache

def reset_cache(self) -> None:
    """Clear thread-local alpha tensor cache."""
    self._alpha_cache_local.cache = None  # Only clears calling thread!
```

### Impact

- Memory accumulation across epochs
- Each DataParallel worker holds onto cached tensors
- Impact is small per-worker but scales with worker count

---

## Recommended Fix

### Option A: Document reset_cache() requirements

Clearly document that `reset_cache()` must be called from each worker thread:

```python
def reset_cache(self) -> None:
    """Clear this thread's alpha tensor cache.

    IMPORTANT: In DataParallel, this only clears the calling thread's cache.
    To clear all worker caches, call this from within each worker's context
    (e.g., via a custom collate_fn hook at epoch boundaries).
    """
```

### Option B: Track all thread-local instances

More complex but comprehensive:

```python
_all_thread_locals: weakref.WeakSet[threading.local] = weakref.WeakSet()

def __init__(self) -> None:
    self._alpha_cache_local = threading.local()
    BlendAlgorithm._all_thread_locals.add(self._alpha_cache_local)

@classmethod
def reset_all_caches(cls) -> None:
    """Clear caches across all threads (requires GC cooperation)."""
    for local in cls._all_thread_locals:
        if hasattr(local, 'cache'):
            del local.cache
```

---

## Verification

### How to Verify the Fix

- [ ] Profile memory usage in DataParallel training across epochs
- [ ] Verify cache clearing works in worker threads

---

## Related Findings

| Ticket ID | Relationship | Notes |
|-----------|--------------|-------|
| `B2-PT-01` | `duplicate` | Primary ticket - P1 severity |
| `B2-DRL-09` | `related` | DRL perspective on same issue |

---

## Cross-Review

| Agent | Verdict | Evaluation |
|-------|---------|------------|
| **PyTorch** | ENDORSE | Thread-local caching is incompatible with torch.compile (Dynamo cannot trace across thread boundaries) and DataParallel is deprecated in favor of DDP. Memory leak is real but symptom of deeper architectural issue - recommend migrating to DDP with process-isolated caching. |
| **CodeReview** | ENDORSE | Valid memory leak concern - thread-local cleanup is a known pitfall with DataParallel. Option A (documentation) is the pragmatic fix; Option B adds complexity for marginal benefit in typical training runs. |
| **DRL** | ENDORSE | Alpha tensor caching directly affects blending weight computation during policy forward passes. Stale/accumulating caches across DataParallel workers cause inconsistent alpha values per worker, which corrupts the blended policy output and introduces non-reproducible variance into the advantage estimates. |

---

## Resolution

**Fix:** Implemented Option A â€” enhanced `reset_cache()` docstring with explicit warning about thread-local semantics.

**Changes:**
- `src/esper/kasmina/blending.py:93-107`: Updated docstring to warn that in DataParallel, `reset_cache()` only clears the calling thread's cache

**Sign-off:** Approved by `feature-dev:code-reviewer`

**Commits:** `6f3f4b7b`

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch2-codereview.md`
**Section:** "File-by-File Analysis" - blending.py - P2
