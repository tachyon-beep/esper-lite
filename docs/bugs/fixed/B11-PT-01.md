# Bug Ticket: AMP Breaks Gradient Flow to Action Heads

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-PT-01` |
| **Severity** | `P0` |
| **Status** | `fixed` |
| **Batch** | 11 |
| **Agent** | `pytorch` |
| **Domain** | `simic/agent`, `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2026-01-02 |
| **Updated** | 2026-01-02 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `src/esper/tamiyo/policy/action_masks.py`, `src/esper/simic/training/vectorized.py` |
| **Line(s)** | `action_masks.py:497-498`, `vectorized.py:366-369` |
| **Function/Class** | `MaskedCategorical.__init__()`, vectorized training loop |

---

## Summary

**One-line summary:** Under AMP (Automatic Mixed Precision), gradient flow to all action heads is completely broken, causing all head gradient norms to be NaN in Sanctum UX.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [x] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Reproduction

**Command:**
```bash
uv run python -m esper.scripts.train ppo \
    --devices cuda:0 cuda:1 \
    --telemetry-dir ./telemetry \
    --sanctum \
    --telemetry-level debug \
    --config-json config.json \
    --gpu-preload \
    --rounds 2000 \
    --envs 64 \
    --experimental-gpu-preload-gather \
    --task cifar_impaired
```

**Expected:** After Tamiyo's first episode, Sanctum UX shows average gradient norms for all action heads (finite values).

**Actual:** All action head gradient averages show `NaN` in Sanctum UX.

---

## Root Cause Analysis

### The Problem

When training runs under AMP (`torch.amp.autocast`), the network forward pass outputs logits in `float16`. These float16 logits are passed directly to `MaskedCategorical`, which creates a `Categorical` distribution and computes `log_prob` in float16.

The log-softmax operation inside `Categorical.log_prob()` is numerically unstable in float16:
- Small differences in logits become indistinguishable
- The gradient path through log-softmax can produce NaN/Inf
- Backward propagation fails, leaving head parameters with `grad = None`

### Evidence

Debug probes show:
```
Compiled network params with grad: 9/49
slot_head: None=4, NaN/Inf=0, Finite=0 (of 4 params)
op_head: None=4, NaN/Inf=0, Finite=0 (of 4 params)
value_head: None=4, NaN/Inf=0, Finite=0 (of 4 params)
```

All head parameters have `grad = None` after `loss.backward()` under AMP. The 9 parameters with gradients are from the feature network and LSTM (which don't go through `log_prob`).

### Code Path

1. `vectorized.py:366-369`: `agent.update()` runs inside `torch.amp.autocast(device_type="cuda", dtype=dtype)`
2. Network forward produces float16 logits
3. `factored_lstm.py:891`: `MaskedCategorical(logits=logits_flat, mask=mask_flat)`
4. `action_masks.py:497-498`: `self.masked_logits = logits.masked_fill(~mask, MASKED_LOGIT_VALUE)` stays float16
5. `Categorical(logits=self.masked_logits)` operates in float16
6. `log_prob()` computes log-softmax in float16 - gradients break

---

## Implemented Fix

### Primary Fix (ppo.py)

Run `evaluate_actions()` outside the autocast context to ensure the entire network
forward pass uses float32 for stable gradient computation:

```python
# In PPOAgent.update(), around line 537:
with torch_amp.autocast(device_type="cuda", enabled=False):
    hidden_h = data["initial_hidden_h"].float()
    hidden_c = data["initial_hidden_c"].float()
    result = self.policy.evaluate_actions(
        data["states"].float(),
        data["blueprint_indices"],
        actions,
        masks,
        hidden=(hidden_h, hidden_c),
    )
```

This is standard practice in RL with AMP: run policy evaluation in float32 while
keeping backbone/feature extraction in AMP for speed.

### Secondary Fix (action_masks.py, belt-and-suspenders)

Upcast logits to float32 in MaskedCategorical for numerical stability:

```python
logits_f32 = logits.float()
self.masked_logits = logits_f32.masked_fill(~mask, MASKED_LOGIT_VALUE)
```

**Note:** The MaskedCategorical upcast alone is NOT sufficient - the full
`evaluate_actions` must run outside autocast. The upcast provides additional
safety if MaskedCategorical is called from other contexts.

---

## Regression Test

Test location: `tests/integration/test_sanctum_head_gradients.py`

Run with: `pytest tests/integration/test_sanctum_head_gradients.py -v -m ""`

All tests now pass:
- `test_head_gradients_finite_cuda_default` - **PASS** (no AMP)
- `test_head_gradients_finite_cuda_compiled` - **PASS** (torch.compile, no AMP)
- `test_head_gradients_finite_cuda_with_amp[float16]` - **PASS** (AMP with fix)
- `test_head_gradients_finite_cuda_with_amp[bfloat16]` - **PASS** (AMP with fix)

**Acceptance Criteria (User-Facing):**
The bug is fixed when running:
```bash
uv run python -m esper.scripts.train ppo --devices cuda:0 cuda:1 --telemetry-dir ./telemetry --sanctum --telemetry-level debug --config-json config.json --gpu-preload --rounds 2000 --envs 2 --experimental-gpu-preload-gather --task cifar_impaired
```
...results in Sanctum UX showing **finite values** (not `nan`) for all head gradient averages after Tamiyo's first episode.

The automated tests are proxies - they pass when the underlying gradient flow works correctly, which is what Sanctum displays.

---

## Impact

- **Severity:** P0 - Training telemetry is completely broken under AMP (default for CUDA)
- **Scope:** All CUDA training runs with AMP enabled
- **User Impact:** Cannot monitor gradient health during training; no visibility into head learning
- **Data Impact:** Telemetry data contains NaN values, degrading analysis capabilities

---

## References

- PyTorch AMP documentation recommends float32 for loss computation
- Similar issues: https://discuss.pytorch.org/t/nan-gradients-with-amp/
- Related ticket: `B10-PT-01` (inference_mode tensors)
