# Finding Ticket: Checkpointing Gap - Normalizer State Not Saved

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-PT-02` |
| **Severity** | `P2` |
| **Status** | `closed` |
| **Batch** | 5 |
| **Agent** | `pytorch` |
| **Domain** | `simic/control` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/control/normalization.py`, `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | N/A (integration issue) |
| **Function/Class** | Checkpointing |

---

## Summary

**One-line summary:** Normalizers have `state_dict()`/`load_state_dict()` but they're not used in checkpointing, causing distribution mismatch on resume.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The normalizers implement proper `state_dict()` and `load_state_dict()` methods, but grep search shows they are **not used** in checkpointing code:

```bash
$ grep -r "obs_normalizer.*state_dict\|reward_normalizer.*state_dict" src/esper/simic
# No matches
```

This means:
- Resuming training loses normalization statistics
- Distribution mismatch between old observations and new normalizer
- Can cause catastrophic policy collapse on resume

### Impact

When training is resumed from a checkpoint:
1. The normalizer starts fresh with zero/epsilon counts
2. Observations from continued training are normalized with wrong mean/var
3. Policy receives drastically different inputs than during original training
4. Likely immediate policy collapse

---

## Recommended Fix

Save and restore normalizer state in checkpoints:

```python
# In checkpoint save (vectorized.py)
checkpoint = {
    "network_state_dict": ...,
    "obs_normalizer_state": self.obs_normalizer.state_dict(),
    "reward_normalizer_state": self.reward_normalizer.state_dict(),
    ...
}

# In checkpoint load
if "obs_normalizer_state" in checkpoint:
    self.obs_normalizer.load_state_dict(checkpoint["obs_normalizer_state"])
if "reward_normalizer_state" in checkpoint:
    self.reward_normalizer.load_state_dict(checkpoint["reward_normalizer_state"])
```

---

## Verification

### How to Verify the Fix

- [ ] Add normalizer state to checkpoint save
- [ ] Add normalizer state to checkpoint load
- [ ] Test training resume preserves normalization statistics
- [ ] Verify policy doesn't collapse on resume

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-pytorch.md`
**Section:** "Cross-Cutting Integration Risks" (ID 1)

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `OBJECT` |
| **Reviewer** | DRL Specialist |

**Evaluation:** The ticket's premise is partially incorrect. Examining vectorized.py lines 815-835 shows normalizer restoration code exists in the LOAD path. However, the SAVE path (line 3320: `agent.save(save_path)`) does NOT pass metadata containing normalizer state. The issue is real but misdescribed: the state_dict/load_state_dict methods are correctly implemented and the load code exists, but the save call site doesn't include normalizer state in the metadata dict. The fix location should be vectorized.py around line 3320, not the normalizer class itself.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Verified and confirmed. The `vectorized.py` file at line 3320 calls `agent.save(save_path)` without passing metadata containing normalizer state, yet the loading code at lines 816-835 explicitly expects and restores `obs_normalizer_mean`, `obs_normalizer_var`, `obs_normalizer_count`, and `reward_normalizer_*` from metadata. This is a genuine correctness bug - training resumes will lose normalization statistics, causing the distribution mismatch described. The fix should include passing normalizer state in the `metadata` dict to `agent.save()`. P2 severity is appropriate given the impact on training resume capability.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** Confirmed critical gap. Lines 815-835 in vectorized.py show code to LOAD normalizer state from metadata, but `agent.save(save_path)` at line 3320 does NOT pass normalizer metadata to the save. The asymmetry is dangerous: resume logic exists but save logic does not populate the required fields. The impact assessment (distribution mismatch causing policy collapse) is accurate. Priority should arguably be P1 given catastrophic failure mode on checkpoint resume. Recommend saving normalizer state using the existing `state_dict()` methods rather than individual field extraction for cleaner code.

---

## Resolution

### Status: FIXED

**Fixed via systematic debugging investigation.**

#### Evidence Table

| Claim | Status | Evidence |
|-------|--------|----------|
| "Normalizers have state_dict()/load_state_dict()" | ✅ TRUE | `normalization.py:152-164` (RunningMeanStd), `:229-241` (RewardNormalizer) |
| "Load code exists for normalizer state" | ✅ TRUE | `vectorized.py:859-880` correctly restores from metadata |
| "Save code doesn't pass normalizer metadata" | ✅ TRUE | `vectorized.py:3767` was just `agent.save(save_path)` with no metadata |
| "PPOAgent.save() accepts metadata parameter" | ✅ TRUE | `ppo.py:1011` - `metadata: dict[str, Any] | None = None` |

#### The Fix

Modified `vectorized.py:3766-3782` to build and pass checkpoint metadata:

```python
if save_path:
    # B5-PT-02 FIX: Save normalizer state for correct training resume.
    checkpoint_metadata = {
        "obs_normalizer_mean": obs_normalizer.mean.tolist(),
        "obs_normalizer_var": obs_normalizer.var.tolist(),
        "obs_normalizer_count": obs_normalizer.count.item(),
        "obs_normalizer_momentum": obs_normalizer.momentum,
        "reward_normalizer_mean": reward_normalizer.mean,
        "reward_normalizer_m2": reward_normalizer.m2,
        "reward_normalizer_count": reward_normalizer.count,
        "n_episodes": episodes_completed,
    }
    agent.save(save_path, metadata=checkpoint_metadata)
```

#### PyTorch Specialist Review

The PyTorch specialist confirmed the fix is **correct and safe to ship**:
- `.tolist()` and `.item()` work correctly for checkpoint round-trip
- Minor dtype imprecision (float32 vs original) is acceptable since normalizers use float32 by default
- The fix correctly handles `momentum` which is missing from `RunningMeanStd.state_dict()`

#### Regression Tests Added

Added to `tests/simic/test_reward_normalizer_checkpoint.py`:
- `test_checkpoint_metadata_format_obs_normalizer` - verifies observation normalizer serialization
- `test_checkpoint_metadata_format_reward_normalizer` - verifies reward normalizer serialization
- `test_full_checkpoint_roundtrip` - end-to-end save/load verification

#### Severity Confirmation

- Original: P2 (correctness bug affecting training resume)
- Confirmed: P2 (all reviewers endorsed, PyTorch specialist suggested P1)
- Resolution: FIXED
