# Finding Ticket: Missing record_stream() for Fused Validation Input Tensors

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B8-PT-01` |
| **Severity** | `P1` |
| **Status** | `closed` |
| **Batch** | 8 |
| **Agent** | `pytorch` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-29 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | `1607-1608` |
| **Function/Class** | `process_fused_val_batch()` |

---

## Summary

**One-line summary:** Input tensors moved to device lack `record_stream()` calls, risking premature deallocation.

**Category:**
- [x] Correctness bug
- [x] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Line 1607-1608
inputs = inputs.to(env_dev, non_blocking=True)
targets = targets.to(env_dev, non_blocking=True)
# ... later
fused_inputs = inputs.repeat(num_configs, ...)
if env_state.stream and inputs.is_cuda:
    fused_inputs.record_stream(env_state.stream)
    fused_targets.record_stream(env_state.stream)
```

The original `inputs` and `targets` tensors are moved to the device with `non_blocking=True` and used to create `fused_inputs` via `repeat()`. However, `record_stream()` is only called on the fused tensors, not the originals.

**The problem:** `repeat()` reads from `inputs` asynchronously. If the CUDA allocator reuses the memory backing `inputs` before the stream completes reading, data corruption can occur.

### Impact

- **Data corruption**: Fused validation could use garbage data from reused memory
- **Silent failures**: No error raised, just incorrect counterfactual computations
- **Intermittent**: Only manifests under memory pressure when allocator reuses buffers

---

## Recommended Fix

```python
inputs = inputs.to(env_dev, non_blocking=True)
targets = targets.to(env_dev, non_blocking=True)

# Record stream BEFORE any operations that read from these tensors
if env_state.stream and inputs.is_cuda:
    inputs.record_stream(env_state.stream)
    targets.record_stream(env_state.stream)

fused_inputs = inputs.repeat(num_configs, ...)
fused_targets = targets.repeat(num_configs)

# Also record the output tensors
if env_state.stream and fused_inputs.is_cuda:
    fused_inputs.record_stream(env_state.stream)
    fused_targets.record_stream(env_state.stream)
```

---

## Verification

### How to Verify the Fix

- [x] Add `record_stream()` calls for input tensors immediately after device transfer
- [ ] Stress test with multiple environments on same GPU under memory pressure
- [ ] Verify no CUDA memory errors with `CUDA_LAUNCH_BLOCKING=1`

---

## Related Findings

- B8-PT-02: Alpha override tensors also missing record_stream()

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch8-pytorch.md`
**Section:** "P1 - Missing record_stream() for fused validation tensors"

---

## Resolution

| Field | Value |
|-------|-------|
| **Fixed By** | Claude Code |
| **Fixed Date** | 2025-12-29 |
| **Resolution** | `closed - CUDA memory safety fix applied` |

### Root Cause

**Scope Lifetime Mismatch:** The `.to(device, non_blocking=True)` call creates a new tensor via async copy when moving data between devices. The caller's `record_stream()` protects the original tensor, but the NEW tensor created inside `process_fused_val_batch()` was unprotected.

When the function returns:
1. Local `inputs`/`targets` variables go out of scope
2. CUDA allocator sees refcount=0, marks memory as reusable
3. But `repeat()` kernel might still be reading from that memory on `env_state.stream`
4. Result: Silent data corruption in counterfactual computations

### Fix Applied

Added `record_stream()` calls immediately after `.to()` in `process_fused_val_batch()`:

```python
inputs = inputs.to(env_dev, non_blocking=True)
targets = targets.to(env_dev, non_blocking=True)

# B8-PT-01 FIX: Protect source tensors from premature deallocation.
if env_state.stream and inputs.is_cuda:
    inputs.record_stream(env_state.stream)
    targets.record_stream(env_state.stream)

fused_inputs = inputs.repeat(num_configs, ...)
```

### B8-PT-02 Assessment

During investigation, confirmed that B8-PT-02 (alpha_overrides) is **NOT a bug**:
- Alpha tensors are created synchronously with `torch.full()`, not async copy
- Tensors live until `stream.synchronize()` at batch end (lines 2112-2114)
- CPU cannot exit scope until GPU completes â†’ no premature deallocation possible

Recommend closing B8-PT-02 as "not a bug".
