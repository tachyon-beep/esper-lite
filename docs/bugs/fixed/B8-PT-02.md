# Finding Ticket: Alpha Override Tensors Missing record_stream()

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B8-PT-02` |
| **Severity** | `P1` |
| **Status** | `closed` |
| **Batch** | 8 |
| **Agent** | `pytorch` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-29 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | `2053-2069` |
| **Function/Class** | `process_fused_val_batch()` |

---

## Summary

**One-line summary:** Alpha override tensors created for fused validation lack `record_stream()`, risking memory corruption.

**Category:**
- [x] Correctness bug
- [x] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 2053-2069 (approximate)
# Alpha override tensors are created on env_state.env_device
alpha_overrides = {
    slot_id: torch.tensor([alpha], device=env_state.env_device)
    for slot_id, alpha in config.items()
}
# These tensors are used in the fused forward pass
# But record_stream() is never called on them
```

When fused validation creates alpha override tensors, they're allocated on the environment's device and used in an async CUDA stream. Without `record_stream()`, the allocator may reuse this memory before the stream completes.

### Impact

- **Memory corruption**: Alpha values could be overwritten mid-computation
- **Incorrect blending**: Seeds could be evaluated with wrong alpha weights
- **Hard to debug**: Manifests as incorrect accuracy, not crashes

---

## Recommended Fix

```python
# After creating alpha override tensors
for alpha_tensor in alpha_overrides.values():
    if env_state.stream and alpha_tensor.is_cuda:
        alpha_tensor.record_stream(env_state.stream)
```

Alternative: Preallocate alpha tensors per config at batch start to avoid per-batch allocation.

---

## Verification

### How to Verify the Fix

- [ ] Add `record_stream()` calls after alpha tensor creation
- [ ] Verify under CUDA memory pressure with multiple fused configs
- [ ] Check counterfactual accuracy stability across runs

---

## Related Findings

- B8-PT-01: Fused validation input tensors also missing record_stream()

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch8-pytorch.md`
**Section:** "P1 - Alpha override tensors missing record_stream()"

---

## Resolution

| Field | Value |
|-------|-------|
| **Fixed By** | Claude Code |
| **Fixed Date** | 2025-12-29 |
| **Resolution** | `closed - not a bug` |

### Investigation Findings

During the B8-PT-01 investigation, this ticket was analyzed and determined to be **NOT a bug**.

**Why it's safe:**

1. **Synchronous creation**: Alpha tensors are created with `torch.full()`, which is synchronous (not an async copy like `.to(non_blocking=True)`)

2. **Lifetime guaranteed by sync barrier**: The counterfactual batch loop has an explicit `stream.synchronize()` at lines 2112-2114:
   ```python
   for env_state in env_states:
       if env_state.stream:
           env_state.stream.synchronize()
   ```

3. **CPU blocked until GPU completes**: The Python code cannot exit the scope where `alpha_overrides` is defined until `synchronize()` returns. This means the tensors are guaranteed to be alive for the entire duration of their GPU usage.

**Contrast with B8-PT-01**: The input tensors in B8-PT-01 ARE a bug because:
- They're created via async copy (`.to(non_blocking=True)`)
- The function returns BEFORE the sync barrier
- The local variables go out of scope while GPU is still using them

The alpha_overrides are created in the outer loop and live until after the sync barrier, so they're safe.
