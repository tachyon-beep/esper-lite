# Finding Ticket: Optimizer Momentum Zeroing on Orphaned Parameter References

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B1-PT-01` |
| **Severity** | `P2` |
| **Status** | `fixed` |
| **Batch** | 1 |
| **Agent** | `pytorch` |
| **Domain** | `tolaria` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tolaria/governor.py` |
| **Line(s)** | `310-322` |
| **Function/Class** | `TolariaGovernor.execute_rollback()` |

---

## Summary

**One-line summary:** After `load_state_dict()`, optimizer momentum zeroing operates on orphaned parameter references, making the operation harmless but useless.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

After `execute_rollback()` calls `model.load_state_dict(state_on_device, strict=False)`, the model's parameters are **new tensor objects**. However, the optimizer's `param_groups` still reference the **old parameter tensors** (before `load_state_dict`).

The momentum zeroing code iterates over `optimizer.param_groups[]["params"]` and zeros their momentum - but these are the OLD parameter objects that are about to be garbage collected.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/tolaria/governor.py:310-322

# Zero momentum/variance in optimizer to prevent stale state from polluting
# the restored model. Note: This only affects parameters in param_groups,
# which should match the model's parameters.
for group in optimizer.param_groups:
    for p in group["params"]:
        state = optimizer.state.get(p)
        if state:
            for value in state.values():
                if isinstance(value, torch.Tensor) and value.is_floating_point():
                    value.zero_()
```

### Why This Matters

1. **Wasted Work:** The code is zeroing momentum on parameter tensors that will be garbage collected
2. **False Confidence:** The comment suggests this prevents stale state, but the new parameters (from snapshot) have no optimizer state at all
3. **Silent Reset:** Training implicitly "cold starts" momentum after rollback because the new parameters aren't in `optimizer.state`
4. **Test Gap:** No test verifies that momentum is actually zeroed on the correct parameters

### System Context

When `load_state_dict()` is called:
1. PyTorch creates NEW tensor objects for all parameters
2. These new tensors are assigned to `model.named_parameters()`
3. The optimizer still holds references to the OLD tensors in `param_groups`
4. `optimizer.state` is a dict keyed by parameter OBJECTS (not names)
5. The new parameters aren't in `optimizer.state` - they have no momentum/variance yet

---

## Recommended Fix

### Option A: Remove the Dead Code

If implicit cold-start is acceptable:

```python
# Delete lines 310-322 entirely
# Add comment explaining that optimizer state is implicitly reset
# because parameter objects change after load_state_dict
```

### Option B: Reconstruct Optimizer After Rollback

For proper semantics (recommended):

```python
# Store optimizer class and kwargs in governor
# After rollback, reconstruct optimizer with new parameters:
new_optimizer = optimizer.__class__(
    self.model.parameters(),
    **self._optimizer_kwargs
)
return new_optimizer  # Return new optimizer to caller
```

### Option C: Document the Implicit Reset

If current behavior is intentional:

```python
# NOTE: After load_state_dict(), model parameters are new tensor objects.
# The optimizer still references old parameters, so its state won't apply
# to the new parameters. Training will effectively "cold start" momentum.
# This is intentional - we want clean optimization state after rollback.
# The momentum zeroing below is a no-op but kept for code clarity.
```

---

## Verification

### How to Verify the Fix

- [ ] Unit test: Create optimizer with momentum, run updates, rollback, verify momentum is zero for new parameters (not old)
- [ ] Integration test: Verify training dynamics after rollback match expected cold-start behavior
- [ ] Code review: Confirm either optimizer reconstruction or proper documentation

### Test Code

```python
def test_optimizer_state_after_rollback():
    """Verify optimizer state is properly handled after rollback."""
    model = nn.Linear(10, 2)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    governor = TolariaGovernor(model)

    # Snapshot clean state
    governor.snapshot()

    # Run some training to build up momentum
    for _ in range(5):
        loss = model(torch.randn(4, 10)).sum()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Verify momentum exists
    first_param = next(model.parameters())
    assert optimizer.state[first_param]['exp_avg'].abs().sum() > 0

    # Trigger rollback
    governor.execute_rollback(env_id=0, optimizer=optimizer)

    # After rollback, NEW parameters should have no momentum
    new_first_param = next(model.parameters())

    # These should be DIFFERENT tensor objects
    assert first_param is not new_first_param

    # New parameter should have no optimizer state
    assert new_first_param not in optimizer.state
```

---

## Related Findings

| Ticket ID | Relationship | Notes |
|-----------|--------------|-------|
| `B1-PT-05` | `related` | Multi-stream rollback test gap |

---

## Cross-Review

| Agent | Verdict | Evaluation |
|-------|---------|------------|
| **DRL** | ENDORSE | This is a real issue for RL training dynamics: after rollback, Adam's momentum buffers are orphaned, causing an implicit cold-start. For PPO stability during panic recovery, recommend Option B (reconstruct optimizer) or at minimum Option C (document the implicit reset and verify this is intentional). |
| **PyTorch** | ENDORSE | This is a real bug - after `load_state_dict()` creates new parameter tensors, the optimizer holds stale references. The momentum zeroing is dead code operating on GC-bound tensors; recommend Option C (document implicit cold-start) or Option B (rebuild optimizer). |
| **CodeReview** | ENDORSE | Excellent catch - the momentum zeroing operates on orphaned parameter references after load_state_dict() replaces tensor objects. This is dead code that creates false confidence. Recommend Option A (delete the dead code with explanatory comment) or Option C (document the implicit cold-start). |

---

## Resolution

**Status:** Fixed
**Resolved:** 2024-12-28
**Sign-off:** PyTorch Expert

**Fix applied (Option A + C):**
1. Removed dead momentum zeroing code that operated on orphaned parameter references
2. Added explanatory comment documenting the implicit cold-start behavior
3. Removed unused `optimizer` parameter from `execute_rollback()` signature
4. Updated call site in `vectorized.py:2549`

**Files modified:**
- `src/esper/tolaria/governor.py` - Removed dead code, updated signature
- `src/esper/simic/training/vectorized.py` - Updated call site

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch1-pytorch.md`
**Section:** "Cross-Cutting Integration Risks" - "Optimizer State After Rollback"

---

## CORRECTION (2024-12-28)

**Status:** This fix was based on incorrect assumptions and has been corrected.

### The Error

The original ticket claimed:
> "After `load_state_dict()` calls `model.load_state_dict(state_on_device, strict=False)`, the model's parameters are **new tensor objects**."

**This is factually incorrect.** PyTorch's `load_state_dict()` (with default `assign=False`) copies weights **in-place** via `param.copy_()`. Parameter objects retain their identity (same `id()`).

### Verification Method

```python
import torch.nn as nn

model = nn.Linear(10, 2)
params_before = list(model.parameters())
ids_before = [id(p) for p in params_before]

state_dict = model.state_dict()
model.load_state_dict(state_dict)

params_after = list(model.parameters())
ids_after = [id(p) for p in params_after]

assert ids_before == ids_after  # PASSES - same objects
assert params_before[0] is params_after[0]  # PASSES - same identity
```

### Consequence of the Error

1. The original momentum zeroing code was **NOT dead code** - it was functional
2. Removing it caused optimizer momentum to survive rollback
3. Stale momentum pointing toward diverged state risks immediate re-divergence

### Corrective Fix Applied

**Files modified:**
- `src/esper/simic/training/vectorized.py:2552-2562` - Added `optimizer.state.clear()` calls after rollback
- `src/esper/tolaria/governor.py:318-341` - Replaced incorrect comment with accurate documentation

**Key insight:** The `assign=True` parameter (PyTorch 2.1+) DOES replace Parameter objects, but then `optimizer.param_groups` references orphaned tensors and `optimizer.step()` updates garbage. Using `assign=True` would require optimizer recreation.

### Lessons Learned

1. **Verify PyTorch internals empirically** - don't assume based on mental models
2. **`load_state_dict()` is in-place by default** - Parameter objects survive
3. **Optimizer state is keyed by Parameter objects** - survives if params survive
4. **`assign=True` breaks optimizers** - would require full optimizer recreation
