# Finding Ticket: Unseeded RNG for Shapley Value Computation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-CR-01` |
| **Severity** | `P1` |
| **Status** | `fixed` |
| **Batch** | 5 |
| **Agent** | `codereview` |
| **Domain** | `simic/attribution` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2025-12-29 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/attribution/counterfactual.py` |
| **Line(s)** | `351, 400-402` |
| **Function/Class** | `CounterfactualEngine._generate_shapley_configs()` |

---

## Summary

**One-line summary:** `random.shuffle(perm)` uses global random state, making Shapley values non-reproducible.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The Shapley value computation uses `random.shuffle(perm)` without setting a seed:

```python
for _ in range(n_samples // 2):
    perm = list(range(n))
    random.shuffle(perm)  # Uses global state!
```

This causes:
1. Flaky tests
2. Irreproducible attribution results between runs
3. Difficulty debugging attribution anomalies

### Impact

Research reproducibility is compromised. Two runs with identical inputs may produce different Shapley values.

---

## Recommended Fix

Accept an optional `seed` parameter or use a local `random.Random()` instance:

```python
def _generate_shapley_configs(
    self, slot_ids: list[str], rng: random.Random | None = None
) -> list[tuple[bool, ...]]:
    rng = rng or random.Random()  # Or use self._rng initialized in __init__
    ...
    rng.shuffle(perm)
```

Or add a `seed` field to `CounterfactualConfig` that gets passed through.

---

## Verification

### How to Verify the Fix

- [x] Add seed parameter to CounterfactualConfig or _generate_shapley_configs
- [x] Verify two runs with same seed produce identical Shapley values
- [x] Add reproducibility test

---

## Related Findings

- B5-DRL-01: Population vs sample variance (related Shapley computation issue)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-codereview.md`
**Section:** "P1 - Important (Should Fix)" (ID 1)

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** This is a legitimate P1 finding. Using global `random.shuffle()` without a seeded RNG instance violates reproducibility, which is critical for both testing and research. The recommended fix of accepting a `random.Random` instance or adding a seed parameter is the correct approach and aligns with Python best practices for deterministic behavior.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** Reproducibility is critical for DRL training debugging and experiment validation. Using Python's `random.shuffle()` with global state is particularly problematic because it decouples from PyTorch's `torch.manual_seed()` ecosystem that controls tensor operations. The recommended fix using a local `random.Random()` instance is correct; for full reproducibility across the stack, consider also exposing a seed parameter that can be coordinated with `torch.manual_seed()` and `numpy.random.seed()` at the experiment level.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Shapley value computation is a Monte Carlo estimation process, and reproducibility is critical for debugging credit assignment in multi-seed RL training. Non-deterministic attribution makes it impossible to isolate whether training instability comes from the RL algorithm itself or from variance in the attribution signal. The recommended fix using a local `random.Random()` instance seeded from the run seed is correct and essential for experiment reproducibility.

---

## Resolution

| Field | Value |
|-------|-------|
| **Fixed By** | Claude Code |
| **Fixed Date** | 2025-12-29 |
| **Commit** | (pending) |

### Fix Applied

Added `seed` parameter to `CounterfactualConfig` and created local `random.Random` instance in `CounterfactualEngine`:

```python
# In CounterfactualConfig (line 54-56):
# B5-CR-01: Reproducibility seed for Shapley permutation sampling
# If None, uses unseeded RNG (non-reproducible but varied across runs)
seed: int | None = None

# In CounterfactualEngine.__init__ (line 202-203):
# B5-CR-01: Local RNG for reproducible Shapley permutation sampling
self._rng = random.Random(self.config.seed)

# Replace 2 shuffle calls (lines 359, 408):
self._rng.shuffle(perm)  # B5-CR-01: Use local RNG for reproducibility
```

### Key Implementation Decisions

1. **Local `random.Random` instance**: Thread-safe, decouples from global state
2. **Optional `seed` parameter**: Defaults to `None` for backwards compatibility
3. **Config-level seed**: Allows coordination with experiment-level seed management (as recommended by PyTorch specialist)

### Tests Added

- `test_generate_configs_reproducible_with_seed` - Same seed produces identical configs
- `test_generate_configs_different_without_seed` - Unseeded engines produce different configs
- `test_compute_shapley_reproducible_with_seed` - Same seed produces identical Shapley values
- `test_different_seeds_produce_different_results` - Different seeds produce different configs
- `test_seed_default_is_none` - Backwards compatibility verified
- `test_seed_can_be_set` - Config accepts seed parameter
