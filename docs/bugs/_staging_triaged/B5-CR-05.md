# Finding Ticket: Magic Numbers for Interaction Regime Thresholds

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-CR-05` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 5 |
| **Agent** | `codereview` |
| **Domain** | `simic/attribution` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/attribution/counterfactual.py` |
| **Line(s)** | `174-176` |
| **Function/Class** | `InteractionTerm.regime` |

---

## Summary

**One-line summary:** Hardcoded thresholds (0.5, -0.5) for synergy/interference classification lack documentation.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The regime classification uses hardcoded thresholds:

```python
if self.interaction > 0.5:  # Threshold for "significant"
    return "synergy"
elif self.interaction < -0.5:
    return "interference"
```

These thresholds may not be appropriate for all accuracy scales (e.g., if accuracy is 0-1 vs 0-100).

---

## Recommended Fix

Either:

1. **Normalize by baseline accuracy:**
```python
# Threshold relative to baseline (e.g., 5% of baseline)
threshold = abs(self.baseline) * 0.05
if self.interaction > threshold:
    return "synergy"
```

2. **Make thresholds configurable** via leyline constants:
```python
from esper.leyline import SYNERGY_THRESHOLD, INTERFERENCE_THRESHOLD

if self.interaction > SYNERGY_THRESHOLD:
    return "synergy"
```

3. **Document the rationale:**
```python
# Thresholds based on typical accuracy scale [0, 100].
# 0.5 represents a 0.5% accuracy change, considered meaningful.
```

---

## Verification

### How to Verify the Fix

- [ ] Add constants to leyline or document thresholds
- [ ] No functional change needed

---

## Related Findings

- B5-DRL-03: Hardcoded z-scores in is_significant (similar magic numbers)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-codereview.md`
**Section:** "P3 - Code Quality" (ID 7)

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Valid P3 finding. Magic numbers without documentation make code harder to maintain and tune. Per the project's leyline guidelines, shared constants belong in the leyline module. Moving `0.5`/`-0.5` to named constants like `SYNERGY_THRESHOLD` and `INTERFERENCE_THRESHOLD` in leyline would improve readability and enable configuration. The concern about accuracy scale (0-1 vs 0-100) is also worth addressing in the constant's docstring.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** This is primarily a domain/documentation concern rather than a PyTorch issue. The magic numbers affect classification logic, not tensor operations or numerical stability. However, if these thresholds are used in loss functions or gradient-based optimization, hard-coded values can create discontinuities. For pure classification/reporting purposes, moving to leyline constants is reasonable hygiene but not urgent from a PyTorch correctness standpoint.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Interaction regime thresholds directly influence which seed combinations are flagged as synergistic or interfering, which in turn affects policy decisions about seed retention and blending. Hardcoded thresholds are problematic because (1) reward scales vary across environments, and (2) early vs late training may require different sensitivity. Recommend moving these to leyline constants with clear documentation of their derivation, ideally as a fraction of baseline performance rather than absolute values.
