# Finding Ticket: Private API torch._foreach_norm Dependency

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B7-CR-07` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 7 |
| **Agent** | `codereview` |
| **Domain** | `simic/telemetry` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/telemetry/gradient_collector.py` |
| **Line(s)** | `154, 280, 487-524` |
| **Function/Class** | `SeedGradientCollector`, `collect_seed_gradients()` |

---

## Summary

**One-line summary:** Code depends on private API `torch._foreach_norm` without fallback implementation.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Line 154, 280
norms = torch._foreach_norm(grads, ord=2)
```

The code acknowledges the private API:
```python
# Comment: "stable internal API used by clip_grad_norm_"
```

However:
1. `torch._foreach_norm` is prefixed with `_` indicating private API
2. Private APIs can change without notice in PyTorch updates
3. The suggested fallback is documented but NOT implemented
4. If PyTorch removes this API, the code will break

---

## Recommended Fix

Add a fallback implementation:

```python
def _compute_norms(grads: list[torch.Tensor], ord: int = 2) -> list[torch.Tensor]:
    """Compute norms with fallback for private API changes."""
    try:
        # Prefer vectorized version (10-50x faster for many grads)
        return torch._foreach_norm(grads, ord=ord)
    except AttributeError:
        # Fallback if private API removed in future PyTorch
        return [torch.norm(g, p=ord) for g in grads]


# Usage:
norms = _compute_norms(grads, ord=2)
```

Also document minimum PyTorch version (2.0+) requirement prominently.

---

## Verification

### How to Verify the Fix

- [ ] Add _compute_norms helper with fallback
- [ ] Test both paths (force fallback via monkeypatch)
- [ ] Document PyTorch version requirement

---

## Related Findings

None.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `REFINE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** API concern is valid, but for telemetry (not training-critical), fallback adds complexity without proportional benefit.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |

**Evaluation:** `_foreach_norm` is stable internal API (used by `clip_grad_norm_`) but could change. Formalize into try/except fallback.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `REFINE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Existing comment shows awareness. Could downgrade to P4 given documentation and low blast radius (telemetry-only).

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch7-codereview.md`
**Section:** "Private API dependency"

**Report file:** `docs/temp/2712reports/batch7-pytorch.md`
**Section:** "Private API torch._foreach_norm"

**Report file:** `docs/temp/2712reports/batch7-drl.md`
**Section:** "Uses internal torch._foreach_norm API without fallback"
