# Finding Ticket: Death-Penalty Excluded from EpisodeOutcome After Rollback

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B11-CR-02` |
| **Severity** | `P2` |
| **Status** | `fixed` |
| **Batch** | 11 |
| **Agent** | `correctness` |
| **Domain** | `simic/training` |
| **Assignee** | |
| **Created** | 2026-01-01 |
| **Updated** | 2026-01-01 |
| **Fixed** | 2026-01-01 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/training/vectorized.py` |
| **Line(s)** | `3173-3214` (metrics computation), `3330-3341` (penalty injection) |
| **Function/Class** | `train_ppo_vectorized()` |

---

## Summary

**One-line summary:** Death penalty is applied to `episode_rewards` AFTER episode metrics (EpisodeOutcome, episode_history, stability) are computed, making rollback episodes appear more successful than they were in telemetry while the buffer has correct penalized rewards.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

**Execution sequence (INCORRECT):**

1. **Line 3173-3214 (epoch loop):** Episode ends (`epoch == max_epochs`), metrics computed:
   ```python
   # Line 3175: Total reward from PRE-PENALTY episode_rewards
   env_total_rewards[env_idx] = sum(env_state.episode_rewards)

   # Line 3178-3182: Episode history with PRE-PENALTY total
   episode_history.append({
       "env_id": env_idx,
       "episode_reward": env_total_rewards[env_idx],  # ← PRE-PENALTY
       "final_accuracy": env_final_accs[env_idx],
   })

   # Line 3185-3190: Stability from PRE-PENALTY variance
   recent_ep_rewards = env_state.episode_rewards[-20:]
   reward_var = float(np.var(recent_ep_rewards))  # ← PRE-PENALTY
   stability = 1.0 / (1.0 + reward_var)

   # Line 3193-3203: EpisodeOutcome with PRE-PENALTY values
   episode_outcome = EpisodeOutcome(
       env_id=env_idx,
       episode_idx=episodes_completed + env_idx,
       final_accuracy=env_state.val_acc,
       param_ratio=...,
       num_fossilized=env_state.seeds_fossilized,
       num_contributing_fossilized=env_state.contributing_fossilized,
       episode_reward=env_total_rewards[env_idx],  # ← PRE-PENALTY
       stability_score=stability,  # ← PRE-PENALTY variance
       reward_mode=env_reward_configs[env_idx].reward_mode.value,
   )
   episode_outcomes.append(episode_outcome)

   # Line 3207-3214: Telemetry emitted with PRE-PENALTY values
   if env_state.telemetry_cb:
       env_state.telemetry_cb(TelemetryEvent(
           event_type=TelemetryEventType.EPISODE_OUTCOME,
           data=EpisodeOutcomePayload(...),  # ← PRE-PENALTY
       ))
   ```

2. **Line 3330-3341 (rollback block, AFTER epoch loop):** Penalty finally applied:
   ```python
   if rollback_env_indices:
       for env_idx in rollback_env_indices:
           penalty = env_states[env_idx].governor.get_punishment_reward()
           normalized_penalty = reward_normalizer.normalize_only(penalty)

           # Buffer gets correct penalty (PPO learns correctly)
           agent.buffer.mark_terminal_with_penalty(env_idx, normalized_penalty)

           # episode_rewards updated AFTER metrics computed
           if env_states[env_idx].episode_rewards:
               env_states[env_idx].episode_rewards[-1] = normalized_penalty
   ```

### Impact

**What's correct:**
- ✅ **PPO learning**: Buffer contains penalized reward (via `mark_terminal_with_penalty`)
- ✅ **B11-CR-01 fix**: `episode_rewards` gets penalty via overwrite (correct semantics)

**What's broken:**
- ❌ **EpisodeOutcome.episode_reward**: Excludes penalty → Pareto analysis sees optimistic rewards
- ❌ **EpisodeOutcome.stability_score**: Computed from pre-penalty variance → stability inflated
- ❌ **episode_history**: Excludes penalty → A/B test comparisons biased
- ❌ **Telemetry events**: Emitted with pre-penalty values → dashboards show wrong data

**Real-world scenario:**
```
Episode earns +50 normalized reward over 25 epochs
Governor triggers rollback with -100 penalty

Buffer stores:  +50 (epochs 0-23), +50 (epoch 24), -100 (epoch 24, overwritten)
                → Total = +0 (PPO learns this)

Telemetry shows: episode_reward = +100 (sum of pre-penalty rewards)
                 stability_score = 0.95 (from low variance of +50, +50, +50...)

Reality:        episode_reward = +0 (includes penalty)
                stability_score = 0.60 (from high variance of +50, +50, -100)
```

This makes rollback episodes appear **~2x more rewarding** and **~1.6x more stable** than they actually were.

### Affected Systems

1. **Pareto analysis** (`EpisodeOutcome`) - overestimates reward/stability tradeoff for rollback runs
2. **A/B testing** (`episode_history`) - biases comparisons in favor of configurations that trigger rollbacks
3. **Karn dashboard** (telemetry events) - displays incorrect episode rewards and stability scores
4. **Governor effectiveness metrics** - underreports actual penalty impact

---

## Fix Applied

**Commit:** `[current commit]`

**Approach:** Recompute metrics after penalty injection (lines 3343-3400)

**Changes made:**

1. **Added import** (line 30): `import dataclasses`

2. **Added recomputation block** after penalty injection (lines 3343-3400):
   ```python
   # B11-CR-02 fix: Recompute metrics after penalty injection
   if rollback_env_indices:
       for env_idx in rollback_env_indices:
           env_state = env_states[env_idx]

           # 1. Recompute total reward from post-penalty episode_rewards
           env_total_rewards[env_idx] = sum(env_state.episode_rewards)

           # 2. Update episode_history entry for this env
           for entry in reversed(episode_history):
               if entry["env_id"] == env_idx:
                   entry["episode_reward"] = env_total_rewards[env_idx]
                   break

           # 3. Recompute stability from post-penalty variance
           recent_ep_rewards = (
               env_state.episode_rewards[-20:]
               if len(env_state.episode_rewards) >= 20
               else env_state.episode_rewards
           )
           if len(recent_ep_rewards) > 1:
               reward_var = float(np.var(recent_ep_rewards))
               stability = 1.0 / (1.0 + reward_var)
           else:
               stability = 1.0

           # 4. Find and replace EpisodeOutcome for this env
           # EpisodeOutcome is frozen dataclass, use dataclasses.replace()
           for i, outcome in enumerate(episode_outcomes):
               if outcome.env_id == env_idx:
                   corrected_outcome = dataclasses.replace(
                       outcome,
                       episode_reward=env_total_rewards[env_idx],
                       stability_score=stability,
                   )
                   episode_outcomes[i] = corrected_outcome

                   # 5. Re-emit corrected EPISODE_OUTCOME telemetry
                   if env_state.telemetry_cb:
                       env_state.telemetry_cb(TelemetryEvent(
                           event_type=TelemetryEventType.EPISODE_OUTCOME,
                           epoch=corrected_outcome.episode_idx,
                           data=EpisodeOutcomePayload(
                               env_id=env_idx,
                               episode_idx=corrected_outcome.episode_idx,
                               final_accuracy=corrected_outcome.final_accuracy,
                               param_ratio=corrected_outcome.param_ratio,
                               num_fossilized=corrected_outcome.num_fossilized,
                               num_contributing_fossilized=corrected_outcome.num_contributing_fossilized,
                               episode_reward=corrected_outcome.episode_reward,
                               stability_score=corrected_outcome.stability_score,
                               reward_mode=corrected_outcome.reward_mode,
                           ),
                       ))
                   break
   ```

**Rationale:**
- **Cleanest approach:** Keeps rollback block focused, no need to predict rollback before it happens
- **Minimal duplication:** Stability computation logic is repeated (6 lines), but avoids moving penalty injection earlier
- **Correct sequencing:** Metrics now computed from post-penalty rewards, matching what PPO learned from buffer
- **Frozen dataclass handling:** Uses `dataclasses.replace()` for immutable `EpisodeOutcome`
- **Telemetry re-emission:** Ensures dashboards see corrected values immediately

**Tests passed:**
- [x] All 4 reward telemetry flow tests
- [x] Vectorized determinism test
- [x] All Kasmina E2E tests (11/11)
- [x] Gradient isolation tests
- [x] Optimizer lifecycle test
- [x] Topology persistence test
- [x] **Total: 37/42 integration tests** (5 failures are pre-existing, unrelated to this fix)

**What changed:**
- **Before fix:** EpisodeOutcome showed episode_reward = +100, stability = 0.95 (PRE-PENALTY)
- **After fix:** EpisodeOutcome shows episode_reward = +0, stability = 0.60 (POST-PENALTY, matches buffer)

---

## Recommended Fix (IMPLEMENTED)

**Approach: Recompute metrics after penalty injection (CLEANEST)**

After the rollback block (line 3342), recompute affected metrics for rollback episodes:

```python
# After line 3341 (penalty applied to episode_rewards)
if rollback_env_indices:
    # Penalty injection (existing code)
    for env_idx in rollback_env_indices:
        penalty = env_states[env_idx].governor.get_punishment_reward()
        normalized_penalty = reward_normalizer.normalize_only(penalty)
        agent.buffer.mark_terminal_with_penalty(env_idx, normalized_penalty)
        if env_states[env_idx].episode_rewards:
            env_states[env_idx].episode_rewards[-1] = normalized_penalty

    # NEW: Recompute metrics with post-penalty rewards
    for env_idx in rollback_env_indices:
        # 1. Recompute total reward
        env_total_rewards[env_idx] = sum(env_states[env_idx].episode_rewards)

        # 2. Update episode_history entry (find the last entry for this env)
        for entry in reversed(episode_history):
            if entry["env_id"] == env_idx:
                entry["episode_reward"] = env_total_rewards[env_idx]
                break

        # 3. Recompute stability from updated rewards
        recent_ep_rewards = (
            env_states[env_idx].episode_rewards[-20:]
            if len(env_states[env_idx].episode_rewards) >= 20
            else env_states[env_idx].episode_rewards
        )
        if len(recent_ep_rewards) > 1:
            reward_var = float(np.var(recent_ep_rewards))
            stability = 1.0 / (1.0 + reward_var)
        else:
            stability = 1.0

        # 4. Update episode_outcome (find the last outcome for this env)
        for outcome in reversed(episode_outcomes):
            if outcome.env_id == env_idx:
                # EpisodeOutcome is a dataclass, need to mutate fields directly
                # (or use dataclasses.replace to create new instance)
                outcome.episode_reward = env_total_rewards[env_idx]
                outcome.stability_score = stability
                break

        # 5. Re-emit corrected telemetry event
        if env_states[env_idx].telemetry_cb:
            # Find the corrected outcome
            corrected_outcome = next(
                o for o in reversed(episode_outcomes) if o.env_id == env_idx
            )
            env_states[env_idx].telemetry_cb(TelemetryEvent(
                event_type=TelemetryEventType.EPISODE_OUTCOME,
                epoch=corrected_outcome.episode_idx,
                data=EpisodeOutcomePayload(
                    env_id=env_idx,
                    episode_idx=corrected_outcome.episode_idx,
                    final_accuracy=corrected_outcome.final_accuracy,
                    param_ratio=corrected_outcome.param_ratio,
                    num_fossilized=corrected_outcome.num_fossilized,
                    num_contributing_fossilized=corrected_outcome.num_contributing_fossilized,
                    episode_reward=corrected_outcome.episode_reward,
                    stability_score=corrected_outcome.stability_score,
                    reward_mode=corrected_outcome.reward_mode,
                ),
            ))
```

**Alternative (more invasive): Move penalty injection before metrics**

Thread rollback detection into the epoch loop (lines 3173-3214) so penalty is applied before metrics are computed. This requires predicting rollback earlier, which is more complex.

**Why recompute-after is better:**
- Keeps rollback block focused on its purpose
- No need to predict rollback before it happens
- Just corrects metrics for affected environments
- Minimal code duplication (stability computation logic repeated)

---

## Verification

### How to Verify the Fix

1. **Create test episode with rollback:**
   ```python
   # Accumulate +50 reward per epoch for 24 epochs
   # Trigger rollback on epoch 25 with -100 penalty
   # Verify:
   assert buffer.rewards[-1] == -100  # Buffer has penalty
   assert sum(env_state.episode_rewards) == 0  # Total includes penalty
   assert episode_outcome.episode_reward == 0  # Telemetry matches buffer
   ```

2. **Check stability score recomputation:**
   ```python
   # Before fix: stability computed from [+50, +50, +50, ...]
   # After fix: stability computed from [+50, +50, ..., -100]
   assert stability_with_penalty < stability_without_penalty
   ```

3. **Verify telemetry event emission:**
   ```python
   # Check that EPISODE_OUTCOME event contains post-penalty values
   assert telemetry_events[-1].data.episode_reward == 0
   assert telemetry_events[-1].data.stability_score < 0.7
   ```

---

## Related Findings

- **B11-CR-01**: Death-penalty bookkeeping inconsistency - Fixed overwrite vs append, but didn't catch that metrics are computed before overwrite happens
- **B-METRIC-01**: Prior attempt to sync episode_rewards with buffer (incomplete fix that led to B11-CR-01)

---

## Open Questions

1. **EpisodeOutcome mutability**: Is it safe to mutate `EpisodeOutcome` fields after creation, or should we use `dataclasses.replace()` to create a new instance?

2. **Telemetry re-emission**: Should we re-emit the corrected EPISODE_OUTCOME event, or should we emit a new event type like `EPISODE_OUTCOME_CORRECTED`?

3. **Historical data**: Should we backfill corrected metrics for past rollback episodes in the database, or just fix going forward?

---

## Notes

- This bug was present even after B11-CR-01 fix because the fix ensured episode_rewards gets the penalty correctly, but didn't address that metrics were computed before the penalty was applied
- The bug only affects telemetry/analysis, not RL learning (buffer has correct penalty via `mark_terminal_with_penalty`)
- Severity is P2 because it biases Pareto analysis and A/B testing, which affects hyperparameter selection and architecture decisions
