# Finding Ticket: Silent Fallback for achievable_range

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B6-CR-07` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 6 |
| **Agent** | `codereview` |
| **Domain** | `simic/rewards` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/rewards/rewards.py` |
| **Line(s)** | `1615` |
| **Function/Class** | `compute_loss_reward()` |

---

## Summary

**One-line summary:** `achievable_range or 1.0` silently falls back on zero instead of raising for invalid config.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [x] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
achievable_range = config.achievable_range or 1.0
```

The `achievable_range` property is computed as `self.baseline_loss - self.target_loss`. With defaults (`baseline_loss=2.3`, `target_loss=0.3`), this is `2.0`. The `or 1.0` fallback only triggers if `achievable_range == 0`.

This would require `baseline_loss == target_loss`, which is an invalid configuration (no range to optimize). The code silently accepts this misconfiguration instead of raising an error.

---

## Recommended Fix

Per project policy against silent fallbacks that hide bugs:

```python
achievable_range = config.achievable_range
if achievable_range <= 0:
    raise ValueError(
        f"Invalid config: achievable_range={achievable_range}. "
        f"Ensure baseline_loss > target_loss."
    )
```

Or, if the fallback is intentional for some valid use case, add a comment explaining when:

```python
# Fallback for edge case where baseline_loss == target_loss (e.g., testing)
achievable_range = config.achievable_range or 1.0
```

---

## Verification

### How to Verify the Fix

- [ ] Determine if `achievable_range == 0` is ever valid
- [ ] If not valid: raise ValueError instead of silent fallback
- [ ] If valid: document the use case

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch6-codereview.md`
**Section:** "P3-4: Redundant or-clause in Terminal Bonus"

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** While rewards are Python floats, not tensors, `achievable_range` being zero would cause division-by-zero when normalizing rewards before tensor conversion at training loop level. Silent fallback to 1.0 masks invalid configuration that could produce unexpected reward scaling. Raising a ValueError is the correct approach per project policy and prevents subtle numerical issues from propagating to the training loop.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** This finding directly aligns with the project's Defensive Programming Prohibition. The `or 1.0` pattern silently masks an invalid configuration (`baseline_loss == target_loss`) instead of failing loudly. Per CLAUDE.md: "If code would fail without a defensive pattern, that failure is a bug to fix, not a symptom to suppress." The recommended fix (raise ValueError) is correct. This should be elevated to P2 given explicit policy violation.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** This finding correctly identifies a subtle reward scaling bug. When `baseline_loss == target_loss`, the `achievable_range` becomes zero, and silently defaulting to 1.0 fundamentally changes the reward normalization - progress toward target would be scaled incorrectly, distorting the policy gradient signal. For PBRS-based shaping, consistent reward scaling is critical; silent fallbacks can cause training instability that is extremely difficult to diagnose. Fail-fast with a clear `ValueError` is the correct approach. If zero-range is ever intentional (e.g., test fixtures), it should require explicit opt-in, not implicit fallback.
