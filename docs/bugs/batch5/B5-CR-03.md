# Finding Ticket: compute_time_seconds Always 0.0 for Pre-computed Matrices

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-CR-03` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 5 |
| **Agent** | `codereview` |
| **Domain** | `simic/attribution` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/attribution/counterfactual.py` |
| **Line(s)** | `280-306` |
| **Function/Class** | `CounterfactualEngine.compute_matrix_from_results()` |

---

## Summary

**One-line summary:** When using pre-computed results, `compute_time_seconds` defaults to 0.0, making logging misleading.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

When using `compute_matrix_from_results()` with pre-computed results, `compute_time_seconds` defaults to 0.0. This makes logging at line 152-154 in the helper misleading:

```python
_logger.debug(
    f"Counterfactual computed: {len(matrix.configs)} configs, "
    f"{matrix.compute_time_seconds:.2f}s"  # Always 0.0 for pre-computed
)
```

---

## Recommended Fix

Either:

1. **Accept optional parameter:**
```python
def compute_matrix_from_results(
    self,
    slot_ids: list[str],
    results: dict[tuple[bool, ...], tuple[float, float]],
    compute_time_seconds: float = 0.0,  # Optional timing from caller
) -> CounterfactualMatrix:
```

2. **Document the limitation:**
```python
# NOTE: compute_time_seconds is only meaningful for compute_matrix().
# For pre-computed results, this field is 0.0.
```

---

## Verification

### How to Verify the Fix

- [ ] Add parameter or documentation
- [ ] No functional change needed

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-codereview.md`
**Section:** "P2 - Performance/Correctness Risk" (ID 5)

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** The issue is real but severity is debatable. Logging 0.0 for pre-computed results is technically accurate since no computation occurred at call time. Adding a docstring noting this semantic distinction would be sufficient; adding an optional parameter to pass external timing creates API surface that may not justify the complexity. A documentation fix is appropriate.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** This is a logging/telemetry issue with no direct PyTorch impact. However, accurate timing information is valuable for profiling counterfactual forward passes, especially when comparing GPU vs CPU execution or evaluating torch.compile benefits. If timing is added, use `torch.cuda.synchronize()` before start/end timestamps when running on GPU to ensure accurate wall-clock measurements that account for CUDA kernel asynchronicity.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Accurate compute time tracking is useful for profiling the attribution overhead in the training loop, which matters for sample efficiency analysis. However, this is a logging/telemetry issue that does not affect training correctness or policy optimization. Documentation noting the 0.0 limitation for pre-computed matrices is sufficient for now.
