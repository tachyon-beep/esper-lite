# Finding Ticket: Missing @torch.inference_mode() on normalize()

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-PT-03` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 5 |
| **Agent** | `pytorch` |
| **Domain** | `simic/control` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/control/normalization.py` |
| **Line(s)** | `123` |
| **Function/Class** | `RunningMeanStd.normalize()` |

---

## Summary

**One-line summary:** `normalize()` does tensor operations but lacks `@torch.inference_mode()` decorator.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `update()` method correctly uses `@torch.inference_mode()` (line 61), but `normalize()` does not:

```python
def normalize(self, x: torch.Tensor, clip: float = 10.0) -> torch.Tensor:
    """Normalize input using running statistics."""
    # ... tensor operations without inference_mode
```

Without `@torch.inference_mode()`:
- Autograd may track unnecessary gradients
- Slight memory overhead for gradient bookkeeping
- No functional incorrectness, but defensive best practice

---

## Recommended Fix

Add the decorator for consistency and to prevent accidental gradient tracking:

```python
@torch.inference_mode()
def normalize(self, x: torch.Tensor, clip: float = 10.0) -> torch.Tensor:
    """Normalize input using running statistics."""
    ...
```

---

## Verification

### How to Verify the Fix

- [ ] Add @torch.inference_mode() decorator
- [ ] Verify no test regressions

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-pytorch.md`
**Section:** "P3 (Code Quality/Maintainability)" (ID 1)

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `OBJECT` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Adding @torch.inference_mode() to normalize() would be INCORRECT. The normalize() method is called during the forward pass when gradients ARE needed - the normalized observations flow into the policy network which requires gradient tracking for PPO updates. The update() method correctly uses inference_mode because it only modifies running statistics (no gradient flow needed). Decorating normalize() with inference_mode would break backpropagation through the normalization layer. This ticket should be rejected.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `OBJECT` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Adding `@torch.inference_mode()` to `normalize()` would be incorrect. The `normalize()` method at line 123 is called during the training forward pass (see `vectorized.py` line 2440: `states_batch_normalized = obs_normalizer.normalize(states_batch)`), where gradients may need to flow through the normalized observations to the policy network. Wrapping it in `inference_mode()` would break gradient computation. The `update()` method correctly uses `inference_mode()` because it only updates running statistics and never participates in the computational graph. This ticket should be closed as "won't fix" - the asymmetry is intentional and correct.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `OBJECT` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** Adding `@torch.inference_mode()` to `normalize()` would be INCORRECT. The method at line 123 may be called during training when the normalized tensor feeds into the policy network for gradient computation. Wrapping it in inference_mode would detach the tensor from the computation graph, breaking backpropagation. The `update()` method correctly uses inference_mode because it only modifies internal statistics (mean/var/count) and its outputs are not used in gradient computation. This ticket should be CLOSED as invalid - the asymmetry is intentional and correct.
