# Finding Ticket: expand_mask Helper Defined Inside forward() Method

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-PT-06` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 10 |
| **Agent** | `pytorch` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/lstm_bundle.py` |
| **Line(s)** | `145-150` |
| **Function/Class** | `LSTMPolicyBundle.forward()` |

---

## Summary

**One-line summary:** Helper function `expand_mask` is defined inside `forward()` method, creating a new function object on every call.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
def forward(
    self,
    features: torch.Tensor,
    hidden: HiddenState,
    *,
    slot_mask: torch.Tensor | None = None,
    ...
) -> ForwardResult:
    # Dimension normalization
    need_expand = features.dim() == 2
    if need_expand:
        features = features.unsqueeze(1)

    # This helper is created on EVERY call
    def expand_mask(mask: torch.Tensor | None) -> torch.Tensor | None:
        if mask is None:
            return None
        if need_expand and mask.dim() == 2:
            return mask.unsqueeze(1)
        return mask

    # Used for multiple masks
    slot_mask = expand_mask(slot_mask)
    op_mask = expand_mask(op_mask)
    ...
```

Creating a function inside a method:
1. Allocates a new function object on every call
2. Creates a closure capturing `need_expand`
3. May prevent certain optimizations

### Impact

- **Low severity**: Minor overhead per forward pass
- **Hot path**: forward() is called on every step
- **Cumulative**: Adds up over millions of steps

---

## Recommended Fix

Move to module-level helper:

```python
def _expand_mask(
    mask: torch.Tensor | None,
    need_expand: bool
) -> torch.Tensor | None:
    """Expand 2D mask to 3D if needed."""
    if mask is None:
        return None
    if need_expand and mask.dim() == 2:
        return mask.unsqueeze(1)
    return mask


class LSTMPolicyBundle:
    def forward(self, ...) -> ForwardResult:
        need_expand = features.dim() == 2
        if need_expand:
            features = features.unsqueeze(1)

        slot_mask = _expand_mask(slot_mask, need_expand)
        op_mask = _expand_mask(op_mask, need_expand)
        ...
```

---

## Verification

### How to Verify the Fix

- [ ] Move expand_mask to module level
- [ ] Update forward() to use module-level helper
- [ ] Run tests to verify behavior unchanged

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch10-pytorch.md`
**Section:** P3 (lstm_bundle.py:145-150)
