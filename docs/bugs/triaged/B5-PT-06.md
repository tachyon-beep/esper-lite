# Finding Ticket: Auto Device Migration Triggers Graph Breaks

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B5-PT-06` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 5 |
| **Agent** | `pytorch` |
| **Domain** | `simic/control` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/control/normalization.py` |
| **Line(s)** | `71-72, 131-132` |
| **Function/Class** | `RunningMeanStd.update()`, `RunningMeanStd.normalize()` |

---

## Summary

**One-line summary:** Auto device migration (CPUâ†’GPU) triggers torch.compile graph breaks without warning.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [x] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

Both `update()` and `normalize()` auto-migrate stats to the input device:

```python
# In update()
if x.device != self.mean.device:
    self.to(x.device)  # Triggers graph break

# In normalize()
if x.device != self.mean.device:
    self.to(x.device)  # Triggers graph break
```

If `RunningMeanStd` is created on CPU but used with GPU tensors, the first call will:
1. Call `.to(device)`
2. Break any torch.compile compiled graph

The code has warning comments but no runtime warning is emitted.

---

## Recommended Fix

Add runtime warning when device migration occurs:

```python
if self.mean.device != x.device:
    import warnings
    warnings.warn(
        f"RunningMeanStd stats migrating from {self.mean.device} to {x.device}. "
        "For best torch.compile performance, initialize on target device.",
        stacklevel=2
    )
    self.to(x.device)
```

---

## Verification

### How to Verify the Fix

- [ ] Add warnings.warn for device migration
- [ ] Test that warning appears when expected
- [ ] No functional change

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch5-pytorch.md`
**Section:** "Cross-Cutting Integration Risks" (torch.compile Graph Break)

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** This is a legitimate torch.compile concern. The `.to(device)` call inside a forward-path method will cause graph breaks under Dynamo because device migration is a data-dependent control flow. The recommended fix of adding `warnings.warn()` is appropriate for surfacing the issue to users. Consider also using `torch._dynamo.error_on_graph_break()` in tests to catch this proactively. Elevate to P2 if `RunningMeanStd` is used within compiled policy/value networks.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Graph breaks during observation normalization are a significant concern for compiled RL training loops. RunningMeanStd.normalize() is typically called on every observation batch in the forward pass, so a graph break here defeats torch.compile benefits for the entire policy/value network forward. The recommended runtime warning is appropriate - silent device migration that breaks compilation is a debugging nightmare. Consider also exposing a `strict_device=True` option that raises an error instead of migrating, forcing callers to initialize on the correct device upfront.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Valid torch.compile concern. The code already has WARNING comments (L69-70, L128-130) but no runtime warning is emitted, meaning users only discover the graph break through TorchDynamo logs or poor performance. Adding `warnings.warn()` when device migration occurs is low-cost and provides immediate visibility. The fix is surgical and safe - recommend implementation.
