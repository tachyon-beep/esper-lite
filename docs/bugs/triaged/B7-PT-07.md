# Finding Ticket: Dynamic Tensor Size May Cause Recompilation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B7-PT-07` |
| **Severity** | `P4` |
| **Status** | `open` |
| **Batch** | 7 |
| **Agent** | `pytorch` |
| **Domain** | `simic/telemetry` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/telemetry/debug_telemetry.py` |
| **Line(s)** | `79-95` |
| **Function/Class** | `collect_per_layer_gradients()` |

---

## Summary

**One-line summary:** `torch.stack()` with dynamic layer count may cause recompilation under torch.compile.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [x] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
stat_tensors.append(torch.stack([
    grad_norm, grad_mean, grad_std, grad_min, grad_max,
    weight_norm, weight_mean, weight_std, weight_min, weight_max
]))
# ...
return torch.stack(stat_tensors).tolist()
```

Under `torch.compile(fullgraph=True)`:
- Iteration over `model.named_parameters()` is dynamic
- Number of layers varies by model architecture
- `torch.stack(stat_tensors)` creates varying-size tensors
- This triggers recompilation when layer count changes

**Impact:** Not critical since this is debug-only code that shouldn't be in compiled regions.

---

## Recommended Fix

Document that this function should not be called from compiled code:

```python
def collect_per_layer_gradients(model: nn.Module) -> list[LayerGradientStats]:
    """Collect per-layer gradient statistics.

    WARNING: Not torch.compile compatible. Dynamic layer iteration
    causes graph breaks. Only use in debug mode outside compiled regions.
    """
```

Or add explicit guard:
```python
@torch.compiler.disable  # Prevent inclusion in compiled graph
def collect_per_layer_gradients(...):
    ...
```

---

## Verification

### How to Verify the Fix

- [ ] Add docstring warning about torch.compile
- [ ] Optionally add @torch.compiler.disable decorator
- [ ] Verify debug code is not in compiled paths

---

## Related Findings

None.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | DRL Specialist |

**Evaluation:** Debug telemetry should explicitly opt out of compiled graphs to avoid silently degrading training performance.

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | PyTorch Specialist |

**Evaluation:** `@torch.compiler.disable` decorator is the correct fix. Dynamic shapes + `model.named_parameters()` iteration will cause graph breaks.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `ENDORSE` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** Documentation-only fix is insufficient. Decorator enforces constraint programmatically. Low urgency (P4 appropriate).

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch7-pytorch.md`
**Section:** "torch.compile risk - dynamic tensor size may cause recompilation"
