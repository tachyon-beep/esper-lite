# Finding Ticket: Per-Slot Nested Loops in Feature Extraction

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-CR-02` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 10 |
| **Agent** | `codereview` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/features.py` |
| **Line(s)** | `431-488` |
| **Function/Class** | `batch_obs_to_features()` |

---

## Summary

**One-line summary:** Nested loops with individual element writes are O(num_slots × num_envs) - documented TODO for vectorization.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 431-488
for slot_idx, slot_id in enumerate(slot_config.slot_ids):
    # ...
    for env_idx in range(n_envs):
        report = batch_slot_reports[env_idx].get(slot_id)
        # Individual element writes
        features[env_idx, offset] = 1.0
        features[env_idx, offset + 1] = stage_value
        # ... more individual writes
```

The TODO comment at lines 377-379 acknowledges this:
```python
# TODO: [FUTURE OPTIMIZATION] - Per-slot features use nested loops with individual
# element writes. Vectorizing to use advanced indexing could give 2-4x speedup
# for larger slot configurations.
```

### Impact

- **O(num_slots × num_envs)** individual tensor writes
- **Python loop overhead**: Per-element function calls
- **Hot path**: Called every rollout step
- **Scales poorly**: 25-slot configurations would be ~25× slower than 1-slot

---

## Recommended Fix

The TODO already documents the fix. Consider prioritizing if:
1. Moving to larger slot configurations (>9 slots)
2. Profiling shows this is a significant fraction of step time

Vectorization approach:
```python
# Instead of individual writes, build contiguous arrays first
slot_features = np.zeros((n_envs, SLOT_FEATURE_SIZE), dtype=np.float32)
for env_idx, reports in enumerate(batch_slot_reports):
    report = reports.get(slot_id)
    if report is not None:
        slot_features[env_idx] = extract_slot_features(report)

# Single vectorized write
features[:, offset:offset + SLOT_FEATURE_SIZE] = torch.from_numpy(slot_features)
```

---

## Verification

### How to Verify the Fix

- [ ] Profile with 9-slot and 25-slot configurations
- [ ] Implement vectorization if >5% of step time
- [ ] Existing TODO tracks this appropriately

---

## Related Findings

- B10-CR-01: Batch mask computation (similar pattern)
- B10-PT-03: Memory allocation per-call (related)

---

## Appendix

### Original Report Reference

**Report files:**
- `docs/temp/2712reports/batch10-codereview.md` Section: P2-3
- `docs/temp/2712reports/batch10-drl.md` Section: FE-3
- `docs/temp/2712reports/batch10-pytorch.md` Section: P2 (features.py:431-489)
