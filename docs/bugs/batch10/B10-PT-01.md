# Finding Ticket: initial_hidden() Returns Inference-Mode Tensors

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-PT-01` |
| **Severity** | `P1` |
| **Status** | `open` |
| **Batch** | 10 |
| **Agent** | `pytorch` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/lstm_bundle.py` |
| **Line(s)** | `261-275` |
| **Function/Class** | `LSTMPolicyBundle.initial_hidden()` |

---

## Summary

**One-line summary:** `initial_hidden()` decorated with `@torch.inference_mode()` returns non-differentiable tensors that silently won't contribute gradients if misused.

**Category:**
- [x] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 261-275
@torch.inference_mode()
def initial_hidden(self, batch_size: int = 1) -> HiddenState:
    """Get initial LSTM hidden state for rollout collection.

    WARNING: These tensors are created under torch.inference_mode().
    They are NOT differentiable and cannot be used for backpropagation.
    For training, pass hidden=None to evaluate_actions() which creates
    fresh gradient-compatible hidden states.
    ...
    """
```

The `@torch.inference_mode()` decorator is correct for rollout collection (performance), but:

1. If someone accidentally passes these tensors to `evaluate_actions()`, gradients silently won't flow through the initial hidden state
2. The docstring warns about this, but there's no runtime enforcement
3. Similar issue in Batch 9 with `get_action()` - this is a pattern

### Impact

- **Silent training bug**: Incorrect gradients, learning failure
- **Hard to diagnose**: No error message, just bad training curves
- **Risk**: Any developer unfamiliar with inference_mode semantics

---

## Recommended Fix

Options:

1. **Add marker attribute for runtime detection**:
```python
@torch.inference_mode()
def initial_hidden(self, batch_size: int = 1) -> HiddenState:
    h, c = self._network.get_initial_hidden(batch_size)
    # Mark tensors as inference-mode for potential runtime checks
    h._inference_mode_created = True
    c._inference_mode_created = True
    return h, c
```

2. **Add warning in evaluate_actions if hidden has marker**:
```python
def evaluate_actions(self, ..., hidden: HiddenState | None = None):
    if hidden is not None:
        h, c = hidden
        if getattr(h, '_inference_mode_created', False):
            warnings.warn(
                "Passing inference-mode hidden state to evaluate_actions. "
                "Gradients will not flow through initial state. "
                "Pass hidden=None for gradient-compatible states."
            )
```

3. **Document more prominently** (minimal fix):
   - Add warning to class docstring, not just method docstring
   - Add example showing correct usage pattern

---

## Verification

### How to Verify the Fix

- [ ] Add test that verifies hidden state from initial_hidden() doesn't contribute gradients
- [ ] Add warning or marker attribute for runtime detection
- [ ] Document the correct usage pattern at class level

---

## Related Findings

- B9-PT-02: No runtime guard against backprop through inference_mode (same pattern)
- B10-DRL-02: get_value() uses inference_mode (related)

---

## Appendix

### Original Report Reference

**Report files:**
- `docs/temp/2712reports/batch10-pytorch.md` Section: P1 (lstm_bundle.py:261-275)
- `docs/temp/2712reports/batch10-drl.md` Section: LB-1
