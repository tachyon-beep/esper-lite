# Finding Ticket: get_value() Uses inference_mode - Needs Clearer Documentation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-DRL-02` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 10 |
| **Agent** | `drl` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/lstm_bundle.py` |
| **Line(s)** | (get_value method) |
| **Function/Class** | `LSTMPolicyBundle.get_value()` |

---

## Summary

**One-line summary:** `get_value()` is decorated with `@torch.inference_mode()` but value bootstrap during rollout needs this - documentation should be clearer.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `get_value()` method uses `@torch.inference_mode()` decorator (similar to `initial_hidden()`). This is **correct** for bootstrap value computation during rollout collection, but:

1. If someone calls `get_value()` during training expecting gradients, they'll get silent failures
2. The decorator is correct for the intended use case
3. The method should document this more prominently

### Current vs Expected Usage

**Correct usage (rollout collection):**
```python
# During rollout, get bootstrap value for GAE
with torch.no_grad():  # or inference_mode automatically via decorator
    bootstrap_value = policy.get_value(last_obs, hidden)
```

**Incorrect usage (training):**
```python
# DON'T DO THIS - gradients won't flow
value = policy.get_value(obs, hidden)
loss = criterion(value, target)
loss.backward()  # Won't update anything!
```

For training, use `evaluate_actions()` which returns value along with log_probs/entropy and tracks gradients.

---

## Recommended Fix

Document more prominently:

```python
@torch.inference_mode()
def get_value(self, features: torch.Tensor, hidden: HiddenState) -> torch.Tensor:
    """Get value estimate for observations.

    INFERENCE-MODE: This method runs under torch.inference_mode() for
    performance during rollout collection. The returned value tensor
    is NOT differentiable.

    For training with gradient tracking, use evaluate_actions() instead,
    which returns (log_probs, entropy, values, hidden) with gradients.

    This method is intended for:
    - Bootstrap value computation in GAE
    - Value logging during rollout

    Args:
        features: Feature tensor [batch, feature_dim]
        hidden: LSTM hidden state

    Returns:
        Value estimates [batch, 1] - NOT differentiable
    """
```

---

## Verification

### How to Verify the Fix

- [ ] Add clear INFERENCE-MODE warning to docstring
- [ ] Add example showing correct vs incorrect usage
- [ ] Consider adding test that verifies gradient behavior

---

## Related Findings

- B10-PT-01: initial_hidden() inference-mode (same pattern)
- B9-PT-02: No runtime guard for inference_mode (similar)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch10-drl.md`
**Section:** LB-1 (related)
