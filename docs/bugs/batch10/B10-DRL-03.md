# Finding Ticket: Minor Redundancy in MaskedCategorical Entropy Computation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-DRL-03` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 10 |
| **Agent** | `drl` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/action_masks.py` |
| **Line(s)** | (MaskedCategorical.entropy method) |
| **Function/Class** | `MaskedCategorical.entropy()` |

---

## Summary

**One-line summary:** Entropy computation recalculates log_probs via `logits - logsumexp` then uses `probs * log_probs` - slightly redundant with internal Categorical computation.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `MaskedCategorical.entropy()` method computes entropy as:

```python
# Compute log_probs ourselves to avoid Categorical's internal computation
log_probs = masked_logits - logsumexp(masked_logits, dim=-1, keepdim=True)
probs = log_probs.exp()
entropy = -(probs * log_probs).sum(dim=-1)
# Then normalize
```

This is numerically stable and correct, but the internal Categorical distribution (used for sampling) already computes similar quantities. There's potential redundancy on the hot path.

### Impact

- **Minor overhead**: Extra logsumexp computation
- **Hot path**: Entropy is computed for PPO's entropy bonus
- **Correctness**: Current implementation is correct; this is pure optimization

---

## Recommended Fix

Profile before optimizing. If significant:

```python
def entropy(self) -> torch.Tensor:
    """Normalized entropy of the masked distribution."""
    # Reuse Categorical's internal computation
    base_entropy = self._dist.entropy()  # Uses cached probs
    # Normalize
    max_entropy = torch.log(self._num_valid.float().clamp(min=1))
    return base_entropy / max_entropy.clamp(min=1e-8)
```

However, this requires verifying Categorical.entropy() handles masked logits correctly (it uses the masked logits we provided).

---

## Verification

### How to Verify the Fix

- [ ] Profile entropy computation overhead
- [ ] If significant, refactor to reuse Categorical internals
- [ ] Verify normalized entropy values unchanged

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch10-drl.md`
**Section:** AM-4
