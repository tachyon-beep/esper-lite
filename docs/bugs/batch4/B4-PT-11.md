# Finding Ticket: entropy_loss vs entropy Naming Mismatch

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B4-PT-11` |
| **Severity** | `P4` |
| **Status** | `open` |
| **Batch** | 4 |
| **Agent** | `pytorch` |
| **Domain** | `simic` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/agent/types.py`, `/home/john/esper-lite/src/esper/simic/agent/ppo.py` |
| **Line(s)** | types.py: various, ppo.py: 813 |
| **Function/Class** | `PPOUpdateMetrics` |

---

## Summary

**One-line summary:** TypedDict has `entropy_loss` field but ppo.py stores `entropy` - naming inconsistency.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The TypedDict defines `entropy_loss`:

```python
class PPOUpdateMetrics(TypedDict, total=False):
    entropy_loss: float
```

But ppo.py stores `entropy`:

```python
# Line 813
epoch_metrics["entropy"] = entropy
```

One of these is wrong, or they're different concepts that should both exist.

### Clarification Needed

- `entropy`: The raw entropy value (higher = more exploration)
- `entropy_loss`: The entropy bonus in the loss (typically `-entropy_coef * entropy`)

If both concepts are used, both should be in the TypedDict.

---

## Recommended Fix

Clarify and align:

```python
class PPOUpdateMetrics(TypedDict, total=False):
    entropy: float          # Raw entropy (higher = more random)
    entropy_loss: float     # Loss contribution (-coef * entropy)
```

Or if only one is used, make names consistent.

---

## Verification

### How to Verify the Fix

- [ ] Determine whether both `entropy` and `entropy_loss` are needed
- [ ] Update TypedDict and ppo.py to be consistent
- [ ] Run mypy to verify

---

## Related Findings

- B4-DRL-10: Missing advantage_mean/std (similar TypedDict gap)

---

---

## Cross-Review

### DRL Specialist

| Verdict | ENDORSE |
|---------|---------|
| **Evaluation** | In PPO, both `entropy` (raw exploration measure) and `entropy_loss` (coefficient-weighted loss term) are meaningful metrics. Raw entropy tracks exploration health; entropy_loss shows its contribution to the objective. The TypedDict already has both fields (line 53 and 58) - ppo.py correctly stores raw entropy. The ticket's concern is **already resolved** in current code. |

### PyTorch Specialist

| Verdict | NEUTRAL |
|---------|---------|
| **Evaluation** | No tensor operation concerns. The entropy value logged at line 813 comes from `logging_tensors[2]` which is `-entropy_loss` (see line 806), so it's actually the negated entropy contribution. Naming is semantically correct given the negation. This is a documentation/clarity issue, not a PyTorch correctness issue. |

### Code Review Specialist

| Verdict | ENDORSE |
|---------|---------|
| **Evaluation** | Looking at types.py lines 53 and 58, the TypedDict already defines BOTH `entropy_loss: float` AND `entropy: float`. The ppo.py code stores to `metrics["entropy"]` which matches line 58. **The ticket is based on outdated information** - both fields exist in the TypedDict. Recommend closing as invalid. |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch4-pytorch.md`
**Section:** "P4 (Style/Minor)" (TYP-2)
