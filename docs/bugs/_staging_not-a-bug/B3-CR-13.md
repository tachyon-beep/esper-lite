# Finding Ticket: Private API Usage (torch._foreach_norm)

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-CR-13` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/isolation.py` |
| **Line(s)** | `171-172` |
| **Function/Class** | `GradientHealthMonitor` |

---

## Summary

**One-line summary:** `torch._foreach_norm` is private API (underscore prefix) - should include version constraint note.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The code uses `torch._foreach_norm` which is private (underscore prefix). The comment acknowledges it's "stable since 1.9" but should include version constraint.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/isolation.py:171-172

# NOTE: _foreach_norm is private but stable since PyTorch 1.9
norms = torch._foreach_norm(host_grads)
```

---

## Resolution

**Acceptable as-is** - Comment already documents the constraint. Per project policy, we don't add legacy compatibility code.

---

## Cross-Review (PyTorch Specialist)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **ENDORSE** | pytorch-expert | 2024-12-27 |

`torch._foreach_norm` is used internally by `torch.nn.utils.clip_grad_norm_` and has been stable since 1.9; removing it would regress to O(n) kernel launches.
The existing comment documents the fallback path; adding defensive hasattr would violate project policy against bug-hiding patterns.

---

## Cross-Review (DRL Specialist)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **ENDORSE** | drl-expert | 2024-12-27 |

`torch._foreach_norm` is essential for efficient gradient health monitoring in RL - it provides O(1) kernel launches vs O(n_params) which directly impacts training throughput.
The G2 gate uses these norms for seed graduation decisions; regressing to per-parameter norms would add measurable overhead to the training loop. The fallback comment is adequate.

## Cross-Review (Code Quality)

| Verdict | Reviewer | Date |
|---------|----------|------|
| **ENDORSE** | cross-review-specialist | 2024-12-27 |

The code already has excellent documentation including fallback strategy, stability history, and 10-50x performance justification.
Original assessment correct - this is a well-documented, pragmatic use of a stable private API with clear migration path if needed.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-codereview.md`
**Section:** "P3 - Code Quality" (ISO-2)
