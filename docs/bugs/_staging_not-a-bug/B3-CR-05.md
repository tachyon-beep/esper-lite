# Finding Ticket: Cross-Device Tensor Moves in Isolation

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B3-CR-05` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 3 |
| **Agent** | `codereview` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/isolation.py` |
| **Line(s)** | `177-178, 189-190` |
| **Function/Class** | `GradientHealthMonitor` async methods |

---

## Summary

**One-line summary:** `norms_unified = [n.to(target_device) for n in norms]` creates temporary tensors - with many parameters across devices, this could be expensive.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The gradient norm unification moves tensors across devices to stack them. For model-parallel setups with parameters on multiple GPUs, this creates cross-device transfers.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/isolation.py:177-178,189-190

target_device = norms[0].device
norms_unified = [n.to(target_device) for n in norms]
```

### Why This Is Currently Acceptable

- Comment acknowledges the limitation (PERF NOTE)
- Model parallelism is not the primary use case
- Single-GPU/DDP training doesn't hit this path

---

## Resolution

**Acceptable as-is** - Document the limitation for model-parallel users.

---

## Verification

### How to Verify the Fix

- [ ] Profile with model-parallel setup if needed
- [ ] Consider lazy aggregation for multi-GPU hosts

---

## Related Findings

None.

---

## Cross-Review: PyTorch Specialist

| Verdict | Evaluation |
|---------|------------|
| **NEUTRAL** | Cross-device `.to()` on scalar norms is negligible overhead (<1KB per parameter). The real fix for model-parallel would be `torch.distributed.reduce()` per device then combine, but current single-GPU/DDP path is fine. |

---

## Cross-Review: DRL Specialist

| Verdict | Evaluation |
|---------|------------|
| **ENDORSE** | Correct assessment - cross-device tensor moves for gradient norm computation are a non-issue for RL training. Norms are computed once per update for monitoring, not in the policy gradient hot path. The comment-documented limitation is sufficient. |

## Cross-Review: Code Review Specialist

| Verdict | Evaluation |
|---------|------------|
| **NEUTRAL** | The `# FIX BUG-013` comment documents this as an intentional fix for mixed-device `torch.stack` failures. For single-GPU/DDP (primary training mode), norms are on the same device and `.to()` is a no-op. Acceptable as-is; model-parallel is out of scope. |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch3-codereview.md`
**Section:** "P2 - Performance/Resource" (ISO-1)
