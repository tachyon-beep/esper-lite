# Finding Ticket: ratio_penalty in shaped_reward_ratio Calculation Questionable

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B6-CR-08` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 6 |
| **Agent** | `codereview` |
| **Domain** | `simic/rewards` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/rewards/reward_telemetry.py` |
| **Line(s)** | `84-99` |
| **Function/Class** | `RewardComponentsTelemetry.shaped_reward_ratio` |

---

## Summary

**One-line summary:** `ratio_penalty` included in shaping terms but it modifies the primary attribution signal, not shaping.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [x] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
shaped = (
    # Bonuses
    self.stage_bonus + self.pbrs_bonus + self.synergy_bonus + ...
    # Penalties (these shape behavior, so include in total)
    + self.compute_rent + self.alpha_shock + self.blending_warning
    + self.holding_warning + self.ratio_penalty  # <-- Is this shaping?
)
```

The comment says "PyTorch Expert Review 2025-12-26: Added all shaping terms" but includes `ratio_penalty` which is NOT a pure shaping term. Per `rewards.py` line 616, `ratio_penalty` modifies the attribution computation before weight multiplication - it's part of the primary credit assignment signal, not PBRS-style shaping.

Including `ratio_penalty` in `shaped_reward_ratio` may give misleading results when analyzing reward composition.

---

## Recommended Fix

Review whether `ratio_penalty` belongs in the shaping terms:

**Option 1 - Exclude:**
```python
shaped = (
    self.stage_bonus + self.pbrs_bonus + self.synergy_bonus + ...
    + self.compute_rent + self.alpha_shock + self.blending_warning
    + self.holding_warning
    # ratio_penalty excluded - modifies primary attribution, not shaping
)
```

**Option 2 - Document:**
```python
# ratio_penalty is included because it's behavior-shaping even though
# it modifies attribution rather than adding a separate bonus
+ self.ratio_penalty
```

---

## Verification

### How to Verify the Fix

- [ ] Review semantic role of ratio_penalty
- [ ] Update calculation or add documentation

---

## Related Findings

- B6-PT-02: shaped_reward_ratio includes unset/double-counted fields

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch6-codereview.md`
**Section:** "P3-5: shaped_reward_ratio Includes Some Terms Twice?"

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | PyTorch Specialist |
| **Date** | 2024-12-27 |

**Evaluation:** This is a semantic question about reward decomposition for telemetry, not a PyTorch concern. The classification of `ratio_penalty` as "shaping" vs "primary attribution" affects how the shaped_reward_ratio metric is interpreted in dashboards but has no impact on numerical stability, tensor operations, or torch.compile compatibility. Deferring to DRL specialist for the correct semantic categorization.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NEUTRAL` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** The concern about `ratio_penalty` categorization is valid, but requires domain expertise (DRL specialist) to determine the correct semantic classification. The telemetry metric's accuracy depends on correctly distinguishing "primary attribution" from "shaping terms" - a nuanced RL distinction. The ticket correctly identifies the ambiguity but cannot be resolved without consulting the DRL expert who designed the reward structure. Defer to DRL cross-reviewer for final verdict.

---

## Cross-Review (DRL Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `OBJECT` |
| **Reviewer** | DRL Specialist |

**Evaluation:** The ticket's premise is incorrect. Looking at the code, `ratio_penalty` IS a shaping term - it penalizes structural entanglement (high causal contribution with low improvement) to discourage reward hacking patterns. The fact that it's applied before weight multiplication at line 616 is an implementation detail; semantically it shapes behavior away from exploiting the counterfactual validation gap. Including it in `shaped_reward_ratio` is appropriate for detecting when shaping signals dominate the reward - which is exactly what this metric is designed to monitor. The current implementation correctly captures all behavior-shaping components for reward hacking detection.
