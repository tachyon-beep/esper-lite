# Finding Ticket: Per-Slot Nested Loops in Feature Extraction

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-CR-02` |
| **Severity** | `P4` |
| **Status** | `closed` |
| **Batch** | 10 |
| **Agent** | `codereview` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/features.py` |
| **Line(s)** | `431-488` |
| **Function/Class** | `batch_obs_to_features()` |

---

## Summary

**One-line summary:** Nested loops with individual element writes are O(num_slots × num_envs) - documented TODO for vectorization.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 431-488
for slot_idx, slot_id in enumerate(slot_config.slot_ids):
    # ...
    for env_idx in range(n_envs):
        report = batch_slot_reports[env_idx].get(slot_id)
        # Individual element writes
        features[env_idx, offset] = 1.0
        features[env_idx, offset + 1] = stage_value
        # ... more individual writes
```

The TODO comment at lines 377-379 acknowledges this:
```python
# TODO: [FUTURE OPTIMIZATION] - Per-slot features use nested loops with individual
# element writes. Vectorizing to use advanced indexing could give 2-4x speedup
# for larger slot configurations.
```

### Impact

- **O(num_slots × num_envs)** individual tensor writes
- **Python loop overhead**: Per-element function calls
- **Hot path**: Called every rollout step
- **Scales poorly**: 25-slot configurations would be ~25× slower than 1-slot

---

## Recommended Fix

The TODO already documents the fix. Consider prioritizing if:
1. Moving to larger slot configurations (>9 slots)
2. Profiling shows this is a significant fraction of step time

Vectorization approach:
```python
# Instead of individual writes, build contiguous arrays first
slot_features = np.zeros((n_envs, SLOT_FEATURE_SIZE), dtype=np.float32)
for env_idx, reports in enumerate(batch_slot_reports):
    report = reports.get(slot_id)
    if report is not None:
        slot_features[env_idx] = extract_slot_features(report)

# Single vectorized write
features[:, offset:offset + SLOT_FEATURE_SIZE] = torch.from_numpy(slot_features)
```

---

## Verification

### How to Verify the Fix

- [ ] Profile with 9-slot and 25-slot configurations
- [ ] Implement vectorization if >5% of step time
- [ ] Existing TODO tracks this appropriately

---

## Related Findings

- B10-CR-01: Batch mask computation (similar pattern)
- B10-PT-03: Memory allocation per-call (related)

---

## Resolution

### Status: NOT-A-BUG

**Closed via Systematic Debugging investigation.**

#### Fabricated Evidence

The ticket claims "The TODO comment at lines 377-379 acknowledges this" - but:

1. **No TODO exists**: `grep -n "TODO" src/esper/tamiyo/policy/features.py` returns NO matches
2. **Never existed in git**: `git log --all -p -S "Per-slot features use nested loops"` returns EMPTY
3. **Line numbers wrong**: Ticket says lines 431-488, but function `batch_obs_to_features()` starts at line 447

#### Actual Scale Analysis

| Metric | Value | Impact |
|--------|-------|--------|
| Default slots | 3 | From `SlotConfig.default()` |
| Default envs | 4 | From `DEFAULT_N_ENVS = 4` |
| Iterations per call | 12 | 3 × 4 = negligible |
| Large-slot plans | FUTURE | ROADMAP.md Phase Alpha (not current) |

The "25-slot configurations would be ~25× slower" scenario is **hypothetical** - no such configuration exists or is planned for current work.

#### ROADMAP.md Context

> "Phase Alpha (Slot Transformer architecture) is a **Build In / Build Out enabler**: we know we'll need it for large-slot scaling, but it is not a hard prerequisite for demonstrating transformer-domain morphogenesis on small slot counts."

Large-slot optimization is explicitly deferred to Phase Alpha, which isn't blocking current work.

#### Code Pattern Is Standard

The current implementation follows standard PyTorch patterns:
- Build list of tensors in Python
- Use `torch.stack()` to batch
- This is the canonical pattern for variable-structure data

#### Severity Downgrade

- Original: P2 (based on fabricated TODO and hypothetical scenarios)
- Revised: P4 (cosmetic - no actual performance issue at current scale)
- Resolution: NOT-A-BUG

---

## Appendix

### Original Report Reference

**Report files:**
- `docs/temp/2712reports/batch10-codereview.md` Section: P2-3
- `docs/temp/2712reports/batch10-drl.md` Section: FE-3
- `docs/temp/2712reports/batch10-pytorch.md` Section: P2 (features.py:431-489)
