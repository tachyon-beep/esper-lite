# Finding Ticket: initial_hidden() Returns Inference-Mode Tensors

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-PT-01` |
| **Severity** | `P1` |
| **Status** | `closed` |
| **Batch** | 10 |
| **Agent** | `pytorch` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-29 |

---

## Resolution

**Status:** NOT A BUG

**Root Cause Analysis:**

The concern was valid - inference-mode tensors CANNOT be used in autograd:
```
RuntimeError: Inference tensors cannot be saved for backward.
```

However, the RolloutBuffer storage pattern **neutralizes this risk**:

1. Buffer pre-allocates regular tensors at initialization:
   ```python
   self.hidden_h = torch.zeros(n, m, self.lstm_layers, self.lstm_hidden_dim)
   ```

2. Assignment copies VALUES without the inference_mode flag:
   ```python
   self.hidden_h[env_id, step_idx] = hidden_h.detach().squeeze(1)
   ```

3. PyTorch behavior verified:
   ```python
   buffer = torch.zeros(1, 2, 20)  # is_inference() = False
   with torch.inference_mode():
       h0 = torch.zeros(1, 2, 20)  # is_inference() = True
   buffer[:] = h0.detach()
   print(buffer.is_inference())    # False! Values copied, not flag
   ```

**Why No Defensive Code Added:**

Per CLAUDE.md "No Bug-Hiding Patterns" policy:
- The buffer pattern is well-defined and cannot receive inference tensors at training
- Adding `.clone()` in `evaluate_actions()` would be wasteful and misleading
- If someone bypassed the buffer, they'd get a loud `RuntimeError`, not silent failure

**Documentation Fix Applied:**

Updated `initial_hidden()` docstring to accurately explain the buffer safety pattern
instead of the previous misleading claim that "passing these tensors will NOT cause errors".

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/lstm_bundle.py` |
| **Line(s)** | `261-277` |
| **Function/Class** | `LSTMPolicyBundle.initial_hidden()` |

---

## Summary

**One-line summary:** `initial_hidden()` decorated with `@torch.inference_mode()` returns non-differentiable tensors that silently won't contribute gradients if misused.

**Resolution:** Not a bug - the RolloutBuffer storage pattern removes the inference_mode property before tensors reach training code.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [x] Documentation / naming

---

## Detailed Description

### Original Concern

```python
@torch.inference_mode()
def initial_hidden(self, batch_size: int) -> HiddenState:
    """Get initial LSTM hidden state for rollout collection.
    ...
    """
```

The `@torch.inference_mode()` decorator returns tensors with `is_inference()=True`.
If these were passed directly to `evaluate_actions()`, it would cause `RuntimeError`.

### Why It Works

The data flow through the buffer removes the inference_mode property:

```
initial_hidden() [@inference_mode]
    → tensors with is_inference()=True
    → RolloutBuffer.add() assigns to pre-allocated buffer
    → buffer tensors remain is_inference()=False (values copied, not flag)
    → get_batched_sequences() returns regular tensors
    → evaluate_actions() receives gradient-compatible hidden states
    → backward() works correctly
```

### Risk Assessment

| Risk | Level | Notes |
|------|-------|-------|
| Current correctness | SAFE | Pattern works as designed |
| Fragility | MODERATE | Relies on implicit PyTorch behavior |
| Future breakage | LOW | Buffer pre-allocation is documented requirement for torch.compile |

---

## Cross-Review (PyTorch Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `NOT A BUG` |
| **Reviewer** | PyTorch Expert Agent |

**Evaluation:** The analysis is correct. Assigning inference-mode tensors to pre-allocated regular tensors copies *data* only - the target tensor's properties (including `is_inference()`) are preserved. The buffer storage pattern is intentional for torch.compile compatibility, making accidental removal unlikely. Adding defensive `.clone()` would violate the "No Bug-Hiding Patterns" policy as it defends against an impossible scenario.

---

## Appendix

### Original Report Reference

**Report files:**
- `docs/temp/2712reports/batch10-pytorch.md` Section: P1 (lstm_bundle.py:261-275)
- `docs/temp/2712reports/batch10-drl.md` Section: LB-1
