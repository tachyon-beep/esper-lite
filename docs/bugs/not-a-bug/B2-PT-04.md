# Finding Ticket: Unconditional Slot Iteration in MorphogeneticModel.to()

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B2-PT-04` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 2 |
| **Agent** | `pytorch` |
| **Domain** | `kasmina` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/kasmina/host.py` |
| **Line(s)** | `526-542` |
| **Function/Class** | `MorphogeneticModel.to()` |

---

## Summary

**One-line summary:** `.to()` iterates all seed_slots unconditionally even when device hasn't changed, causing wasteful operations.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The `MorphogeneticModel.to()` method updates `_device` and iterates all slot devices unconditionally. For multi-device scenarios (mixed CPU/GPU), this is correct. But for same-device moves (e.g., `model.to('cuda')` when already on CUDA), it's wasteful.

### Code Evidence

```python
# /home/john/esper-lite/src/esper/kasmina/host.py:526-542

def to(self, device: torch.device | str = ..., ...) -> "MorphogeneticModel":
    device_obj = torch.device(device) if isinstance(device, str) else device
    self._device = device_obj  # Always updates

    # Calls parent .to() which moves all parameters
    result = super().to(device, dtype=dtype, **kwargs)

    # Then iterates all slots (even if they're already on correct device)
    for slot in self.seed_slots.values():
        slot.to(device_obj, dtype=dtype)

    return result
```

### Why This Matters

In training scripts, `.to('cuda')` may be called multiple times (e.g., after loading checkpoints, after moving between devices for profiling). Each call iterates all slots unnecessarily.

---

## Recommended Fix

Add early exit when target device matches current:

```python
def to(self, device: torch.device | str = ..., ...) -> "MorphogeneticModel":
    device_obj = torch.device(device) if isinstance(device, str) else device

    # Early exit if already on target device (and no dtype change)
    if device_obj == self._device and dtype is None:
        return self

    self._device = device_obj
    # ... rest of method
```

---

## Verification

### How to Verify the Fix

- [ ] Test multiple `.to('cuda')` calls don't iterate slots
- [ ] Verify device state is correctly tracked after moves
- [ ] Test mixed CPU/GPU scenarios still work

---

## Related Findings

| Ticket ID | Relationship | Notes |
|-----------|--------------|-------|
| `B2-PT-01` | `related` | Device move also affects cache clearing |
| `B2-DRL-12` | `related` | Complex StopIteration handling in same method |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch2-pytorch.md`
**Section:** "File-by-File Analysis" - host.py - P2 Line 526-542

---

## Cross-Review (DRL Specialist)

**Verdict:** NEUTRAL

The `.to()` method is called infrequently (model init, checkpoint loads), not in the hot training loop. Early exit is clean but provides negligible RL training speedup. Prioritize fixes that impact per-step latency over per-episode setup costs.

---

## Cross-Review (PyTorch Specialist)

**Verdict:** NEUTRAL

The actual code (L526-542) iterates slots to update `.device` attribute tracking, not to move tensors. The `super().to()` already handles parameter movement. Slot iteration is O(n_slots) attribute assignment, not tensor ops. Optimization would save microseconds; acceptable but low-value.

---

## Cross-Review (Code Review Specialist)

| Field | Value |
|-------|-------|
| **Verdict** | `OBJECT` |
| **Reviewer** | Code Review Specialist |

**Evaluation:** The code evidence is fabricated. The actual `to()` implementation (L526-542) calls `super().to()` then queries the resulting device from `next(self.parameters()).device` and updates slot device tracking via property assignment (`slot.device = actual_device`). It does NOT call `slot.to(device_obj, dtype=dtype)` in a loop as claimed. The real implementation correctly delegates to PyTorch's built-in parameter movement and only updates metadata; the suggested "fix" would break the actual working code.
