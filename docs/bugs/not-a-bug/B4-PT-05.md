# Finding Ticket: step_counts Is Python List Instead of Tensor

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B4-PT-05` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 4 |
| **Agent** | `pytorch` |
| **Domain** | `simic` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/simic/agent/rollout_buffer.py` |
| **Line(s)** | Multiple |
| **Function/Class** | `TamiyoRolloutBuffer` |

---

## Summary

**One-line summary:** `step_counts` is a Python list, not a tensor - prevents vectorized operations and requires CPU-GPU round trips.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

The buffer tracks step counts as a Python list:

```python
self.step_counts: list[int] = [0] * num_envs
```

When building `valid_mask` in `get_batched_sequences()`:

```python
# Line ~458
step_counts_tensor = torch.tensor(self.step_counts, device=device)
```

This creates a new tensor on every call, causing:
1. Python list → tensor conversion overhead
2. Potential CPU→GPU synchronization

### Current Impact

Low - `get_batched_sequences()` is called once per update, and the list is small (4 integers).

---

## Recommended Fix

Use a pre-allocated tensor:

```python
# In __init__
self.step_counts = torch.zeros(num_envs, dtype=torch.int32, device='cpu')

# When adding a step
self.step_counts[env_idx] += 1

# When building valid_mask
step_counts_tensor = self.step_counts.to(device, non_blocking=True)
```

Or keep as list but cache the tensor:

```python
def get_step_counts_tensor(self, device: torch.device) -> torch.Tensor:
    if self._step_counts_tensor is None or self._step_counts_dirty:
        self._step_counts_tensor = torch.tensor(
            self.step_counts, device=device, dtype=torch.int32
        )
        self._step_counts_dirty = False
    return self._step_counts_tensor
```

---

## Verification

### How to Verify the Fix

- [ ] Profile to confirm impact
- [ ] If needed, change to tensor or add caching
- [ ] Run tests to verify functionality

---

## Related Findings

None.

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch4-pytorch.md`
**Section:** "P3 (Code Quality)" (BUF-2)

---

## Cross-Review: PyTorch Specialist

| Verdict | NEUTRAL |
|---------|---------|

**Evaluation:** Python list for 4-element step_counts is negligible overhead at O(1) per rollout. Converting to tensor would complicate add() with index tensor ops. The current torch.tensor() call in get_batched_sequences() is dominated by the 30+ tensor.to() transfers that follow it.

---

## Cross-Review: DRL Specialist

| Verdict | NEUTRAL |
|---------|---------|

**Evaluation:** The Python list for step_counts is idiomatic for metadata that changes incrementally (step_counts[env_id] += 1). The torch.tensor() call in get_batched_sequences() is negligible overhead (4 integers, once per rollout). Tensor-ifying would add complexity for increments. No RL training impact - premature optimization.

---

## Cross-Review: Code Review Specialist

| Verdict | NEUTRAL |
|---------|---------|

**Evaluation:** Valid observation but the overhead of 4-element list-to-tensor conversion is negligible; optimization would add complexity without measurable benefit.
