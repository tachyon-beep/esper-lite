# Finding Ticket: Batch Mask Computation Allocates Many Small Tensors

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B10-CR-01` |
| **Severity** | `P2` |
| **Status** | `open` |
| **Batch** | 10 |
| **Agent** | `codereview` |
| **Domain** | `tamiyo/policy` |
| **Assignee** | |
| **Created** | 2025-12-27 |
| **Updated** | 2025-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/src/esper/tamiyo/policy/action_masks.py` |
| **Line(s)** | `282-300` |
| **Function/Class** | `compute_batch_masks()` |

---

## Summary

**One-line summary:** For each environment, individual masks are computed then stacked - creates many small tensors for large batches.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [x] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [ ] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Wrong

```python
# Lines 282-300
masks_list = [
    compute_action_masks(
        slot_states=slot_states,
        slot_config=slot_config,
        device=device,
    )
    for i, slot_states in enumerate(batch_slot_states)
]
return {
    key: torch.stack([m[key] for m in masks_list])
    for key in masks_list[0]
}
```

For each environment in the batch:
1. `compute_action_masks` creates individual mask tensors
2. All masks are collected in a list
3. Then stacked per-key

For 128 environments with 8 action heads, this creates 128 * 8 = 1024 small tensors before stacking.

### Impact

- **Memory fragmentation**: Many small allocations
- **GC pressure**: More objects for garbage collection
- **Hot path**: Called once per rollout step

The comment notes this is "once per rollout step" so impact is bounded, but for high-frequency training this adds up.

---

## Recommended Fix

Pre-allocate batch tensors and fill directly:

```python
def compute_batch_masks(
    batch_slot_states: list[list[SlotMaskState | None]],
    slot_config: SlotConfig,
    device: torch.device | str = "cpu",
) -> ActionMasks:
    batch_size = len(batch_slot_states)

    # Pre-allocate batch tensors
    batch_masks = {
        "slot": torch.zeros(batch_size, slot_config.max_slots, dtype=torch.bool, device=device),
        "blueprint": torch.zeros(batch_size, NUM_BLUEPRINTS, dtype=torch.bool, device=device),
        # ... etc
    }

    # Fill directly
    for i, slot_states in enumerate(batch_slot_states):
        single_masks = compute_action_masks(slot_states, slot_config, device)
        for key, mask in single_masks.items():
            batch_masks[key][i] = mask

    return batch_masks
```

Or add TODO comment for future optimization:

```python
# TODO: [FUTURE OPTIMIZATION] - Pre-allocate batch tensors to reduce small tensor allocations
```

---

## Verification

### How to Verify the Fix

- [ ] Profile current allocation pattern
- [ ] Implement pre-allocation or add TODO
- [ ] Benchmark before/after with batch_size=128

---

## Related Findings

- B10-CR-02: Per-slot nested loops in features.py (similar pattern)

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch10-codereview.md`
**Section:** P2-2
