# Finding Ticket: No Integration Test for Multi-Stream Rollback Safety

---

## Ticket Metadata

| Field | Value |
|-------|-------|
| **Ticket ID** | `B1-PT-05` |
| **Severity** | `P3` |
| **Status** | `open` |
| **Batch** | 1 |
| **Agent** | `pytorch` |
| **Domain** | `tolaria` |
| **Assignee** | |
| **Created** | 2024-12-27 |
| **Updated** | 2024-12-27 |

---

## Location

| Field | Value |
|-------|-------|
| **File(s)** | `/home/john/esper-lite/tests/tolaria/test_governor.py` |
| **Line(s)** | N/A - missing test |
| **Function/Class** | N/A |

---

## Summary

**One-line summary:** No test verifies rollback safety when multiple CUDA streams are operating on the model simultaneously.

**Category:**
- [ ] Correctness bug
- [ ] Race condition / concurrency
- [ ] Memory leak / resource issue
- [ ] Performance bottleneck
- [ ] Numerical stability
- [ ] torch.compile compatibility
- [ ] Dead code / unwired functionality
- [ ] API design / contract violation
- [x] Test coverage gap
- [ ] Documentation / naming
- [ ] Defensive programming violation
- [ ] Legacy code policy violation

---

## Detailed Description

### What's Missing

The test suite for `TolariaGovernor` doesn't include a test that exercises rollback behavior when multiple CUDA streams are active. The code uses `torch.cuda.synchronize(device)` (full device sync) which should be safe, but this hasn't been verified under concurrent stream conditions.

### Why This Matters

1. **Race Condition Risk:** If rollback occurs while another stream is modifying model parameters, data corruption could occur
2. **Current Mitigation:** `torch.cuda.synchronize(device)` blocks ALL streams, which should prevent races
3. **Verification Gap:** We assume the sync is sufficient but haven't tested it

### System Context

In production, Esper may have:
- Data loading on one stream
- Model forward pass on another
- Gradient computation on default stream
- Custom CUDA kernels on yet another

If a panic triggers rollback while these are in flight, we need to ensure safety.

---

## Recommended Fix

### Suggested Test

```python
@pytest.mark.cuda
@pytest.mark.skipif(not torch.cuda.is_available(), reason="CUDA required")
def test_rollback_multi_stream_safety():
    """Verify rollback is safe when multiple CUDA streams are active."""
    device = torch.device("cuda:0")
    model = nn.Sequential(
        nn.Linear(100, 100),
        nn.ReLU(),
        nn.Linear(100, 10)
    ).to(device)
    optimizer = torch.optim.Adam(model.parameters())
    governor = TolariaGovernor(model)

    # Snapshot clean state
    governor.snapshot()

    # Create secondary stream
    secondary_stream = torch.cuda.Stream(device)

    # Start async operation on secondary stream
    with torch.cuda.stream(secondary_stream):
        # Long-running operation that might be in-flight during rollback
        large_tensor = torch.randn(1000, 1000, device=device)
        for _ in range(10):
            large_tensor = large_tensor @ large_tensor.T

    # Trigger rollback while secondary stream may still be running
    # This should NOT corrupt the model
    report = governor.execute_rollback(env_id=0, optimizer=optimizer)

    # Verify model is functional after rollback
    test_input = torch.randn(4, 100, device=device)
    output = model(test_input)

    # Should not contain NaN/Inf
    assert not torch.isnan(output).any()
    assert not torch.isinf(output).any()

    # Verify rollback report
    assert report.success
```

---

## Verification

### How to Verify the Fix

- [ ] Add test to `tests/tolaria/test_governor.py`
- [ ] Run on GPU CI to verify CUDA behavior
- [ ] Consider stress test with many concurrent streams

---

## Related Findings

| Ticket ID | Relationship | Notes |
|-----------|--------------|-------|
| `B1-PT-01` | `related` | Optimizer state after rollback |
| `B1-PT-02` | `related` | Non-blocking transfer synchronization |

---

## Cross-Review

| Agent | Verdict | Evaluation |
|-------|---------|------------|
| **DRL** | ENDORSE | CUDA stream race conditions during rollback could cause silent model corruption, leading to policy collapse or reward degradation that would be extremely difficult to diagnose. Multi-stream safety is critical infrastructure for RL training robustness; if a panic-triggered rollback corrupts weights mid-trajectory, gradients and advantages become meaningless. |
| **PyTorch** | ENDORSE | Valid test coverage gap - multi-stream rollback safety is a real CUDA correctness concern that could mask latent races. The proposed test correctly exercises concurrent streams and validates torch.cuda.synchronize() behavior under rollback conditions. |
| **CodeReview** | ENDORSE | This test coverage gap is legitimate - multi-stream CUDA behavior during rollback is a real race condition risk that warrants verification. The proposed test is well-designed, and the `torch.cuda.synchronize(device)` assumption should indeed be validated under concurrent stream conditions. |

---

## Appendix

### Original Report Reference

**Report file:** `docs/temp/2712reports/batch1-pytorch.md`
**Section:** "Cross-Cutting Integration Risks" - "Multi-Stream Safety During Rollback"
