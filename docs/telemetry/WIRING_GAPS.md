# Telemetry Wiring Gaps

> **Last Updated:** 2026-01-04
> **Generated by:** TELE record audit
> **Latest fix:** TELE-220 to TELE-228 (value function metrics) - wired into PPO update loop

This document tracks all xfailing telemetry tests that document known wiring gaps. Each entry explains what's missing to complete the data flow from emitter → aggregator → snapshot.

---

## Summary

| Range | Category | Xfails | Root Cause |
|-------|----------|--------|------------|
| TELE-600 to TELE-610 | Environment Metrics | 7 | Emitters not implemented |
| TELE-701 to TELE-730 | Infrastructure Metrics | 9 | Schema-only tests, need integration tests |

**Total: 16 xfailing tests**

---

## Training Progress: ALL FIXED ✓

**File:** `tests/telemetry/test_training_metrics.py`

All wiring gaps in this category have been resolved:

> **Fixed:**
> - TELE-001 (task_name) - wiring tests verify aggregator→snapshot flow
> - TELE-010 (current_episode) - wiring tests verify batch updates and start_episode
> - TELE-011 (current_epoch) - wiring tests verify epoch updates and reset on TRAINING_STARTED
> - TELE-012 (max_epochs) - wiring tests verify TRAINING_STARTED sets max_epochs
> - TELE-013 (current_batch) - wiring tests verify batch updates and fallback to batches_completed
> - TELE-014 (max_batches) - wiring tests verify TRAINING_STARTED sets max_batches
> - TELE-020 (runtime_seconds) - wiring tests verify computed metric from _start_time
> - TELE-021 (episode_return_history) - wiring tests verify accumulation from batch events

---

## TELE-220 to TELE-228: Value Function Metrics - ALL FIXED ✓

**File:** `tests/telemetry/test_tele_value_metrics.py`

All wiring gaps in this category have been resolved:

> **Fixed (2026-01-04):**
> - TELE-220 (v_return_correlation) - computed from value predictions vs actual returns
> - TELE-221 (td_error_mean) - computed from TD errors stored during GAE
> - TELE-222 (td_error_std) - computed from TD errors stored during GAE
> - TELE-223 (bellman_error) - computed as MAE of TD errors
> - TELE-224 (return_p10) - computed from return distribution percentiles
> - TELE-225 (return_p50) - computed from return distribution percentiles
> - TELE-226 (return_p90) - computed from return distribution percentiles
> - TELE-227 (return_variance) - computed from return distribution
> - TELE-228 (return_skewness) - computed from return distribution
>
> **Implementation:** `src/esper/simic/telemetry/emitters.py` → `compute_value_function_metrics()` called from `emit_ppo_update_event()`

---

## TELE-300 to TELE-340: Gradient Metrics - ALL FIXED ✓

**File:** `tests/telemetry/test_gradient_metrics.py`

All wiring gaps in this category have been resolved:

> **Fixed:**
> - TELE-301 (inf_grad_count) - `aggregate_layer_gradient_health()` now sums `inf_count` from LayerGradientStats and returns it; `emit_ppo_update_event()` reads from metrics dict instead of hardcoding 0
> - TELE-340 (lstm_health) - wired into PPO update loop

---

## TELE-600 to TELE-610: Environment Metrics

**File:** `tests/telemetry/test_environment_metrics.py`

Observation statistics and episode metrics have schema but no emitters.

| TELE | Metric | Gap | Fix Required |
|------|--------|-----|--------------|
| TELE-600 | `obs_nan_count` | No emitter | Add to `EpochCompletedPayload` or new `OBSERVATION_STATS` event |
| TELE-601 | `obs_inf_count` | No emitter | Same as TELE-600 |
| TELE-602 | `outlier_pct` | Stubbed to 0.0 | Compute outlier percentage in observation tensor |
| TELE-603 | `normalization_drift` | Stubbed to 0.0 | Track running mean/std drift over time |
| TELE-610 | `episode_stats.avg_length` | Stubbed to 0 | Compute from episode lengths in batch |
| TELE-610 | `episode_stats.success_rate` | Stubbed to 0.0 | Compute success/total ratio |
| TELE-610 | `episode_stats.avg_actions_per_episode` | Stubbed to 0.0 | Compute action count / episodes |

---

## TELE-701 to TELE-730: Infrastructure Metrics

**File:** `tests/telemetry/test_infrastructure_metrics.py`

These are aggregator-computed metrics. Tests verify schema exists but don't test the aggregator computation.

| TELE | Metric | Gap | Fix Required |
|------|--------|-----|--------------|
| TELE-701 | `connected` | Schema-only test | Integration test: aggregator sets `True` on TRAINING_STARTED |
| TELE-702 | `staleness_seconds` | Schema-only test | Integration test: verify computed from `time.monotonic() - _last_event_time` |
| TELE-703 | `training_thread_alive` | Schema-only test | Integration test: verify `Thread.is_alive()` check in SanctumApp |
| TELE-710 | `epochs_per_second` | Schema-only test | Integration test: verify computed from EPOCH_COMPLETED events |
| TELE-711 | `batches_per_hour` | Schema-only test | Integration test: verify computed from BATCH_COMPLETED events |
| TELE-720 | `cpu_percent` | Schema-only test | Integration test: verify `psutil.cpu_percent()` collection |
| TELE-721 | `ram_usage` | Schema-only test | Integration test: verify `psutil.virtual_memory()` collection |
| TELE-730 | `gpu_memory_usage` | Schema-only test | Integration test: verify `torch.cuda.memory_allocated()` collection |

**Note:** TELE-740/741 (memory alarms) and TELE-750/760/770 (group_id, compile_enabled, memory_usage_percent) have VALID tests that test real data flow.

---

## How to Use This Document

### When Fixing a Wiring Gap

1. Find the TELE record in this document
2. Read the "Gap" column to understand what's missing
3. Read the "Fix Required" column for implementation guidance
4. After implementing, remove the `@pytest.mark.xfail` marker from the test
5. Verify the test passes
6. Remove the entry from this document

### When Adding New Telemetry

1. Create the TELE record in `docs/telemetry/telemetry_needs/`
2. Add a test that verifies REAL DATA FLOW (not just schema)
3. If emitter isn't ready, mark the test `@pytest.mark.xfail` with clear reason
4. Add an entry to this document

### Test Categories

**Transport Tests (VALID):** Create payload → emit through hub → verify arrives at backend
- These test NissaHub routing, not full wiring
- Useful for verifying payload schema

**Wiring Tests (what we need):** Trigger real emitter OR process event through aggregator → verify snapshot contains non-default values
- These test the actual production data flow
- Mark xfail if emitter/aggregator isn't implemented

---

## Quick Reference: Test Files

| File | TELE Range | Status |
|------|------------|--------|
| `test_training_metrics.py` | TELE-001 to TELE-021 | ✓ All 8 wiring gaps fixed |
| `test_policy_metrics.py` | TELE-100 to TELE-170 | ✓ All valid |
| `test_value_metrics.py` | TELE-200 to TELE-214 | ✓ All valid |
| `test_tele_value_metrics.py` | TELE-220 to TELE-228 | ✓ All 9 wiring gaps fixed |
| `test_gradient_metrics.py` | TELE-300 to TELE-340 | ✓ All valid |
| `test_reward_metrics.py` | TELE-400 to TELE-402 | ✓ All valid |
| `test_seed_lifecycle.py` | TELE-500 to TELE-515 | ✓ All valid |
| `test_environment_metrics.py` | TELE-600 to TELE-650 | 7 xfails (no emitters) |
| `test_infrastructure_metrics.py` | TELE-700 to TELE-770 | 9 xfails (schema-only) |
| `test_decision_metrics.py` | TELE-800+ | ✓ All valid |
| `test_tele_action_targeting.py` | TELE-801, TELE-802 | ✓ All valid |
