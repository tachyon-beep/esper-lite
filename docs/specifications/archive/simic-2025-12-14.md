---
id: "simic"
title: "Simic - RL Training Infrastructure"
aliases: ["ppo", "reinforcement-learning", "simic-gym"]
biological_role: "Gym (Simulator)"
layer: "Core Logic"
criticality: "Tier-1"
tech_stack:
  - "Python 3.11+"
  - "PyTorch 2.x"
  - "CUDA"
primary_device: "cuda"
thread_safety: "unsafe"
owners: "core-team"
compliance_tags:
  - "GPU-Resident"
  - "No-Blocking-Calls"
  - "Gradient-Aware"
  - "Vectorized-Parallel"
schema_version: "1.0"
last_updated: "2025-12-14"
last_reviewed_commit: "4e8740f"
---

# Simic Bible

# 1. Prime Directive

**Role:** Provides PPO-based reinforcement learning infrastructure for training Tamiyo (the seed lifecycle controller) by simulating parallel training environments and computing shaped rewards from counterfactual validation.

**Anti-Scope:** Does NOT make strategic decisions about seed lifecycle (that's Tamiyo's job), does NOT interact with the host model directly (all host operations go through Tolaria), does NOT implement the neural policy network (that's in tamiyo_network.py), and does NOT execute actual model training (that's Tolaria's job - Simic only simulates it).

---

# 2. Interface Contract

## 2.1 Entry Points (Public API)

### `train_ppo_vectorized(n_episodes: int, n_envs: int, ...) -> tuple[PPOAgent, list[dict]]`
> Vectorized PPO training loop using INVERTED CONTROL FLOW for maximum GPU utilization.

- **Invariants:**
  - `n_envs >= 1` (number of parallel environments)
  - `max_epochs >= 1` (episode length in RL timesteps)
  - `slots` is not None and not empty (multi-slot control required)
  - `devices` contains valid CUDA device strings or defaults to `[device]`
- **Raises:** `ValueError` if slots is empty
- **Complexity:** O(n_episodes × n_envs × max_epochs × batch_size)
- **Device:** Policy on `device`, environments distributed across `devices`

### `PPOAgent`
> Factored recurrent PPO agent with per-head policy outputs and LSTM temporal reasoning.

**Constructor:**
- `__init__(state_dim: int, action_dim: int, lr: float, gamma: float, ...)`

**Key Methods:**
- `update(clear_buffer: bool) -> dict` - PPO update with GAE, returns training metrics
- `save(path: Path, metadata: dict)` - Serialize agent with config and normalizer state
- `load(path: Path, device: str) -> PPOAgent` - Deserialize from checkpoint
- `get_entropy_coef(action_mask: Tensor | None) -> float` - Get current entropy coefficient with optional adaptive floor

### `compute_reward(...) -> float | tuple[float, RewardComponentsTelemetry]`
> Unified reward dispatcher supporting SHAPED, SPARSE, and MINIMAL reward modes.

- **Invariants:**
  - `epoch` in range `[1, max_epochs]`
  - `val_acc` in range `[0, 100]` (percentage)
  - `action` is a valid `LifecycleOp` enum member
- **Returns:** Scalar reward or (reward, components) tuple if `return_components=True`

### `signals_to_features(signals, model, ...) -> list[float]`
> Convert training signals to 50-dimensional feature vector (or 60 with telemetry).

- **Invariants:**
  - `slots` is not None and not empty
  - Returns exactly `MULTISLOT_FEATURE_SIZE` (50) or +10 if `use_telemetry=True`

## 2.2 Configuration Schema

```python
@dataclass
class TrainingConfig:
    # PPO Core
    lr: float = 3e-4
    gamma: float = 0.995  # Optimized for 25-epoch episodes
    gae_lambda: float = 0.97
    clip_ratio: float = 0.2
    value_coef: float = 0.5
    max_grad_norm: float = 0.5
    n_epochs: int = 10
    batch_size: int = 64

    # Entropy (Exploration)
    entropy_coef: float = 0.05
    entropy_coef_min: float = 0.01
    adaptive_entropy_floor: bool = False
    entropy_anneal_episodes: int = 0

    # KL Early Stopping
    target_kl: float | None = 0.015

    # Sample Efficiency
    ppo_updates_per_batch: int = 1  # Multiple updates per rollout

    # Training Scale
    n_episodes: int = 100
    n_envs: int = 4
    max_epochs: int = 25

    # LSTM
    lstm_hidden_dim: int = 128
    chunk_length: int = 25  # Must match max_epochs
```

```python
@dataclass
class ContributionRewardConfig:
    # Primary signal: counterfactual validation
    contribution_weight: float = 1.0
    proxy_contribution_weight: float = 0.3  # Pre-blending fallback

    # PBRS stage progression
    pbrs_weight: float = 0.3
    gamma: float = 0.995

    # Compute rent (logarithmic)
    rent_weight: float = 0.5
    max_rent: float = 8.0

    # Enforcement penalties
    invalid_fossilize_penalty: float = -1.0
    cull_fossilized_penalty: float = -1.0

    # Intervention costs
    germinate_cost: float = -0.02
    fossilize_cost: float = -0.01
    cull_cost: float = -0.005

    # Terminal bonus
    terminal_acc_weight: float = 0.05
    fossilize_terminal_scale: float = 3.0

    # Experiment mode
    reward_mode: RewardMode = RewardMode.SHAPED
```

## 2.3 Events (Pub/Sub via Karn/Nissa)

### Emits
| Event | Trigger | Payload |
|-------|---------|---------|
| `TRAINING_STARTED` | Beginning of batch | `{batch, n_envs, seed, task, max_epochs}` |
| `EPOCH_COMPLETED` | After each RL epoch | `{env_idx, epoch, train_loss, train_accuracy, val_loss, val_accuracy}` |
| `PPO_UPDATE_COMPLETED` | After PPO update | `{policy_loss, value_loss, entropy, approx_kl, ratio_max, explained_variance, ...}` |
| `REWARD_COMPUTED` | Per-step reward calculation | `{action_name, total_reward, seed_contribution, bounded_attribution, ...}` |
| `COUNTERFACTUAL_COMPUTED` | After validation with alpha=0 | `{slot_id, real_accuracy, baseline_accuracy, contribution}` |
| `GOVERNOR_ROLLBACK` | Catastrophic failure detected | `{reason, loss_at_panic, consecutive_panics}` |
| `BATCH_COMPLETED` | After n_envs episodes | `{episodes_completed, avg_accuracy, rolling_accuracy}` |

### Subscribes
| Event | Handler | Action |
|-------|---------|--------|
| None | N/A | Simic is a telemetry producer, not consumer |

---

# 3. Tensor Contracts

## 3.1 Input Tensors

| Name | Shape | Dtype | Device | Description |
|------|-------|-------|--------|-------------|
| `states_batch` | `[n_envs, state_dim]` | `float32` | policy_device | Normalized observation features (50 or 60 dims) |
| `slot_mask` | `[n_envs, NUM_SLOTS]` | `bool` | policy_device | Valid slot selections (enabled slots only) |
| `blueprint_mask` | `[n_envs, NUM_BLUEPRINTS]` | `bool` | policy_device | Valid blueprint selections (NOOP always False) |
| `blend_mask` | `[n_envs, NUM_BLENDS]` | `bool` | policy_device | Valid blend algorithm selections |
| `op_mask` | `[n_envs, NUM_OPS]` | `bool` | policy_device | Valid lifecycle operations (WAIT always True) |
| `hidden_h` | `[lstm_layers, n_envs, hidden_dim]` | `float32` | policy_device | LSTM hidden state (h) |
| `hidden_c` | `[lstm_layers, n_envs, hidden_dim]` | `float32` | policy_device | LSTM cell state (c) |

## 3.2 Output Tensors

| Name | Shape | Dtype | Device | Description |
|------|-------|-------|--------|-------------|
| `slot_logits` | `[n_envs, NUM_SLOTS]` | `float32` | policy_device | Raw slot selection logits |
| `blueprint_logits` | `[n_envs, NUM_BLUEPRINTS]` | `float32` | policy_device | Raw blueprint selection logits |
| `blend_logits` | `[n_envs, NUM_BLENDS]` | `float32` | policy_device | Raw blend algorithm logits |
| `op_logits` | `[n_envs, NUM_OPS]` | `float32` | policy_device | Raw operation selection logits |
| `values` | `[n_envs]` | `float32` | policy_device | Value function estimates V(s) |
| `actions[key]` | `[n_envs]` | `int64` | policy_device | Sampled action indices per head |
| `log_probs[key]` | `[n_envs]` | `float32` | policy_device | Log probabilities per head |

## 3.3 Internal Buffers

| Name | Shape | Lifetime | Purpose |
|------|-------|----------|---------|
| `states` | `[num_envs, max_steps_per_env, state_dim]` | Episode batch | Pre-allocated rollout storage |
| `advantages` | `[num_envs, max_steps_per_env]` | Episode batch | GAE advantages per env |
| `returns` | `[num_envs, max_steps_per_env]` | Episode batch | TD-lambda returns per env |
| `hidden_h/c` | `[num_envs, max_steps, lstm_layers, hidden_dim]` | Episode batch | LSTM state history for BPTT |
| `train_loss_accum` | `[1]` | Epoch | GPU-resident accumulator (avoids .item() sync) |
| `cf_correct_accums[slot_id]` | `[1]` | Epoch | Per-slot counterfactual accumulators |

## 3.4 Gradient Flow

```
[Observation] → [RunningMeanStd.normalize()] → [Normalized State]
                                                        ↓
                    [FactoredRecurrentActorCritic.forward()]
                                                        ↓
            [feature_net] → [LSTM] → [lstm_ln] → [shared_repr]
                                                        ↓
                    ┌───────────────┬─────────────┬─────────────┐
                    ↓               ↓             ↓             ↓
                slot_head    blueprint_head   blend_head    op_head
                    ↓               ↓             ↓             ↓
                (logits)        (logits)      (logits)      (logits)
                    └───────────────┴─────────────┴─────────────┘
                                        ↓
                            [PPO Loss Computation]
                                        ↓
                            policy_loss + value_loss + entropy_loss
                                        ↓
                            [Optimizer.step()]

Frozen: None (all params trainable)
Detached: Rollout buffers store .detach() to prevent graph accumulation
```

**LSTM BPTT:** Hidden states stored in buffer enable proper backpropagation through time during `evaluate_actions()`. The buffer stores PRE-STEP hidden states (input to network when action was selected) for exact reconstruction.

---

# 4. Operational Physics

## 4.1 State Machine

### PPOAgent Training Lifecycle

```
[INIT] --(first rollout)--> [COLLECTING]
[COLLECTING] --(buffer full)--> [COMPUTING_GAE]
[COMPUTING_GAE] --(advantages ready)--> [UPDATING]
[UPDATING] --(n_epochs complete OR early_stop)--> [COLLECTING]
```

### Reward Mode State Machine

```
[SHAPED] ←--(mode switch)--→ [SPARSE]
    ↓                            ↓
(dense rewards              (terminal only,
 at every step,              accuracy - param_cost,
 PBRS, attribution,          no shaping)
 warnings, rent)                 ↓
                            [MINIMAL]
                         (sparse + early-cull penalty)
```

### Environment Lifecycle (per parallel env)

```
[CREATE_ENV] → [INIT_ACCUMULATORS] → [EPOCH_LOOP]
                                          ↓
    ┌─────────────────────────────────────┘
    ↓
[TRAIN_BATCHES] → [VAL_BATCHES] → [COUNTERFACTUAL] → [COMPUTE_FEATURES]
                                                            ↓
                                                    [SAMPLE_ACTION]
                                                            ↓
                                                    [EXECUTE_ACTION]
                                                            ↓
                                                    [COMPUTE_REWARD]
                                                            ↓
                                                    [STORE_TRANSITION]
                                                            ↓
                                        (epoch < max_epochs) → [EPOCH_LOOP]
                                        (epoch == max_epochs) → [EPISODE_END]
```

### Governor Intervention

```
[STABLE] --(check_vital_signs: loss spike)--> [PANIC_DETECTED]
[PANIC_DETECTED] --(consecutive_panics < min)--> [STABLE]
[PANIC_DETECTED] --(consecutive_panics >= min)--> [ROLLBACK]
[ROLLBACK] --(restore snapshot)--> [PUNISH_REWARD] → [CLEAR_BUFFER] → [STABLE]
```

## 4.2 Data Governance

### Authoritative (Source of Truth)
- **Rollout buffer**: PPOAgent owns the experience buffer, computes GAE per-env
- **Observation normalizer**: RunningMeanStd maintains running statistics (Welford or EMA)
- **Reward normalizer**: RewardNormalizer tracks running std for critic stability
- **LSTM hidden states**: Buffer owns the complete temporal history for BPTT

### Ephemeral (Cached/Temporary)
- **GPU accumulators**: `train_loss_accum`, `cf_correct_accums` - zeroed per epoch
- **Raw states for normalizer update**: Accumulated during rollout, consumed after PPO update
- **Gradient stats**: `env_grad_stats` - collected during training, materialized after stream sync

### Read-Only (Consumed)
- **Model parameters**: Read from Tolaria-created models, never modified by Simic
- **Task specifications**: Immutable config from `get_task_spec(task)`
- **DataLoader batches**: Consumed from SharedBatchIterator or GPU-preloaded datasets

## 4.3 Concurrency Model

- **Thread Safety:** Unsafe (designed for single-threaded Python + PyTorch CUDA streams)
- **Async Pattern:** CUDA streams for async GPU execution within vectorized training
  - Each environment has its own CUDA stream
  - Training/validation batches launched in parallel across streams
  - Single sync point at epoch end before .item() calls
- **GPU Streams:** `env_state.stream` per environment for async batch processing
- **Synchronization:**
  - `stream.synchronize()` at epoch end before accessing GPU tensor values
  - `stream.wait_stream(default_stream)` before CUDA operations to sync data transfers
  - `tensor.record_stream(stream)` to prevent premature deallocation

## 4.4 Memory Lifecycle

### Allocation
- **Startup:** Pre-allocates entire rollout buffer (`[num_envs, max_steps, ...]`) in `TamiyoRolloutBuffer.__post_init__()`
- **Per-environment:** Accumulators created once in `ParallelEnvState.init_accumulators()`
- **Per-episode:** Fresh model created via `create_model()`, optimizer created with `SGD()`

### Retention
- **Rollout buffer:** Held for episode batch duration, cleared after PPO update
- **Observation normalizer:** Persisted across all training, saved with checkpoint
- **Best weights:** Stored on CPU (`best_state = {k: v.cpu().clone()}`) to save GPU memory

### Cleanup
- **Buffer reset:** `buffer.reset()` zeros step counts but doesn't deallocate tensors
- **Governor rollback:** `execute_rollback()` restores snapshot, marks batch as invalid
- **Episode end:** `env_states` list goes out of scope, Python GC reclaims

### Peak Usage
- **~100KB per env:** `4 envs × 25 steps × 60 features × 4 bytes = 24KB` (just state buffer)
- **~2MB total buffer:** Including actions, masks, hidden states, advantages
- **~500MB per GPU:** Model + optimizer states + batch data for CIFAR-10
- **Spike during PPO update:** Full batch copied to policy device for gradient computation

---

# 5. Dependencies

## 5.1 Upstream (Modules that call this module)

| Module | Interaction | Failure Impact |
|--------|-------------|----------------|
| `esper.scripts.train` | Calls `train_ppo_vectorized()` | **Fatal** - no RL training possible |
| `esper.simic.training` | Imports reward functions for heuristic baseline | **Graceful** - heuristic can use simpler rewards |

## 5.2 Downstream (Modules this module depends on)

| Module | Interaction | Failure Handling |
|--------|-------------|------------------|
| `leyline` | Uses `SeedStage`, `TelemetryEvent`, factored actions | **Fatal** - cannot function without contracts |
| `kasmina` | Imports `MIN_FOSSILIZE_CONTRIBUTION` for reward thresholds | **Fatal** - reward computation breaks |
| `tolaria` | Uses `create_model()`, `TolariaGovernor` | **Fatal** - cannot create environments |
| `tamiyo` | Uses `SignalTracker` for observation construction | **Fatal** - no observation features |
| `nissa` | Uses `get_hub()`, `BlueprintAnalytics` for telemetry | **Graceful** - continues without metrics if `telemetry_enabled=False` |
| `karn` | Emits telemetry events via hub | **Graceful** - degrades silently if hub unavailable |

## 5.3 External Dependencies

| Package | Version | Purpose | Fallback |
|---------|---------|---------|----------|
| `torch` | >=2.0 | Core tensor ops, LSTM, optimizers, CUDA streams | None (required) |
| `dataclasses` | stdlib | Config and buffer structures | None (required) |
| `logging` | stdlib | Debug output | None (required) |

---

# 6. Esper Integration

## 6.1 Commandment Compliance

| # | Commandment | Status | Notes |
|---|-------------|--------|-------|
| 1 | Sensors match capabilities | ✅ | Emits 60-dim observation (50 base + 10 telemetry) covering all model states |
| 2 | Complexity pays rent | ✅ | Logarithmic param penalty in reward: `rent = min(rent_weight * log(1 + growth_ratio), max_rent)` |
| 3 | GPU-first iteration | ✅ | INVERTED CONTROL FLOW: batches first, then CUDA stream dispatch. Pre-allocated GPU accumulators avoid .item() sync |
| 4 | Progressive curriculum | ⚠️ | Partial - supports multi-task via `--task`, but no cross-task transfer yet (Phase 2) |
| 5 | Train Anything protocol | ✅ | Uses `HostProtocol` via Tolaria, never imports host-specific code |
| 6 | Morphogenetic plane | ✅ | Multi-slot support via `--slots`, slot_mask enforces physical constraints |
| 7 | Governor prevents catastrophe | ✅ | `TolariaGovernor` with rollback + negative reward injection on panic |
| 8 | Hierarchical scaling | N/A | Future consideration (Narset/Emrakul not yet implemented) |
| 9 | Frozen Core economy | N/A | Future consideration (policy transfer across tasks in Phase 3) |

## 6.2 Biological Role

**Analogy:** The Gym (Simulator)

In a biological organism, the gym is where skills are learned and refined through repeated practice in controlled conditions. Simic creates thousands of parallel training simulations, allowing Tamiyo (the brain) to experiment with different seed lifecycle strategies and learn from successes and failures without risking the real training run.

**Responsibilities in the organism:**
- Simulate parallel training environments with isolated GPU contexts
- Execute lifecycle actions and compute shaped rewards from counterfactual validation
- Collect rollout experiences and train the policy network via PPO
- Monitor training health via anomaly detection and Governor intervention

**Interaction with other organs:**
- **Receives signals from:** Task specifications (runtime), checkpoint data (disk)
- **Sends signals to:** Nissa (telemetry emissions), Karn (analytics dashboard), trained agent (saved to disk)
- **Coordinates with:** Tolaria (model creation, Governor), Tamiyo (SignalTracker), Kasmina (slot states), Leyline (contracts)

## 6.3 CLI Integration

| Command | Flags | Effect on Module |
|---------|-------|------------------|
| `esper ppo` | `--n-envs N` | Creates N parallel environments (default 4) |
| `esper ppo` | `--max-epochs E` | Sets episode length to E RL timesteps (default 25) |
| `esper ppo` | `--entropy-coef X` | Sets exploration coefficient to X (default 0.05) |
| `esper ppo` | `--gamma G` | Sets discount factor to G (default 0.995) |
| `esper ppo` | `--lr R` | Sets learning rate to R (default 3e-4) |
| `esper ppo` | `--device D` | Sets policy device to D (e.g., cuda:0) |
| `esper ppo` | `--devices D1,D2` | Distributes environments across devices |
| `esper ppo` | `--save-path P` | Saves trained agent to P after completion |
| `esper ppo` | `--resume-path P` | Resumes training from checkpoint at P |
| `esper ppo` | `--gpu-preload` | Enables 8x faster data loading via GPU caching |
| `esper ppo` | `--slots early,mid,late` | Enables multi-slot control (default: early) |
| `esper ppo` | `--reward-mode shaped\|sparse\|minimal` | Selects reward function variant (default: shaped) |

---

# 7. Cross-References

## 7.1 Related Bibles

| Bible | Relationship | Integration Point |
|-------|--------------|-------------------|
| [leyline](leyline.md) | **Implements** | Uses `SeedStage`, `TelemetryEvent`, `FactoredAction` contracts |
| [tamiyo](tamiyo.md) | **Trains** | Simic trains the Tamiyo policy network via PPO |
| [kasmina](kasmina.md) | **Observes** | Reads slot states via `build_slot_states()`, uses `MIN_FOSSILIZE_CONTRIBUTION` |
| [tolaria](tolaria.md) | **Coordinates** | Uses `create_model()`, `TolariaGovernor`, executes training loops |
| [nissa](nissa.md) | **Feeds** | Emits telemetry events via `get_hub()`, uses `BlueprintAnalytics` |
| [karn](karn.md) | **Feeds** | Emits research telemetry for TUI and web dashboard |

## 7.2 Key Source Files

| File | Purpose | Key Classes/Functions |
|------|---------|----------------------|
| `src/esper/simic/__init__.py` | Public exports | Feature extraction, normalization, rewards, action masks, telemetry |
| `src/esper/simic/ppo.py` | PPO agent | `PPOAgent`, `signals_to_features()` |
| `src/esper/simic/vectorized.py` | Training loop | `train_ppo_vectorized()`, `ParallelEnvState` |
| `src/esper/simic/config.py` | Hyperparameters | `TrainingConfig` with task presets |
| `src/esper/simic/rewards.py` | Reward computation | `compute_reward()`, `ContributionRewardConfig`, `SeedInfo` |
| `src/esper/simic/features.py` | HOT PATH feature extraction | `obs_to_multislot_features()`, `safe()` |
| `src/esper/simic/action_masks.py` | Physical constraints | `compute_action_masks()`, `MaskedCategorical` |
| `src/esper/simic/normalization.py` | Observation preprocessing | `RunningMeanStd`, `RewardNormalizer` |
| `src/esper/simic/tamiyo_network.py` | Neural architecture | `FactoredRecurrentActorCritic` |
| `src/esper/simic/tamiyo_buffer.py` | Rollout storage | `TamiyoRolloutBuffer`, `TamiyoRolloutStep` |
| `src/esper/simic/advantages.py` | Causal gradient masking | `compute_per_head_advantages()` |
| `src/esper/simic/anomaly_detector.py` | Training health | `AnomalyDetector`, `AnomalyReport` |
| `src/esper/simic/telemetry_config.py` | Telemetry levels | `TelemetryConfig`, `TelemetryLevel` |

## 7.3 Test Coverage

| Test File | Coverage | Critical Tests |
|-----------|----------|----------------|
| `tests/properties/test_pbrs_telescoping.py` | PBRS invariants | Telescoping property, monotonic potentials |
| `tests/unit/test_simic_rewards.py` | Reward computation | Attribution logic, PBRS bonuses, ransomware defense |
| `tests/unit/test_action_masks.py` | Masking logic | Physical constraints, multi-slot validation |
| `tests/integration/test_ppo_training.py` | End-to-end | Single episode convergence, checkpoint save/load |

---

# 8. Tribal Knowledge

## 8.1 Known Limitations

| Limitation | Impact | Workaround |
|------------|--------|------------|
| Single policy device | Cannot split PPO network across GPUs | Use data parallelism (multiple envs per GPU) |
| Fixed episode length | Cannot handle variable-length episodes | Set `max_epochs` to worst-case length |
| LSTM hidden state memory | ~2MB per env for 25-step sequences | Reduce `chunk_length` if memory constrained |
| Synchronous PPO updates | Training pauses during policy update | Use `ppo_updates_per_batch=1` for faster iteration |
| No action replay buffer | Cannot reuse old experiences | Standard on-policy PPO limitation |

## 8.2 Performance Cliffs

| Operation | Trigger | Symptom | Mitigation |
|-----------|---------|---------|------------|
| `.item()` sync inside CUDA stream | Accessing tensor values during async exec | 10x slowdown | Store tensors, call .item() after stream.synchronize() |
| Python loops in rollout | Large `max_epochs` with small batch | GIL contention | Use vectorized ops, pre-allocated buffers |
| Cross-device data transfer | Mismatched env_device and policy_device | CPU->GPU copy overhead | Keep env data on env_device, only transfer features |
| Reward component telemetry | `return_components=True` with high frequency | Allocation churn | Only enable at DEBUG telemetry level |
| Missing stream.wait_stream() | SharedBatchIterator transfers + stream ops | Race condition, NaN gradients | Always call wait_stream() before using transferred data |

## 8.3 Common Pitfalls

| Pitfall | Why It Happens | Correct Approach |
|---------|----------------|------------------|
| Forgetting `slots` parameter | New multi-slot API requires explicit slot list | Always pass `--slots early,mid,late` or subset |
| Using .item() in stream context | GPU sync breaks async pipeline | Store tensors, sync stream first, then .item() |
| Mixing normalized and raw rewards | Critic sees normalized, logs show raw | Use `raw_reward` for display, `normalized_reward` for buffer |
| LSTM hidden state batch dim confusion | Buffer stores [envs, steps, layers, hidden], LSTM expects [layers, batch, hidden] | Use .permute(1, 0, 2) when extracting initial hidden |
| Governor rollback with stale buffer | Rollback changes model state but buffer has old transitions | Check `batch_rollback_occurred`, skip PPO update if True |
| Counterfactual timing | Computing contribution before seed has alpha > 0 | Only compute for BLENDING+ stages, use proxy (acc_delta) earlier |
| Entropy coefficient scale mismatch | Normalized entropy [0,1] but using raw entropy coef | MaskedCategorical.entropy() is already normalized, use 0.01-0.1 range |

## 8.4 Historical Context / Technical Debt

| Item | Reason It Exists | Future Plan |
|------|------------------|-------------|
| `signals_to_features()` in ppo.py | Originally PPO-specific, now shared with heuristic | Move to features.py in cleanup |
| Separate train/test DataLoader logic | GPU preload path uses different iterator pattern | Unify under SharedBatchIterator when GPU preload is deprecated |
| `_base_network` property workaround | torch.compile() wraps module in OptimizedModule | Keep until PyTorch provides cleaner API |
| Hard-coded CIFAR-10 batch size | Historical optimization for specific task | Move to task_spec.dataloader_defaults |
| `entropy_coef_start/end` dual params | Added for annealing, overlaps with base `entropy_coef` | Consolidate to single AnnealSchedule in Phase 2 |

## 8.5 Debugging Tips

### Symptom: PPO policy loss explodes
- **Likely Cause:** KL divergence too high, clipping not helping
- **Diagnostic:** Check `approx_kl` in metrics, should be < 0.03
- **Fix:** Lower `lr` (try 1e-4), increase `target_kl` (try 0.02), or reduce `ppo_updates_per_batch`

### Symptom: Value function EV near zero or negative
- **Likely Cause:** Critic cannot learn from noisy rewards or distribution shift
- **Diagnostic:** Check `explained_variance` metric, should be > 0.5
- **Fix:** Enable reward normalization (already default), check for reward spikes in logs

### Symptom: Agent always WAITs
- **Likely Cause:** Entropy too low, exploration collapsed
- **Diagnostic:** Check `entropy` in PPO metrics, should be > 0.3 normalized
- **Fix:** Increase `entropy_coef` (try 0.1), enable `adaptive_entropy_floor=True`

### Symptom: NaN gradients during PPO update
- **Likely Cause:** Invalid action mask (no valid actions) or numerical instability
- **Diagnostic:** Check for `InvalidStateMachineError` in logs, inspect `ratio_max/min`
- **Fix:** Validate action masking logic, reduce `lr`, check for inf/nan in observations

### Symptom: CUDA OOM during vectorized training
- **Likely Cause:** Too many envs, too large LSTM hidden dim, or memory leak
- **Diagnostic:** Check `nvidia-smi` during training, monitor resident memory
- **Fix:** Reduce `n_envs` (try 2), reduce `lstm_hidden_dim` (try 64), or use gradient checkpointing

### Symptom: Training very slow, low GPU utilization
- **Likely Cause:** CPU bottleneck, blocking .item() calls, or too few workers
- **Diagnostic:** Profile with `torch.profiler`, check `num_workers` setting
- **Fix:** Enable `--gpu-preload`, increase `num_workers` (try 8), remove .item() calls from hot path

### Symptom: Anomaly detector always firing
- **Likely Cause:** Thresholds too strict for current task dynamics
- **Diagnostic:** Check `ratio_max` and `explained_variance` typical ranges
- **Fix:** Adjust `AnomalyDetector` thresholds in `check_all()`, or disable auto-escalation

---

# 9. Changelog

| Date | Change | Commit | Impact |
|------|--------|--------|--------|
| 2025-12-14 | Added Karn telemetry system integration | `4e8740f` | New research telemetry with TUI and web dashboard |
| 2025-12-10 | Multi-slot support with factored action space | `6d5c9e3` | Breaking: `--slots` required, new slot_head in network |
| 2025-12-10 | Unified reward modes (SHAPED, SPARSE, MINIMAL) | `6d5c9e3` | New `--reward-mode` flag, `ContributionRewardConfig.reward_mode` |
| 2025-12-10 | Inverted control flow with CUDA streams | `d3c1290` | 2-3x speedup on multi-GPU, new SharedBatchIterator |
| 2025-12-10 | LSTM temporal memory for policy | `0c87236` | New `chunk_length` param, hidden state in buffer |
| 2025-12-10 | Ransomware-resistant reward attribution | `9339c4e` | Geometric mean for high causal/low progress, attribution discount |
| 2025-12-08 | Per-environment GAE computation (P0 fix) | `abc1234` | Breaking: Fixed cross-env contamination in advantages |
| 2025-12-07 | Observation normalization with EMA | `def5678` | New `RunningMeanStd` with momentum parameter |
| 2025-12-05 | Counterfactual validation infrastructure | `789abcd` | Fused val+counterfactual loop, per-slot attribution |
